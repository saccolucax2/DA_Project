{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **ðŸ EU's Residential Buildings Costs - Data Analytics Campaign**",
   "id": "d3e7a8fece29910f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation and Dataset Split",
   "id": "16fbbe096992dcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# Carica il dataset\n",
    "dataset = pd.read_csv(r\"dataset.csv\")\n",
    "\n",
    "# Splitta il dataset per Business Tendency Indicator (Cost)\n",
    "df = dataset[dataset['indic_bt'] == 'COST']\n",
    "\n",
    "# Prima rimuoviamo le righe dove unit Ã¨ I15 o I21\n",
    "df = df[~df['unit'].isin(['I15', 'I21'])]\n",
    "\n",
    "mapping_aggregati = {\n",
    "    'EA19': ['AT','BE','CY','EE','FI','FR','DE','GR','IE','IT','LV','LT','LU','MT','NL','PT','SK','SI','ES'],\n",
    "    'EA20': ['AT','BE','CY','HR','EE','FI','FR','DE','GR','IE','IT','LV','LT','LU','MT','NL','PT','SK','SI','ES'],\n",
    "    'EU27_2020': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE']\n",
    "}\n",
    "\n",
    "# 1) Separiamo le righe aggregate\n",
    "aggregati_da_espandere = set(mapping_aggregati.keys())\n",
    "df_agg = df[df['geo'].isin(aggregati_da_espandere)].copy()\n",
    "df_rest = df[~df['geo'].isin(aggregati_da_espandere)].copy()\n",
    "\n",
    "# 2) costruiamo il set di chiavi esistenti per paesi singoli\n",
    "existing_keys = set(zip(df_rest['geo'], df_rest['TIME_PERIOD']))\n",
    "\n",
    "# 2) Creiamo un DataFrame vuoto per le righe espanse\n",
    "df_expanded = []\n",
    "\n",
    "# 3) Per ogni riga in df_agg, generiamo copie per ciascun membro\n",
    "for idx, row in df_agg.iterrows():\n",
    "    codice_agg = row['geo']\n",
    "    anno = row['TIME_PERIOD']\n",
    "    paesi_membri = mapping_aggregati[codice_agg]\n",
    "\n",
    "    for iso2 in paesi_membri:\n",
    "         if (iso2, anno) in existing_keys:\n",
    "            continue\n",
    "         nuova_riga = row.copy()\n",
    "         nuova_riga['geo'] = iso2\n",
    "         nuova_riga['OBS_VALUE'] = round(nuova_riga['OBS_VALUE'], 1)\n",
    "         df_expanded.append(nuova_riga)\n",
    "\n",
    "# 4) Concateniamo df_rest e tutte le righe espanse in df_expanded\n",
    "df_expanded = pd.DataFrame(df_expanded)\n",
    "df_final = pd.concat([df_rest, df_expanded], ignore_index=True)\n",
    "\n",
    "# df_final ora contiene tutte le righe non aggregate + copie per paesi singoli\n",
    "df = df_final.copy()\n",
    "\n",
    "# Rimuovere gli Outliers\n",
    "df = df[~df['geo'].isin(['UA', 'TR', 'ME', 'RS', 'RO'])]\n",
    "\n",
    "# Elimina le colonne vuote o rindondanti\n",
    "df = df.drop(columns=['DATAFLOW', 'LAST UPDATE', 's_adj', 'indic_bt', 'freq', 'cpa2_1', 'CONF_STATUS', 'OBS_FLAG', 'unit'])\n",
    "df = df.fillna(dataset.mean(numeric_only=True))\n",
    "\n",
    "# Approssima OBS_VALUE a una cifra dopo la virgola\n",
    "df['OBS_VALUE'] = df['OBS_VALUE'].round(1)\n",
    "\n",
    "print(df.head(10))"
   ],
   "id": "47c2c4832770344a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Crea la tabella pivot\n",
    "pivot_df = df.pivot_table(index='TIME_PERIOD', columns='geo', values='OBS_VALUE')\n",
    "\n",
    "# Riapprossima perchÃ¨ Ã¨ coglione\n",
    "pivot_df = pivot_df.round(1)\n",
    "\n",
    "print(pivot_df)"
   ],
   "id": "72ee2931a3b07766",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Imputation and Standardization",
   "id": "ebb2c799c395bc48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 1. Trasposizione\n",
    "data = pivot_df.T\n",
    "\n",
    "# 2. Rimuove righe completamente vuote\n",
    "data = data.dropna(how='all')\n",
    "\n",
    "# 3. Imputazione: riempi i NaN con la media di ogni riga\n",
    "data_filled = data.apply(lambda column: column.fillna(column.mean().round(1)), axis=1)\n",
    "\n",
    "# 4. Rimuovi righe che ancora hanno tutti NaN (es. Media era NaN)\n",
    "data_filled = data_filled.dropna()\n",
    "\n",
    "# 7. Standardizzazione\n",
    "data_scaled = StandardScaler().fit_transform(data_filled)\n",
    "\n",
    "print(data_filled.head(10))"
   ],
   "id": "2d79b1cc9603c166",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Engineering",
   "id": "e9a89ae702730655"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prima, ordina per paese e anno\n",
    "df = df.sort_values(by=['geo', 'TIME_PERIOD']).reset_index(drop=True)\n",
    "\n",
    "# Creiamo un DataFrame con costi per paese e anno\n",
    "# Raggruppiamo per 'geo' e poi applichiamo shift(-1) su OBS_VALUE per avere il valore dellâ€™anno successivo\n",
    "df['target_cost'] = df.groupby('geo')['OBS_VALUE'].shift(-1)\n",
    "\n",
    "# Rimuovi righe dove target_cost Ã¨ NaN (ultimo anno di ciascun paese)\n",
    "df = df.dropna(subset=['target_cost'])\n",
    "\n",
    "print(df[['geo', 'TIME_PERIOD', 'OBS_VALUE', 'target_cost']].head(10))"
   ],
   "id": "600db4ee614d30dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calcola la variazione percentuale anno su anno\n",
    "df['prev_cost'] = df.groupby('geo')['OBS_VALUE'].shift(1)\n",
    "df['var_perc'] = (df['OBS_VALUE'] - df['prev_cost']) / df['prev_cost'] * 100.0\n",
    "\n",
    "# Etichetta le classi\n",
    "def label_func(x):\n",
    "    if x > 15:\n",
    "        return 'aumento'\n",
    "    elif x < -15:\n",
    "        return 'diminuzione'\n",
    "    else:\n",
    "        return 'stabile'\n",
    "\n",
    "df['label_variation'] = df['var_perc'].apply(lambda x: label_func(x) if pd.notna(x) else None)\n",
    "\n",
    "# Gestiamo i valori infiniti\n",
    "df['var_perc'] = df['var_perc'].replace([np.inf, -np.inf], 100.0).round(1)\n",
    "\n",
    "# Rimuovi righe dove prev_cost Ã¨ NaN (primo anno di ciascun paese)\n",
    "df = df.dropna(subset=['prev_cost', 'label_variation'])\n",
    "\n",
    "print(df[['geo', 'TIME_PERIOD', 'OBS_VALUE', 'var_perc', 'label_variation']].head(10))"
   ],
   "id": "f1b5ff21d790b1cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Media mobile su 3 anni (incluso l'anno corrente e i due precedenti)\n",
    "df['rolling_mean_3'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window=3, min_periods=1).mean()).round(1)\n",
    "\n",
    "# Media mobile su 5 anni\n",
    "df['rolling_mean_5'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window=5, min_periods=1).mean()).round(1)\n",
    "\n",
    "# Deviazione standard su 3 anni\n",
    "df['rolling_std_3'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window=3, min_periods=1).std()).round(1)\n",
    "\n",
    "# Variazione percentuale media su 3 anni\n",
    "df['pct_change_3'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.pct_change(periods=3)).round(1)\n",
    "\n",
    "# 3) Sostituisci inf e -inf con NaN\n",
    "df['pct_change_3'] = df['pct_change_3'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Dato che il primo anno di ogni paese non ha tre valori precedenti, std e pct saranno NaN\n",
    "# Sostituire NaN con 0 o con un valore minimo\n",
    "df['rolling_std_3'] = df['rolling_std_3'].fillna(0.0)\n",
    "df['pct_change_3'] = df['pct_change_3'].fillna(0.0)\n",
    "\n",
    "# Feature binaria se var_perc > 0 (crescita dellâ€™anno precedente)\n",
    "df['grew_last_year'] = (df['var_perc'] > 0).astype(int)\n",
    "\n",
    "# Slope di regressione lineare sui 3 anni precedenti (trend locale)\n",
    "def local_slope(series):\n",
    "    # Calcola slope ultimo valore basandosi sui 3 punti precedenti\n",
    "    if series.shape[0] < 3:\n",
    "        return 0\n",
    "    y = series.values[-3:]\n",
    "    x = np.arange(len(y))\n",
    "    # fit lineare y = ax + b\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    return a\n",
    "\n",
    "df['slope_3'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.expanding().apply(local_slope, raw=False)).round(1)\n",
    "print(df)"
   ],
   "id": "aea77d8214a4b1d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Clustering: KMeans",
   "id": "ab8702eaa08e8583"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 8. PCA a 3 componenti prima del clustering\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# 9. Clustering con KMeans e silhouette score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "best_k = 2\n",
    "best_score = -1\n",
    "for k in range(2, min(11, data_pca.shape[0])):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(data_pca)\n",
    "    score = silhouette_score(data_pca, labels)\n",
    "    print(f\"[PCA] Silhouette score per k={k}: {score:.3f}\")\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "print(f\"\\n[PCA] Miglior numero di cluster: {best_k} con Silhouette Score = {best_score:.3f}\")\n",
    "\n",
    "# 10. Applica KMeans finale\n",
    "kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "labels = kmeans_final.fit_predict(data_pca)"
   ],
   "id": "59d340024f8c8bc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Clustered Countries",
   "id": "9bf42fd6ecdff82a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clustered_df = pd.DataFrame(data_filled)\n",
    "clustered_df = clustered_df.rename(columns={'index': 'country'})\n",
    "labels_df = pd.Series(labels)\n",
    "clustered_df = clustered_df.iloc[:len(labels)]\n",
    "clustered_df['cluster'] = labels\n",
    "clustered_df.to_csv(\"clustered_countries.csv\")\n",
    "print(clustered_df.head())"
   ],
   "id": "c079e633ba72accc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## K-Means Visualization - Scatter Plot",
   "id": "3f8d525b259c7e5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Riduci a 2 dimensioni per lo scatter plot\n",
    "pca_2d = PCA(n_components=3, random_state=42)\n",
    "data_2d = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Prima del plotting, verifichiamo le dimensioni\n",
    "data_2d_len = len(data_2d)\n",
    "labels_len = len(labels)\n",
    "\n",
    "# Assicuriamoci che i dati siano della stessa lunghezza\n",
    "data_2d = data_2d[:min(data_2d_len, labels_len)]\n",
    "labels = labels[:min(data_2d_len, labels_len)]\n",
    "\n",
    "# Ora creiamo il plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=data_2d[:, 0], y=data_2d[:, 1], hue=labels, palette='tab10', s=80)\n",
    "\n",
    "# Aggiungiamo le etichette solo per i punti che abbiamo effettivamente plottato\n",
    "for i, name in enumerate(data_filled.index[:len(data_2d)]):\n",
    "    plt.text(data_2d[i, 0]+0.2, data_2d[i, 1]+0.2, name, fontsize=8)\n",
    "plt.title(f\"Cluster con K={best_k}, Silhouette Score={best_score:.3f}\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "6487a877418dfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## K-Means Visualization - Geographic Map",
   "id": "6b354f8a16f279f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "import pycountry\n",
    "\n",
    "def iso2_to_iso3(code):\n",
    "    # Verifica se il codice Ã¨ una stringa\n",
    "    if not isinstance(code, str):\n",
    "        return None\n",
    "    # Normalizza il codice in maiuscolo\n",
    "    code = code.upper()\n",
    "    try:\n",
    "        country = pycountry.countries.get(alpha_2=code)\n",
    "        return country.alpha_3 if country else None\n",
    "    except (AttributeError, LookupError):\n",
    "        return None\n",
    "\n",
    "iso3_codes = [iso2_to_iso3(c) for c in data_filled.index]\n",
    "\n",
    "map_iso3 = [iso2_to_iso3(c) for c in data_filled.index]\n",
    "\n",
    "map_k_df = pd.DataFrame({\n",
    "    'iso_alpha': iso3_codes,\n",
    "    'cluster': labels_df\n",
    "}).dropna(subset=['iso_alpha'])\n",
    "\n",
    "# 2) Disegniamo la choropleth per KMeans\n",
    "fig_k = px.choropleth(\n",
    "    map_k_df,\n",
    "    locations='iso_alpha',\n",
    "    color='cluster',\n",
    "    hover_name='iso_alpha',\n",
    "    color_discrete_sequence=px.colors.qualitative.Light24,\n",
    "    projection=\"mercator\",\n",
    "    locationmode=\"ISO-3\"\n",
    ")\n",
    "fig_k.update_geos(\n",
    "    scope=\"europe\",\n",
    "    showcountries=True,\n",
    "    showcoastlines=False,\n",
    "    showland=True,\n",
    "    fitbounds=\"locations\"\n",
    ")\n",
    "fig_k.update_layout(\n",
    "    title=f\"KMeans (k={best_k}) â€“ Cluster su mappa europea\",\n",
    "    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}\n",
    ")\n",
    "fig_k.show()"
   ],
   "id": "1d6601ef57387070",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Re-Clustering: DBSCAN",
   "id": "6c8e4d3505ec683c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=4).fit(data_scaled)  # k = min_samples\n",
    "distances, indices = nbrs.kneighbors(data_scaled)\n",
    "\n",
    "# Prendi la distanza k-esima per ogni punto e ordina\n",
    "k_distances = np.sort(distances[:, 3])  # 3 perchÃ© 0=min_auto, 1,2,3=4th nearest neighbor\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(k_distances)\n",
    "plt.ylabel('4th Nearest Neighbor Distance')\n",
    "plt.xlabel('Sorted Points')\n",
    "plt.title('k-distance Graph per DBSCAN')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "3efe2a4425f0b441",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Grid-search per DBSCAN\n",
    "# -------------------------------\n",
    "\n",
    "# 1) Definire range di eps e min_samples da testare\n",
    "eps_values = np.linspace(1.0, 8.0, num=15)\n",
    "min_samples_values = list(range(3, 7))\n",
    "\n",
    "best_score = -1\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "\n",
    "for eps_candidate in eps_values:\n",
    "    for min_s in min_samples_values:\n",
    "        db = DBSCAN(eps=eps_candidate, min_samples=min_s, metric='euclidean')\n",
    "        labels_tmp = db.fit_predict(data_scaled)\n",
    "\n",
    "        # Consideriamo solo i punti non outlier (label != -1)\n",
    "        mask = labels_tmp != -1\n",
    "        unique_clusters = set(labels_tmp[mask])\n",
    "\n",
    "        # Serve almeno 2 cluster reali per silhouette; altrimenti skip\n",
    "        if len(unique_clusters) <= 1:\n",
    "            continue\n",
    "\n",
    "        score_tmp = silhouette_score(data_scaled[mask], labels_tmp[mask])\n",
    "        # Se il silhouette Ã¨ migliorativo, aggiorniamo\n",
    "        if score_tmp > best_score:\n",
    "            best_score = score_tmp\n",
    "            best_eps = eps_candidate\n",
    "            best_min_samples = min_s\n",
    "\n",
    "# 2) Stampa dei parametri ottimali (o avviso se non ne trovi)\n",
    "if best_eps is None:\n",
    "    print(\"DBSCAN grid-search: nessuna combinazione ha prodotto â‰¥2 cluster validi.\")\n",
    "    best_eps = 7.5\n",
    "    best_min_samples = 5\n",
    "    print(f\"Usiamo valori di default eps={best_eps}, min_samples={best_min_samples}\")\n",
    "else:\n",
    "    print(f\"DBSCAN grid-search â€“ miglior eps={best_eps:.2f}, min_samples={best_min_samples}, silhouette={best_score:.3f}\")\n",
    "\n",
    "# 3) Applichiamo DBSCAN finale con i parametri selezionati\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples, metric='euclidean')\n",
    "labels_db = dbscan.fit_predict(data_scaled)\n",
    "\n",
    "# Eventuale verifica finale della silhouette\n",
    "mask_final = labels_db != -1\n",
    "if len(set(labels_db[mask_final])) > 1:\n",
    "    final_score = silhouette_score(data_scaled[mask_final], labels_db[mask_final])\n",
    "    print(f\"Silhouette finale (inlier): {final_score:.3f}\")\n",
    "else:\n",
    "    print(\"DBSCAN finale: meno di 2 cluster validi o troppi outlier.\")"
   ],
   "id": "207db70b42ca0be4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DBSCAN Visualization - Scatter Plot",
   "id": "51f59f74800dc546"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PCA 2D per visualizzare tutti i punti (anche rumore)\n",
    "pca2 = PCA(n_components=2, random_state=42)\n",
    "data_pca2 = pca2.fit_transform(data_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "palette = sns.color_palette(\"tab10\", len(set(labels_db)))\n",
    "sns.scatterplot(x=data_pca2[:, 0], y=data_pca2[:, 1],\n",
    "                hue=labels_db, palette=palette, legend=\"full\", s=80)\n",
    "n_points = min(len(data_filled.index), len(data_pca2))\n",
    "countries = data_filled.index[:n_points]\n",
    "for i, country in enumerate(countries):\n",
    "    plt.text(data_pca2[i, 0] + 0.1, data_pca2[i, 1] + 0.1, country, fontsize=7)\n",
    "\n",
    "plt.title(f\"DBSCAN su PCA 2D\")  #(Silhouette inlier: {score_db:.3f})\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Cluster DBSCAN\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ],
   "id": "a5e7d2a49acfaa2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DBSCAN Visualization - Geographic Map",
   "id": "7fc631d5b3097823"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def iso2_to_iso3(code):\n",
    "    try:\n",
    "        country = pycountry.countries.get(alpha_2=code)\n",
    "        return country.alpha_3 if country else None\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "# Applichiamo la conversione\n",
    "iso3_list = [iso2_to_iso3(c) for c in data_filled.index]\n",
    "\n",
    "map_iso3 = [iso2_to_iso3(c) for c in data_filled.index]\n",
    "\n",
    "# Prepara DataFrame per la mappa\n",
    "map_df = pd.DataFrame({\n",
    "    'iso_alpha': iso3_list,\n",
    "    'cluster': labels_db\n",
    "}).dropna(subset=['iso_alpha'])\n",
    "\n",
    "fig = px.choropleth(\n",
    "    map_df,\n",
    "    locations='iso_alpha',\n",
    "    color='cluster',\n",
    "    hover_name='iso_alpha',\n",
    "    color_discrete_sequence=px.colors.qualitative.Light24,\n",
    "    projection=\"mercator\",\n",
    "    locationmode=\"ISO-3\"\n",
    ")\n",
    "\n",
    "# Limita la mappa allâ€™Europa\n",
    "fig.update_geos(\n",
    "    scope=\"europe\",\n",
    "    showcountries=True,\n",
    "    showcoastlines=False,\n",
    "    showland=True,\n",
    "    fitbounds=\"locations\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Cluster DBSCAN su mappa europea\",\n",
    "    margin={\"r\": 0, \"t\": 40, \"l\": 0, \"b\": 0}\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "id": "e55b422d7f7da02e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classification & Regression (Split Training-Testing)",
   "id": "e0a4161ca6617104"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Consideriamo solo le colonne numeriche candidate per la regressione\n",
    "numerical_cols = ['OBS_VALUE', 'rolling_mean_3', 'rolling_std_3', 'grew_last_year', 'pct_change_3', 'rolling_mean_5', 'slope_3']\n",
    "\n",
    "# Calcolo della matrice di correlazione\n",
    "corr_matrix = df[numerical_cols].corr().abs()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='Blues', vmin=0, vmax=1)\n",
    "plt.title(\"Matrice di correlazione fra feature numeriche\")\n",
    "plt.show()"
   ],
   "id": "7c4cef3eb271774a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature Scelte per la regressione (coefficiente di correlazione < 0.8)\n",
    "print(df[['geo', 'TIME_PERIOD', 'OBS_VALUE', 'pct_change_3', 'rolling_std_3', 'grew_last_year']].head(10))"
   ],
   "id": "770c7cb460a77c1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Definiamo le feature candidate per regressione e classificazione\n",
    "feature_cols = ['OBS_VALUE', 'pct_change_3', 'rolling_std_3', 'grew_last_year']\n",
    "\n",
    "# Per regressione:\n",
    "regression_df = df.dropna(subset=['target_cost']).copy()\n",
    "X_reg = regression_df[feature_cols]\n",
    "y_reg = regression_df['target_cost']\n",
    "\n",
    "# Splitting: usiamo 'TIME_PERIOD' per train/test\n",
    "train_mask = regression_df['TIME_PERIOD'] <= 2016\n",
    "X_reg_train = X_reg[train_mask]\n",
    "X_reg_test = X_reg[~train_mask]\n",
    "y_reg_train = y_reg[train_mask]\n",
    "y_reg_test = y_reg[~train_mask]\n",
    "\n",
    "# Per classificazione:\n",
    "classification_df = df.dropna(subset=['label_variation']).copy()\n",
    "X_clf = classification_df[feature_cols]\n",
    "y_clf = classification_df['label_variation']\n",
    "\n",
    "# Stesso criterio temporale per la classificazione\n",
    "train_mask_clf = classification_df['TIME_PERIOD'] <= 2016\n",
    "X_clf_train = X_clf[train_mask_clf]\n",
    "X_clf_test = X_clf[~train_mask_clf]\n",
    "y_clf_train = y_clf[train_mask_clf]\n",
    "y_clf_test = y_clf[~train_mask_clf]\n",
    "\n",
    "print(\"Reg train size:\", X_reg_train.shape, \"Reg test size:\", X_reg_test.shape)\n",
    "print(\"Clf train size:\", X_clf_train.shape, \"Clf test size:\", X_clf_test.shape)"
   ],
   "id": "5a42c1be88bc3ea9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decision Tree - Classification",
   "id": "772c2db5c8c1b64d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepara i dati (X_clf_train/Test e y_clf_train/Test giÃ  definiti)\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "# Predizione\n",
    "y_dt_clf_pred = dt_clf.predict(X_clf_test)\n",
    "\n",
    "print(\"Decision Tree Classificazione â€“ Report:\\n\", classification_report(y_clf_test, y_dt_clf_pred))"
   ],
   "id": "a85f63c6ac59d885",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pruned Decision Tree - Classification",
   "id": "da51fb9980c1f82e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Trova percorso di ccp_alpha ottimale\n",
    "path_clf = dt_clf.cost_complexity_pruning_path(X_clf_train, y_clf_train)\n",
    "ccp_alphas_clf = path_clf.ccp_alphas[:-1]\n",
    "clf_trees = []\n",
    "f1_scores = []\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for ccp in ccp_alphas_clf:\n",
    "    dt = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp)\n",
    "    dt.fit(X_clf_train, y_clf_train)\n",
    "    y_pred = dt.predict(X_clf_test)\n",
    "    f1 = f1_score(y_clf_test, y_pred, average='weighted', zero_division=1)\n",
    "    clf_trees.append(dt)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "best_index_clf = np.argmax(f1_scores)\n",
    "best_alpha_clf = ccp_alphas_clf[best_index_clf]\n",
    "best_dt_clf = clf_trees[best_index_clf]\n",
    "print(f\"Pruned Tree Classificazione â€“ miglior ccp_alpha: {best_alpha_clf:.6f}, F1-weighted: {f1_scores[best_index_clf]:.3f}\")\n",
    "\n",
    "# Confronto con DT non potato\n",
    "base_f1 = f1_score(y_clf_test, y_dt_clf_pred, average='weighted', zero_division=1)\n",
    "print(f\"DT non potato F1-weighted: {base_f1:.3f}\")"
   ],
   "id": "bbf989f80b7614f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Confusion Matrix",
   "id": "206a770ee64f3a7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title='Matrice di Confusione'):\n",
    "    # Calcola la matrice di confusione\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Ottieni le classi uniche\n",
    "    classes = sorted(set(y_true))\n",
    "\n",
    "    # Crea la figura\n",
    "    plt.figure(figsize=(5, 4))\n",
    "\n",
    "    # Crea heatmap con seaborn\n",
    "    sns.heatmap(cm,\n",
    "                annot=True,  # Mostra i valori nelle celle\n",
    "                fmt='d',     # Formato numeri interi\n",
    "                cmap='Blues',  # Palette di colori\n",
    "                xticklabels=classes,\n",
    "                yticklabels=classes)\n",
    "\n",
    "    # Personalizza il grafico\n",
    "    plt.title(title, pad=20)\n",
    "    plt.xlabel('Predetto')\n",
    "    plt.ylabel('Reale')\n",
    "\n",
    "    # Aggiungi percentuali di accuratezza per ogni classe\n",
    "    accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "    for i, accuracy in enumerate(accuracies):\n",
    "        plt.text(-0.5, i, f'{accuracy:.1%}',\n",
    "                va='center',\n",
    "                ha='right',\n",
    "                fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizza la matrice per il Decision Tree originale\n",
    "plot_confusion_matrix(y_clf_test, y_dt_clf_pred,\n",
    "                     title='Matrice di Confusione - Decision Tree')\n",
    "\n",
    "# Visualizza la matrice per il Decision Tree potato\n",
    "plot_confusion_matrix(y_clf_test, best_dt_clf.predict(X_clf_test),\n",
    "                     title='Matrice di Confusione - Decision Tree Potato')"
   ],
   "id": "448a7a318d633844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decision Tree - Regression",
   "id": "f98f8be2e7efca64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Prepara i dati (assumendo tu abbia X_reg_train/Test e y_reg_train/Test giÃ  definiti)\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "# Predizioni\n",
    "y_dt_pred = dt_reg.predict(X_reg_test)\n",
    "\n",
    "# Metriche iniziali\n",
    "rmse_dt = mean_squared_error(y_reg_test, y_dt_pred)\n",
    "mae_dt  = mean_absolute_error(y_reg_test, y_dt_pred)\n",
    "r2_dt   = r2_score(y_reg_test, y_dt_pred)\n",
    "\n",
    "print(f\"Decision Tree Regressione â€“ RMSE: {rmse_dt:.3f}, MAE: {mae_dt:.3f}, RÂ²: {r2_dt:.3f}\")"
   ],
   "id": "ed150b7f3d834c9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pruned Decision Tree - Regression",
   "id": "6074695d9d90da03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Trova il percorso di ccp_alpha ottimale\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "path = dt_reg.cost_complexity_pruning_path(X_reg_train, y_reg_train)\n",
    "ccp_alphas = path.ccp_alphas[:-1]  # lâ€™ultimo crea un albero con un solo nodo\n",
    "reg_trees = []\n",
    "rmse_scores = []\n",
    "\n",
    "ccp_alphas = np.maximum(ccp_alphas, 0.0)\n",
    "\n",
    "for ccp in ccp_alphas:\n",
    "    dt = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp)\n",
    "    dt.fit(X_reg_train, y_reg_train)\n",
    "    y_pred = dt.predict(X_reg_test)\n",
    "    rmse_val = mean_squared_error(y_reg_test, y_pred)\n",
    "    reg_trees.append(dt)\n",
    "    rmse_scores.append(rmse_val)\n",
    "\n",
    "# Seleziona lâ€™alpha che minimizza RMSE\n",
    "best_index = np.argmin(rmse_scores)\n",
    "best_alpha = ccp_alphas[best_index]\n",
    "best_dt = reg_trees[best_index]\n",
    "print(f\"Pruned Tree â€“ miglior ccp_alpha: {best_alpha:.6f}, RMSE: {rmse_scores[best_index]:.3f}\")\n",
    "\n",
    "# Confronto con il DT non potato\n",
    "print(f\"DT non potato RMSE: {rmse_dt:.3f}\")"
   ],
   "id": "51cdf79c45a8139c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Diagnostica di Regressione",
   "id": "92bea125c3a53579"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_regression_diagnostics(y_true, y_pred, title='Diagnostica Regressione'):\n",
    "    # Calcola i residui\n",
    "    residuals = y_true - y_pred\n",
    "\n",
    "    # Crea una figura con 2x2 subplot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "    fig.suptitle(title, fontsize=16, y=1.02)\n",
    "\n",
    "    # 1. Valori Predetti vs Valori Reali\n",
    "    axes[0,0].scatter(y_true, y_pred, alpha=0.5)\n",
    "    axes[0,0].plot([y_true.min(), y_true.max()],\n",
    "                   [y_true.min(), y_true.max()],\n",
    "                   'r--', lw=2)\n",
    "    axes[0,0].set_xlabel('Valori Reali')\n",
    "    axes[0,0].set_ylabel('Valori Predetti')\n",
    "    axes[0,0].set_title('Predetti vs Reali')\n",
    "\n",
    "    # 2. Distribuzione dei Residui\n",
    "    sns.histplot(residuals, kde=True, ax=axes[0,1])\n",
    "    axes[0,1].axvline(x=0, color='r', linestyle='--')\n",
    "    axes[0,1].set_xlabel('Residui')\n",
    "    axes[0,1].set_title('Distribuzione dei Residui')\n",
    "\n",
    "    # 3. Residui vs Valori Predetti\n",
    "    axes[1,0].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axes[1,0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[1,0].set_xlabel('Valori Predetti')\n",
    "    axes[1,0].set_ylabel('Residui')\n",
    "    axes[1,0].set_title('Residui vs Predetti')\n",
    "\n",
    "    # 4. Q-Q Plot dei Residui\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1,1])\n",
    "    axes[1,1].set_title('Q-Q Plot dei Residui')\n",
    "\n",
    "    # Aggiungi metriche di performance\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    metrics_text = f'RMSE: {rmse:.2f}\\nMAE: {mae:.2f}\\nRÂ²: {r2:.2f}'\n",
    "    fig.text(0.02, 0.98, metrics_text, fontsize=12,\n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizza diagnostica per il Decision Tree originale\n",
    "plot_regression_diagnostics(y_reg_test,\n",
    "                          dt_reg.predict(X_reg_test),\n",
    "                          'Diagnostica Regressione - Decision Tree')\n",
    "\n",
    "# Visualizza diagnostica per il Decision Tree potato\n",
    "plot_regression_diagnostics(y_reg_test,\n",
    "                          best_dt.predict(X_reg_test),\n",
    "                          'Diagnostica Regressione - Decision Tree Potato')"
   ],
   "id": "47d78a843538071a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decision Trees - Visualization",
   "id": "3a859f04ae1077b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Per la visualizzazione dei Decision Tree\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Decision Tree Classificazione\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt_clf,\n",
    "          feature_names=feature_cols,\n",
    "          class_names=sorted(y_clf_train.unique()),\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Decision Tree - Classificazione (Non Potato)\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Decision Tree Classificazione Potato\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(best_dt_clf,\n",
    "          feature_names=feature_cols,\n",
    "          class_names=sorted(y_clf_train.unique()),\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(f\"Decision Tree - Classificazione (Potato con ccp_alpha={best_alpha_clf:.6f})\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Decision Tree Regressione\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt_reg,\n",
    "          feature_names=feature_cols,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Decision Tree - Regressione (Non Potato)\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Decision Tree Regressione Potato\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(best_dt,\n",
    "          feature_names=feature_cols,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(f\"Decision Tree - Regressione (Potato con ccp_alpha={best_alpha:.6f})\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Grafico dell'andamento dell'errore in funzione di ccp_alpha\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ccp_alphas_clf, f1_scores, marker='o')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.ylabel('F1-score')\n",
    "plt.title('F1-score vs ccp_alpha (Classificazione)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 6. Grafico dell'andamento dell'errore in funzione di ccp_alpha per regressione\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ccp_alphas, rmse_scores, marker='o')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs ccp_alpha (Regressione)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "f8b6cda98889ac6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Forest - Regression",
   "id": "d79c5bdadcfe4a78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Crea e addestra il modello\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "# 1. Fai le predizioni\n",
    "y_reg_pred = regressor.predict(X_reg_test)\n",
    "\n",
    "# 2. Calcola le metriche\n",
    "rmse = np.sqrt(mean_squared_error(y_reg_test, y_reg_pred))\n",
    "mae = mean_absolute_error(y_reg_test, y_reg_pred)\n",
    "r2 = r2_score(y_reg_test, y_reg_pred)\n",
    "\n",
    "print(f\"Regressione â€“ RMSE: {rmse:.3f}, MAE: {mae:.3f}, RÂ²: {r2:.3f}\")"
   ],
   "id": "2aa816bf279bff57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SMOTE & Random Forest - Classification",
   "id": "b2a36c960e498ee9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# 0. Verifica della distribuzione delle classi\n",
    "print(\"Distribuzione delle classi nel training set:\")\n",
    "print(y_clf_train.value_counts(normalize=True) * 100)\n",
    "print(\"\\nDistribuzione delle classi nel test set:\")\n",
    "print(y_clf_test.value_counts(normalize=True) * 100)\n",
    "\n",
    "# 1. Crea e addestra il modello\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "# 2. Predici sui dati di test\n",
    "y_clf_pred = clf.predict(X_clf_test)\n",
    "\n",
    "# 3. Calcola le metriche\n",
    "print(\"Classification Report:\\n\", classification_report(y_clf_test, y_clf_pred, zero_division=1))\n",
    "\n",
    "# Applica SMOTE solo al training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_clf_train_balanced, y_clf_train_balanced = smote.fit_resample(X_clf_train, y_clf_train)\n",
    "\n",
    "print(\"\\nDistribuzione originale delle classi:\", Counter(y_clf_train))\n",
    "print(\"Distribuzione dopo SMOTE:\", Counter(y_clf_train_balanced))\n",
    "\n",
    "# Addestra il modello con i dati bilanciati\n",
    "clf_balanced = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_balanced.fit(X_clf_train_balanced, y_clf_train_balanced)\n",
    "\n",
    "# Fai predizioni e valuta\n",
    "y_clf_pred_balanced = clf_balanced.predict(X_clf_test)\n",
    "print(\"\\nReport di classificazione con SMOTE:\")\n",
    "print(classification_report(y_clf_test, y_clf_pred_balanced, zero_division=1))\n",
    "\n",
    "def plot_confusion_matrix_detailed(y_true, y_pred, title='Matrice di Confusione Dettagliata'):\n",
    "    # Calcola la matrice di confusione\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calcola le percentuali per ogni riga\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "    # Ottieni le classi uniche\n",
    "    classes = sorted(set(y_true))\n",
    "\n",
    "    # Crea la figura\n",
    "    plt.figure(figsize=(5, 4))\n",
    "\n",
    "    # Crea la heatmap principale\n",
    "    sns.heatmap(cm, annot=np.asarray([\n",
    "        [f'{count}\\n({percentage:.1f}%)'\n",
    "         for count, percentage in zip(row_counts, row_percentages)]\n",
    "        for row_counts, row_percentages in zip(cm, cm_percentage)\n",
    "    ]),\n",
    "                fmt='',\n",
    "                cmap='Blues',\n",
    "                xticklabels=classes,\n",
    "                yticklabels=classes,\n",
    "                cbar_kws={'label': 'Numero di campioni'})\n",
    "\n",
    "    # Aggiungi le etichette e il titolo\n",
    "    plt.title(title, pad=20, fontsize=14)\n",
    "    plt.xlabel('Classe Predetta', fontsize=12)\n",
    "    plt.ylabel('Classe Reale', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Valutiamo i modelli\n",
    "plot_confusion_matrix_detailed(y_clf_test, y_clf_pred, \"Modello Base\")\n",
    "plot_confusion_matrix_detailed(y_clf_test, y_clf_pred_balanced, \"Modello con SMOTE\")"
   ],
   "id": "e95533716168e564",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Importance",
   "id": "804d2314f1e280f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualizziamo le feature importance\n",
    "def plot_feature_importance(model, feature_names, title):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(f\"Feature Importance ({title})\")\n",
    "    plt.bar(range(X_clf_train.shape[1]), importances[indices])\n",
    "    plt.xticks(range(X_clf_train.shape[1]), [feature_names[i] for i in indices], rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizza feature importance per ogni modello\n",
    "plot_feature_importance(clf, feature_cols, \"Modello Base\")\n",
    "plot_feature_importance(clf_balanced, feature_cols, \"Modello con SMOTE\")"
   ],
   "id": "a5982481c8976d85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predicted Models Export (.csv)",
   "id": "1c73ed0865e8c9cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Esporta dataset pulito con feature e target per regressione e classificazione\n",
    "df.to_csv(\"dataset_for_models.csv\", index=False)\n",
    "\n",
    "# 2) Esporta previsioni del modello di regressione (aggiungiamo una colonna con y_reg_pred)\n",
    "regression_df = regression_df.copy()\n",
    "regression_df['pred_cost'] = regressor.predict(regression_df[feature_cols]).round(1)\n",
    "regression_df[['geo', 'TIME_PERIOD', 'OBS_VALUE', 'pred_cost']].to_csv(\"regression_results.csv\", index=False)\n",
    "\n",
    "# 3) Esporta etichette di variazione (classification) con probabilitÃ \n",
    "probs = clf.predict_proba(X_clf_test).round(1)\n",
    "classification_output = classification_df.loc[~train_mask_clf, ['geo', 'TIME_PERIOD', 'OBS_VALUE', 'label_variation']].copy()\n",
    "# Assumiamo che le classi siano ['aumento','diminuzione','stabile'] nellâ€™ordine di clf.classes_\n",
    "for idx, cls in enumerate(clf.classes_):\n",
    "    classification_output[f\"prob_{cls}\"] = probs[:, idx]\n",
    "classification_output.to_csv(\"classification_results.csv\", index=False)\n",
    "\n",
    "print(\"Dataset pulito con feature e target per regressione e classificazione:\")\n",
    "print(regression_df)"
   ],
   "id": "dd6f9fdbab1ee7ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Dataset con etichette di variazione (classification):\")\n",
    "print(classification_df)"
   ],
   "id": "e9a393f82f902321",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Proof of Concepts",
   "id": "57f669fb596881ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Esempio: confronto tra COST reale e COST predetto per un dato paese (es. 'IT')\n",
    "it_df = regression_df[regression_df['geo'] == 'IT'].copy()\n",
    "it_df['year_pred'] = it_df['TIME_PERIOD'] + 1\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "# Costo reale in t\n",
    "sns.lineplot(x='TIME_PERIOD', y='OBS_VALUE', data=it_df, label='Costo reale')\n",
    "# Costo predetto allineato su year_pred (cioÃ¨ pred_cost in anno t)\n",
    "sns.lineplot(x='year_pred', y='pred_cost', data=it_df, label='Costo predetto')\n",
    "plt.title(\"Italia: Costo reale vs Costo predetto\")\n",
    "plt.xlabel(\"Anno\")\n",
    "plt.ylabel(\"Indice costo\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "54182ad998f1c952",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
