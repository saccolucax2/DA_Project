{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **üè†EU's Residential Buildings Costs - Data Analytics Campaign**\n",
    "Benvenuti in questo notebook: l‚Äôobiettivo √® mostrare passo passo come abbiamo costruito, testato e automatizzato un‚Äôintera pipeline di **Data Analytics** sui **costi di produzione dei nuovi edifici residenziali in Europa**, basandoci sui dati Eurostat.\n",
    "\n",
    "**Punti principali che affronteremo:**\n",
    "1. **Data Preparation & Feature Engineering**\n",
    "   - Lettura dei CSV Eurostat, espansione degli aggregati (EA19, EA20, EU27_2020), rimozione di outlier geografici.\n",
    "   - Costruzione di tutte le feature (rolling, pct_change, slope, ecc.) necessarie per i modelli.\n",
    "2. **Clustering non supervisionato (KMeans & DBSCAN)**\n",
    "   - Raggruppamento dei paesi in base all‚Äôandamento delle variazioni percentuali sui costi.\n",
    "   - Valutazione con silhouette score e visualizzazione su PCA 2D e mappa europea.\n",
    "3. **Modelli Supervisionati (Regressione & Classificazione)**\n",
    "   - Previsione del ‚Äúcosto‚Äù dell‚Äôanno successivo (regressione) e della ‚Äúvariazione‚Äù (classificazione).\n",
    "   - Costruzione di Decision Tree (base e potati) e Random Forest (tuned), con metriche di valutazione (RMSE, R¬≤, accuracy, F1, ecc.).\n",
    "4. **Output & Automazione**\n",
    "   - Salvataggio di modelli serializzati (joblib), metriche in JSON e previsioni in CSV.\n",
    "   - Generazione di grafici statici (PNG) e interattivi (HTML) in una cartella ‚Äúreports‚Äù.\n",
    "   - Spunti per schedulare il notebook in modalit√† automatizzata (cron, GitHub Actions, ecc.).\n",
    "\n",
    "In questo notebook troverete, prima di ogni blocco di codice, una spiegazione in italiano (e qualche dettaglio metodologico) per capire **cosa** stiamo facendo e **perch√©**. Buona lettura!"
   ],
   "id": "d3e7a8fece29910f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Flow Diagram\n",
    "![DFD Progetto](../report/images/Data_Flow_Diagram.png)\n",
    "\n",
    "#### 1. Data Ingestion & Expansion (P1)\n",
    "- **Input**: file CSV scaricato da Eurostat\n",
    "- **Logica**:\n",
    "  1. Selezione di tutte le righe con `indic_bt == 'COST'` e `unit == 'PCH_SM'`\n",
    "  2. Separazione delle righe di aggregati (`EA19`, `EA20`, `EU27_2020`) ed espansione in singoli paesi (AT, BE, ‚Ä¶)\n",
    "  3. Rimozione degli outlier geografici (`UA`, `TR`, `ME`, `RS`, `RO`)\n",
    "- **Output**: `DS1` (Expanded & Filtered Data)\n",
    "\n",
    "#### 2. Cleaning & Imputation (P2)\n",
    "- **Input**: `DS1`\n",
    "- **Logica**:\n",
    "  1. Rimozione delle colonne ridondanti (`DATAFLOW`, `LAST UPDATE`, `unit`, ‚Ä¶)\n",
    "  2. Imputazione iniziale dei valori mancanti con la media **globale** del dataset (per non perdere troppe righe)\n",
    "- **Output**: `DS2` (Cleaned Data)\n",
    "\n",
    "#### 3. Pivoting & Standardization (P3)\n",
    "- **Input**: `DS2`\n",
    "- **Logica**:\n",
    "  1. Creazione di `pivot_df = country √ó year` con i valori di `OBS_VALUE`\n",
    "  2. Trasposizione (`data = pivot_df.T`) e rimozione delle righe completamente vuote\n",
    "  3. Imputazione riga‚Äëper‚Äëriga con `fillna(row.mean())` e drop delle eventuali righe residue con NaN\n",
    "  4. Applicazione di `StandardScaler` per ottenere `data_scaled`\n",
    "- **Output**: `DS3` (Standardized Pivot Table)\n",
    "\n",
    "#### 4. Feature Engineering (P4)\n",
    "- **Input**: `DS2`\n",
    "- **Logica**:\n",
    "  1. `target_cost = OBS_VALUE.shift(-1)` per la regressione\n",
    "  2. Calcolo di `prev_cost`, `var_perc` (sostituendo ¬±‚àû con 0) e creazione di `label_variation` con soglia ¬±15%\n",
    "  3. Computazione di rolling features: `rolling_mean_3`, `rolling_std_3`, `pct_change_3`, `slope_3`, `grew_last_year`\n",
    "- **Output**: `DS4` (Feature Store)\n",
    "\n",
    "#### 5. Clustering (P5)\n",
    "- **Input**: `DS3` (in particolare `data_scaled`)\n",
    "- **Logica**:\n",
    "  1. Riduzione dimensionale con `PCA(n_components=3)`\n",
    "  2. **KMeans**: ricerca di _k_ ottimale tramite silhouette score ‚Üí cluster ‚Äúsferici‚Äù\n",
    "  3. **DBSCAN**: analisi del _k-distance graph_ + grid‚Äësearch su `eps` e `min_samples` ‚Üí cluster basati sulla densit√† e rilevazione outlier\n",
    "- **Output**: `DS5` (Cluster Assignments)\n",
    "\n",
    "#### 6. Modeling (P6)\n",
    "- **Input**: `DS4` ‚Üí i dataset `(X_reg, y_reg, X_clf, y_clf)` e split train/test basato su `TIME_PERIOD` (anni ‚â§‚ÄØ2016 vs >‚ÄØ2016)\n",
    "- **Logica**:\n",
    "  1. **Decision Tree** (base + pruned) per regressione e classificazione\n",
    "  2. **Random Forest** (base, tuning di iperparametri, SMOTE applicato solo al _training_ set di classificazione, `class_weight`)\n",
    "  3. Decomposizione **Bias‚ÄìVarianza** (`bias_variance_decomp`) per RF e DT\n",
    "- **Output**: `DS6` (Trained Models)\n",
    "\n",
    "#### 7. Evaluation & Reporting (P7)\n",
    "- **Input**: `DS5` + `DS6`\n",
    "- **Logica**:\n",
    "  1. Calcolo delle **metriche**:\n",
    "     - Clustering: silhouette score, Calinski‚ÄìHarabasz, Davies‚ÄìBouldin\n",
    "     - Regressione: RMSE, MAE, R¬≤\n",
    "     - Classificazione: precision, recall, F1, accuracy, balanced accuracy\n",
    "  2. Generazione di **visualizzazioni**: heatmap, scatter‚ÄØPCA, mappe‚ÄØchoropleth, line‚Äëplot reale vs predetto, boxplot, barplot delle feature importance, k‚Äëdistance graph\n",
    "  3. Compilazione del **report testuale** (CRISP‚ÄëDM, ML Canvas, DFD, risultati, discussione, conclusioni)\n",
    "- **Output**: `DS7` (Reports & Visualizations)\n",
    "\n",
    "### Distribuzione agli Stakeholder\n",
    "- **Output finale**:\n",
    "  - Report in PDF o PowerPoint\n",
    "  - Jupyter Notebook completamente documentato\n",
    "  - Modelli serializzati (pickle/joblib)\n",
    "  - CSV con risultati e predizioni\n",
    "  - Script di pipeline pronto per schedulazione"
   ],
   "id": "aad564534a501dc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Preparation and Dataset Split\n",
    "In questa prima sezione prepariamo i dati di base:\n",
    "\n",
    "1. **Import delle librerie necessarie**,\n",
    "2. **Lettura del CSV ‚Äúgrezzo‚Äù** (dataset Eurostat),\n",
    "3. **Split preliminare** dei dati per eventuali usi successivi.\n",
    "\n",
    "L‚Äôidea √® di creare un punto di partenza ‚Äúpulito‚Äù: un DataFrame che contenga tutte le righe ‚Äúpaese‚Äêanno‚Äù di nostro interesse, pronto per le fasi di **feature engineering** e **clustering**.\n",
    "\n",
    "Di seguito:\n",
    "- Caricheremo il file `dataset.csv` dalla cartella `data/raw` (o da un URL, se disponibile).\n",
    "- Verificheremo che non ci siano record duplicati o righe outlier (in questa fase limitiamoci a leggere e dare una prima occhiata alle colonne).  "
   ],
   "id": "16fbbe096992dcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# Carica il dataset\n",
    "dataset = pd.read_csv(r\"../data/raw/dataset.csv\")"
   ],
   "id": "f34c98759f3112ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 Import delle librerie e caricamento del CSV\n",
    "\n",
    "- `os` serve per eventuali operazioni di sistema (es. variabili d‚Äôambiente).\n",
    "- `pandas` √® il pacchetto principale per il caricamento e la manipolazione dei DataFrame.\n",
    "- Impostiamo `os.environ['OMP_NUM_THREADS']='1'` per limitare al singolo thread alcune operazioni BLAS/NumPy, in modo da evitare avvisi di parallellismo eccessivo.\n",
    "\n",
    "Infine, leggiamo il CSV principale (`dataset.csv`) che contiene i dati **Eurostat** sui costi di produzione di nuovi edifici residenziali (annuale) per ciascun paese/anno."
   ],
   "id": "2a3e25347ccf6a9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Splitta il dataset per Business Tendency Indicator (Cost)\n",
    "df = dataset[dataset['indic_bt'] == 'COST']\n",
    "\n",
    "# Prima rimuoviamo le righe dove unit √® I15 o I21\n",
    "df = df[~df['unit'].isin(['I15', 'I21'])]\n",
    "\n",
    "# Mappiamo le sigle aggregate dividendole nei singoli paesi:\n",
    "mapping_aggregati = {\n",
    "    'EA19': ['AT','BE','CY','EE','FI','FR','DE','GR','IE','IT','LV','LT','LU','MT','NL','PT','SK','SI','ES'],\n",
    "    'EA20': ['AT','BE','CY','HR','EE','FI','FR','DE','GR','IE','IT','LV','LT','LU','MT','NL','PT','SK','SI','ES'],\n",
    "    'EU27_2020': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE']\n",
    "}\n",
    "\n",
    "# 1) Separiamo le righe aggregate\n",
    "aggregati_da_espandere = set(mapping_aggregati.keys())\n",
    "df_agg = df[df['geo'].isin(aggregati_da_espandere)].copy()\n",
    "df_rest = df[~df['geo'].isin(aggregati_da_espandere)].copy()\n",
    "\n",
    "# 2) costruiamo il set di chiavi esistenti per paesi singoli\n",
    "existing_keys = set(zip(df_rest['geo'], df_rest['TIME_PERIOD']))\n",
    "\n",
    "# 2) Creiamo un DataFrame vuoto per le righe espanse\n",
    "df_expanded = []\n",
    "\n",
    "# 3) Per ogni riga in df_agg, generiamo copie per ciascun membro\n",
    "for idx, row in df_agg.iterrows():\n",
    "    codice_agg = row['geo']\n",
    "    anno = row['TIME_PERIOD']\n",
    "    paesi_membri = mapping_aggregati[codice_agg]\n",
    "\n",
    "    for iso2 in paesi_membri:\n",
    "         if (iso2, anno) in existing_keys:\n",
    "            continue\n",
    "         nuova_riga = row.copy()\n",
    "         nuova_riga['geo'] = iso2\n",
    "         nuova_riga['OBS_VALUE'] = round(nuova_riga['OBS_VALUE'], 1)\n",
    "         df_expanded.append(nuova_riga)\n",
    "\n",
    "# 4) Concateniamo df_rest e tutte le righe espanse in df_expanded\n",
    "df_expanded = pd.DataFrame(df_expanded)\n",
    "df_final = pd.concat([df_rest, df_expanded], ignore_index=True)\n",
    "\n",
    "# df_final ora contiene tutte le righe non aggregate + copie per paesi singoli\n",
    "df = df_final.copy()\n",
    "\n",
    "# Rimuovere gli Outliers\n",
    "df = df[~df['geo'].isin(['UA', 'TR', 'ME', 'RS', 'RO'])]\n",
    "\n",
    "# Elimina le colonne vuote o rindondanti\n",
    "df = df.drop(columns=['DATAFLOW', 'LAST UPDATE', 's_adj', 'indic_bt', 'freq', 'cpa2_1', 'CONF_STATUS', 'OBS_FLAG', 'unit'])\n",
    "df = df.fillna(dataset.mean(numeric_only=True))\n",
    "\n",
    "# Approssima OBS_VALUE a una cifra dopo la virgola\n",
    "df['OBS_VALUE'] = df['OBS_VALUE'].round(1)\n",
    "\n",
    "# Outlier Elimination\n",
    "def remove_outliers_iqr_per_group(df, group_col='geo', value_col='OBS_VALUE'):\n",
    "    filtered_rows = []\n",
    "    for key, group in df.groupby(group_col):\n",
    "        Q1 = group[value_col].quantile(0.25)\n",
    "        Q3 = group[value_col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        group_filtered = group[(group[value_col] >= lower) & (group[value_col] <= upper)]\n",
    "        filtered_rows.append(group_filtered)\n",
    "    return pd.concat(filtered_rows, ignore_index=True)\n",
    "\n",
    "# Applica la funzione\n",
    "df = remove_outliers_iqr_per_group(df, group_col='geo', value_col='OBS_VALUE')\n",
    "\n",
    "print(df.head(10))"
   ],
   "id": "47c2c4832770344a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Creazione di una tabella pivot iniziale (paesi √ó anni)\n",
    "\n",
    "‚úÖ **Scopo**: avere immediatamente sott‚Äôocchio la struttura ‚Äúpaese √ó anno‚Äù di `OBS_VALUE` (il valore dell‚Äôindice dei costi).\n",
    "- `pivot_df.index` = serie temporali (`TIME_PERIOD`, ovvero gli anni 2000‚Äì2024).\n",
    "- `pivot_df.columns` = codici ‚Äúgeo‚Äù (paesi e aggregati Eurostat).\n",
    "- `pivot_df.values` = i valori numerici di `OBS_VALUE` (l‚Äôindice vero e proprio).\n",
    "\n",
    "Nei prossimi passi manterremo questo formato per operare imputazioni, transizioni verso clustering e cos√¨ via."
   ],
   "id": "acac4b3b9adec81f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Crea la tabella pivot\n",
    "pivot_df = df.pivot_table(index='TIME_PERIOD', columns='geo', values='OBS_VALUE')\n",
    "\n",
    "# Riapprossima perch√® √® coglione\n",
    "pivot_df = pivot_df.round(1)\n",
    "\n",
    "print(pivot_df)"
   ],
   "id": "72ee2931a3b07766",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Imputation and Standardization\n",
    "Prima di procedere con l‚Äôanalisi non supervisionata (clustering), √® fondamentale:\n",
    "\n",
    "1. **Rimuovere le righe (paesi) completamente vuote** (se un paese non ha mai pubblicato dati).\n",
    "2. **Imputare i valori mancanti** (NaN) con la **media** di ciascuna riga (ci√≤ significa: ‚Äúper ogni paese, sostituisci i NaN con la media dei valori disponibili in quell‚Äôanno‚Äêserie‚Äù).\n",
    "3. **Standardizzare** (Z‚Äêscore) le serie, in modo da rendere le variabili confrontabili e garantire che il clustering non sia dominato da scale diverse.\n",
    "\n",
    "Di seguito:\n",
    "- Trasponiamo `pivot_df` per passare da ‚Äúanno √ó paese‚Äù a ‚Äúpaese √ó anno‚Äù.\n",
    "- Riempiamo i NaN con la media riga per riga (`row.mean()`).\n",
    "- Eliminiamo ancora eventuali paesi che restano tutti NaN (criterio di sicurezza).\n",
    "- Applichiamo lo `StandardScaler()` di scikit‚Äêlearn (media 0, varianza 1) a tutta la matrice finale."
   ],
   "id": "ebb2c799c395bc48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 1. Trasposizione\n",
    "data = pivot_df.T\n",
    "\n",
    "# 2. Rimuove righe completamente vuote\n",
    "data = data.dropna(how='all')\n",
    "\n",
    "# 3. Imputazione: riempi i NaN con la media di ogni riga\n",
    "data_filled = data.apply(lambda column: column.fillna(column.mean().round(1)), axis=1)\n",
    "\n",
    "# 4. Rimuovi righe che ancora hanno tutti NaN (es. Media era NaN)\n",
    "data_filled = data_filled.dropna()\n",
    "\n",
    "# 7. Standardizzazione\n",
    "data_scaled = StandardScaler().fit_transform(data_filled)\n",
    "\n",
    "print(data_filled.head(10))"
   ],
   "id": "2d79b1cc9603c166",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\n",
    "Prima di fare qualsiasi clustering o training supervisionato, calcoliamo tutte le feature derivate (rolling, variazioni percentuali, slope, ecc.) per ogni (paese, anno).\n",
    "\n",
    "**Passaggi principali:**\n",
    "\n",
    "1. **Ordinare i dati** per `(geo, TIME_PERIOD)` in modo da avere serie ordinate di anno in anno.\n",
    "2. **Target per regressione (`target_cost`)**:\n",
    "   - `target_cost = OBS_VALUE` dell‚Äôanno successivo, calcolato con `groupby('geo')['OBS_VALUE'].shift(-1)`.\n",
    "   - Rimuoviamo le righe in cui `target_cost` √® NaN (ultimo anno di ciascun paese), perch√© non possiamo fare previsione oltre l‚Äôultimo dato disponibile.\n",
    "3. **Variazione percentuale anno‚Äêsu‚Äêanno (`var_perc`)**:\n",
    "   - Calcoliamo `prev_cost = lag(OBS_VALUE)`.\n",
    "   - `var_perc = 100 * (OBS_VALUE - prev_cost) / prev_cost`.\n",
    "   - Sostituiamo i valori infiniti con 0 (`.replace([np.inf, -np.inf], 0.0)`), poi riempiamo i NaN con 0.\n",
    "   - Creiamo `label_variation` in base a una soglia ¬±15%:\n",
    "     - `\"aumento\"` se `var_perc > 15%`,\n",
    "     - `\"diminuzione\"` se `var_perc < -15%`,\n",
    "     - `\"stabile\"` altrimenti.\n",
    "4. **Rolling e deviazione standard su 3 e 5 anni**:\n",
    "   - `rolling_mean_3`: media mobile 3‚Äêanni incluso l‚Äôanno corrente (`.rolling(window=3, min_periods=1).mean()`).\n",
    "   - `rolling_mean_5`: media mobile 5‚Äêanni.\n",
    "   - `rolling_std_3`: deviazione standard mobile 3‚Äêanni.\n",
    "   - `pct_change_3`: variazione percentuale rispetto a 3 anni prima (`.pct_change(periods=3)`).\n",
    "   - Sostituiamo infiniti con NaN e quindi `fillna(0.0)`.\n",
    "5. **Feature binaria ‚Äúcrescita‚Äù**:\n",
    "   - `grew_last_year = 1` se `var_perc > 0`, altrimenti `0`.\n",
    "6. **Slope locale su 3 anni**:\n",
    "   - Definiamo `slope_3` come la pendenza (coefficiente angolare) di una regressione lineare sui 3 anni precedenti.\n",
    "   - Usiamo `np.polyfit([0,1,2], last_3_values, 1)` dentro a una `transform(lambda x: x.expanding().apply(...))` per calcolare la pendenza dell‚Äôultimo triennio, cumulativa.\n",
    "\n",
    "Al termine, ogni riga del DataFrame contiene:\n",
    "- `OBS_VALUE` (valore indice corrente),\n",
    "- `target_cost` (valore indice t+1),\n",
    "- `prev_cost`, `var_perc`, `label_variation`,\n",
    "- `rolling_mean_3`, `rolling_mean_5`, `rolling_std_3`, `pct_change_3`,\n",
    "- `grew_last_year`, `slope_3`.\n",
    "\n",
    "Queste feature verranno poi usate sia per il clustering sia per i modelli di regressione e classificazione supervisionata.\n"
   ],
   "id": "e9a89ae702730655"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prima, ordina per paese e anno\n",
    "df = df.sort_values(by=['geo', 'TIME_PERIOD']).reset_index(drop=True)\n",
    "\n",
    "# Creiamo un DataFrame con costi per paese e anno\n",
    "# Raggruppiamo per 'geo' e poi applichiamo shift(-1) su OBS_VALUE per avere il valore dell‚Äôanno successivo\n",
    "df['target_cost'] = df.groupby('geo')['OBS_VALUE'].shift(-1)\n",
    "\n",
    "# Rimuovi righe dove target_cost √® NaN (ultimo anno di ciascun paese)\n",
    "df = df.dropna(subset=['target_cost'])\n",
    "\n",
    "print(df[['geo', 'TIME_PERIOD', 'OBS_VALUE', 'target_cost']].head(10))"
   ],
   "id": "600db4ee614d30dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calcola la variazione percentuale anno su anno\n",
    "df['prev_cost'] = df.groupby('geo')['OBS_VALUE'].shift(1)\n",
    "df['var_perc'] = (df['OBS_VALUE'] - df['prev_cost']) / df['prev_cost'] * 100.0\n",
    "\n",
    "# Etichetta le classi\n",
    "def label_func(x):\n",
    "    if x > 15:\n",
    "        return 'aumento'\n",
    "    elif x < -15:\n",
    "        return 'diminuzione'\n",
    "    else:\n",
    "        return 'stabile'\n",
    "\n",
    "df['label_variation'] = df['var_perc'].apply(lambda x: label_func(x) if pd.notna(x) else None)\n",
    "\n",
    "# Conta quante volte compare ciascuna etichetta\n",
    "counts = df['label_variation'].value_counts()\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"Numero totale di osservazioni per tipo di variazione:\")\n",
    "for label, count in counts.items():\n",
    "    print(str(label) + \": \" + str(count))\n",
    "print()\n",
    "\n",
    "# Gestiamo i valori infiniti\n",
    "df['var_perc'] = df['var_perc'].replace([np.inf, -np.inf], 100.0).round(1)\n",
    "\n",
    "# Rimuovi righe dove prev_cost √® NaN (primo anno di ciascun paese)\n",
    "df = df.dropna(subset=['prev_cost', 'label_variation'])\n",
    "\n",
    "print(df[['geo', 'TIME_PERIOD', 'OBS_VALUE', 'var_perc', 'label_variation']].head(10))"
   ],
   "id": "f1b5ff21d790b1cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Media mobile su 3 anni (incluso l'anno corrente e i due precedenti)\n",
    "df['rolling_mean_3'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window=3, min_periods=1).mean()).round(1)\n",
    "\n",
    "# Media mobile su 5 anni\n",
    "df['rolling_mean_5'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window=5, min_periods=1).mean()).round(1)\n",
    "\n",
    "# Deviazione standard su 3 anni\n",
    "df['rolling_std_3'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window=3, min_periods=1).std()).round(1)\n",
    "\n",
    "# Variazione percentuale media su 3 anni\n",
    "df['pct_change_3'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.pct_change(periods=3)).round(1)\n",
    "\n",
    "# 3) Sostituisci inf e -inf con NaN\n",
    "df['pct_change_3'] = df['pct_change_3'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Dato che il primo anno di ogni paese non ha tre valori precedenti, std e pct saranno NaN\n",
    "# Sostituire NaN con 0 o con un valore minimo\n",
    "df['rolling_std_3'] = df['rolling_std_3'].fillna(0.0)\n",
    "df['pct_change_3'] = df['pct_change_3'].fillna(0.0)\n",
    "\n",
    "# Feature binaria se var_perc > 0 (crescita dell‚Äôanno precedente)\n",
    "df['grew_last_year'] = (df['var_perc'] > 0).astype(int)\n",
    "\n",
    "# Slope di regressione lineare sui 3 anni precedenti (trend locale)\n",
    "def local_slope(series):\n",
    "    # Calcola slope ultimo valore basandosi sui 3 punti precedenti\n",
    "    if series.shape[0] < 3:\n",
    "        return 0\n",
    "    y = series.values[-3:]\n",
    "    x = np.arange(len(y))\n",
    "    # fit lineare y = ax + b\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    return a\n",
    "\n",
    "df['slope_3'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.expanding().apply(local_slope, raw=False)).round(1)\n",
    "print(df)"
   ],
   "id": "aea77d8214a4b1d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clustering: KMeans\n",
    "In questa sezione utilizziamo l‚Äôalgoritmo **KMeans** per individuare gruppi di paesi ‚Äúsimili‚Äù sulla base delle caratteristiche **temporali‚Äêstatistiche** calcolate (in questo caso, su `pct_change_3`, ma esporremo comunque il flusso logico completo).\n",
    "\n",
    "### Procedura:\n",
    "1. **PCA a 3 componenti**:\n",
    "   - Riduciamo la dimensionalit√† di `data_scaled` (che √® dimensione ‚Äúpaese √ó numero_anni‚Äù) a uno spazio a 3 dimensioni (`n_components=3`).\n",
    "   - Lo scopo √® catturare almeno il 70‚Äì80% della varianza originale, mantenendo comunque la rappresentazione pi√π compatta.\n",
    "2. **Ricerca del numero ottimale di cluster (k)**:\n",
    "   - Proviamo `k` da `2` a `min(11, numero_paesi)`.\n",
    "   - Per ciascun `k`, eseguiamo `KMeans(n_clusters=k)` con `random_state=42, n_init=10`, quindi calcoliamo il **silhouette score**.\n",
    "   - Il silhouette score varia da `-1 a +1`: valori > 0.50 indicano cluster ben separati.\n",
    "   - Selezioniamo `best_k` che massimizza la silhouette.\n",
    "3. **Applicazione definitiva di KMeans**:\n",
    "   - Eseguiamo KMeans con `k=best_k` sull‚Äôoutput di PCA e otteniamo `labels`, un array di lunghezza pari al numero di paesi, in cui `labels[i]` √® l‚Äôindice del cluster di **data_filled.index[i]** (il codice ISO-2 del paese).\n",
    "4. **Salvataggio dei risultati**:\n",
    "   - Creeremo un DataFrame `clustered_df` in cui ogni riga corrisponde a un paese (index) e conterr√† la colonna `cluster=labels[i]`.\n",
    "   - Lo salveremo in CSV per poterlo usare come ‚Äúfeature aggiuntiva‚Äù o per consultazioni successive.\n",
    "\n",
    "**Nota**: la metrica **silhouette** ci aiuta a capire se i cluster formati sono ‚Äúravvicinati‚Äù all‚Äôinterno e ‚Äúlontani‚Äù tra di loro. Un valore di circa 0.58‚Äì0.60 √® gi√† buono per serie temporali di questo tipo."
   ],
   "id": "ab8702eaa08e8583"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 8. PCA a 3 componenti prima del clustering\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# 9. Clustering con KMeans e silhouette score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "best_k = 2\n",
    "best_score = -1\n",
    "for k in range(2, min(11, data_pca.shape[0])):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(data_pca)\n",
    "    score = silhouette_score(data_pca, labels)\n",
    "    print(f\"[PCA] Silhouette score per k={k}: {score:.3f}\")\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "print(f\"\\n[PCA] Miglior numero di cluster: {best_k} con Silhouette Score = {best_score:.3f}\")\n",
    "\n",
    "# 10. Applica KMeans finale\n",
    "kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "labels = kmeans_final.fit_predict(data_pca)"
   ],
   "id": "59d340024f8c8bc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clustered Countries\n",
    "In questo blocco creiamo il DataFrame `clustered_df` che associa a ogni **paese** (indice di `data_filled`) il `cluster` corrispondente:\n",
    "\n",
    "1. **Costruiamo un DataFrame `clustered_df`** a partire da `data_filled` (paese √ó serie completa),\n",
    "2. Aggiungiamo una colonna `cluster` con il valore di `labels[i]` per ciascun paese.\n",
    "3. Salviamo `clustered_df` in un file CSV (`clustering_results.csv`) per riferimento esterno.\n",
    "\n",
    "In questo modo, potremo facilmente ricavare la ‚Äúetichetta di cluster‚Äù per ogni paese e usarla come feature aggiuntiva o semplicemente per reporting."
   ],
   "id": "9bf42fd6ecdff82a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clustered_df = pd.DataFrame(data_filled)\n",
    "clustered_df = clustered_df.rename(columns={'index': 'country'})\n",
    "labels_df = pd.Series(labels)\n",
    "clustered_df = clustered_df.iloc[:len(labels)]\n",
    "clustered_df['cluster'] = labels\n",
    "clustered_df.to_csv(\"../data/csv/clustering_results.csv\")\n",
    "print(clustered_df.head())"
   ],
   "id": "c079e633ba72accc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## K-Means Visualization - Scatter Plot",
   "id": "3f8d525b259c7e5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Riduci a 2 dimensioni per lo scatter plot\n",
    "pca_2d = PCA(n_components=3, random_state=42)\n",
    "data_2d = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Prima del plotting, verifichiamo le dimensioni\n",
    "data_2d_len = len(data_2d)\n",
    "labels_len = len(labels)\n",
    "\n",
    "# Assicuriamoci che i dati siano della stessa lunghezza\n",
    "data_2d = data_2d[:min(data_2d_len, labels_len)]\n",
    "labels = labels[:min(data_2d_len, labels_len)]\n",
    "\n",
    "# Ora creiamo il plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=data_2d[:, 0], y=data_2d[:, 1], hue=labels, palette='tab10', s=80)\n",
    "\n",
    "# Aggiungiamo le etichette solo per i punti che abbiamo effettivamente plottato\n",
    "for i, name in enumerate(data_filled.index[:len(data_2d)]):\n",
    "    plt.text(data_2d[i, 0]+0.2, data_2d[i, 1]+0.2, name, fontsize=8)\n",
    "plt.title(f\"Cluster con K={best_k}, Silhouette Score={best_score:.3f}\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../report/images/Kmeans(OE).png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "6487a877418dfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## K-Means Visualization - Geographic Map",
   "id": "6b354f8a16f279f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "import pycountry\n",
    "\n",
    "def iso2_to_iso3(code):\n",
    "    # Verifica se il codice √® una stringa\n",
    "    if not isinstance(code, str):\n",
    "        return None\n",
    "    # Normalizza il codice in maiuscolo\n",
    "    code = code.upper()\n",
    "    try:\n",
    "        country = pycountry.countries.get(alpha_2=code)\n",
    "        return country.alpha_3 if country else None\n",
    "    except (AttributeError, LookupError):\n",
    "        return None\n",
    "\n",
    "iso3_codes = [iso2_to_iso3(c) for c in data_filled.index]\n",
    "\n",
    "map_iso3 = [iso2_to_iso3(c) for c in data_filled.index]\n",
    "\n",
    "map_k_df = pd.DataFrame({\n",
    "    'iso_alpha': iso3_codes,\n",
    "    'cluster': labels_df\n",
    "}).dropna(subset=['iso_alpha'])\n",
    "\n",
    "# 2) Disegniamo la choropleth per KMeans\n",
    "fig_k = px.choropleth(\n",
    "    map_k_df,\n",
    "    locations='iso_alpha',\n",
    "    color='cluster',\n",
    "    hover_name='iso_alpha',\n",
    "    color_discrete_sequence=px.colors.qualitative.Light24,\n",
    "    projection=\"mercator\",\n",
    "    locationmode=\"ISO-3\"\n",
    ")\n",
    "fig_k.update_geos(\n",
    "    scope=\"europe\",\n",
    "    showcountries=True,\n",
    "    showcoastlines=False,\n",
    "    showland=True,\n",
    "    fitbounds=\"locations\"\n",
    ")\n",
    "fig_k.update_layout(\n",
    "    title=f\"KMeans (k={best_k}) ‚Äì Cluster su mappa europea\",\n",
    "    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}\n",
    ")\n",
    "fig_k.show()"
   ],
   "id": "1d6601ef57387070",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Re-Clustering: DBSCAN\n",
    "In alternativa a KMeans, applichiamo un algoritmo **DBSCAN** non supervisionato, che raggruppa i punti per densit√†:\n",
    "\n",
    "1. **k‚Äêdistance graph**:\n",
    "   - Calcoliamo la distanza del 4¬∞ nearest neighbor (dato `min_samples=4`) per ogni punto in `data_scaled`.\n",
    "   - Tiriamo fuori le distanze ordinate `k_distances = sorted(distances[:,3])` e tracciamo un grafico per individuare il ‚Äúgomito‚Äù (valore di eps ottimale).\n",
    "2. **Grid‚Äêsearch di eps e min_samples** (per trovare combinazione ottimale con silhouette)"
   ],
   "id": "6c8e4d3505ec683c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=4).fit(data_scaled)  # k = min_samples\n",
    "distances, indices = nbrs.kneighbors(data_scaled)\n",
    "\n",
    "# Prendi la distanza k-esima per ogni punto e ordina\n",
    "k_distances = np.sort(distances[:, 3])  # 3 perch√© 0=min_auto, 1,2,3=4th nearest neighbor\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(k_distances)\n",
    "plt.ylabel('4th Nearest Neighbor Distance')\n",
    "plt.xlabel('Sorted Points')\n",
    "plt.title('k-distance Graph per DBSCAN')\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../report/images/k-distance-DBSCAN.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "3efe2a4425f0b441",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Grid-search per DBSCAN\n",
    "# -------------------------------\n",
    "\n",
    "# 1) Definire range di eps e min_samples da testare\n",
    "eps_values = np.linspace(1.0, 8.0, num=15)\n",
    "min_samples_values = list(range(5, 10))\n",
    "\n",
    "best_score = -1\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "\n",
    "for eps_candidate in eps_values:\n",
    "    for min_s in min_samples_values:\n",
    "        db = DBSCAN(eps=eps_candidate, min_samples=min_s, metric='euclidean')\n",
    "        labels_tmp = db.fit_predict(data_scaled)\n",
    "\n",
    "        # Consideriamo solo i punti non outlier (label != -1)\n",
    "        mask = labels_tmp != -1\n",
    "        unique_clusters = set(labels_tmp[mask])\n",
    "\n",
    "        # Serve almeno 2 cluster reali per silhouette; altrimenti skip\n",
    "        if len(unique_clusters) <= 1:\n",
    "            continue\n",
    "\n",
    "        score_tmp = silhouette_score(data_scaled[mask], labels_tmp[mask])\n",
    "        # Se il silhouette √® migliorativo, aggiorniamo\n",
    "        if score_tmp > best_score:\n",
    "            best_score = score_tmp\n",
    "            best_eps = eps_candidate\n",
    "            best_min_samples = min_s\n",
    "\n",
    "# 2) Stampa dei parametri ottimali (o avviso se non ne trovi)\n",
    "if best_eps is None:\n",
    "    print(\"DBSCAN grid-search: nessuna combinazione ha prodotto ‚â•2 cluster validi.\")\n",
    "    best_eps = 7.5\n",
    "    best_min_samples = 8\n",
    "    print(f\"Usiamo valori di default eps={best_eps}, min_samples={best_min_samples}\")\n",
    "else:\n",
    "    print(f\"DBSCAN grid-search ‚Äì miglior eps={best_eps:.2f}, min_samples={best_min_samples}, silhouette={best_score:.3f}\")\n",
    "\n",
    "# 3) Applichiamo DBSCAN finale con i parametri selezionati\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples, metric='euclidean')\n",
    "labels_db = dbscan.fit_predict(data_scaled)\n",
    "\n",
    "# Eventuale verifica finale della silhouette\n",
    "mask_final = labels_db != -1\n",
    "if len(set(labels_db[mask_final])) > 1:\n",
    "    final_score = silhouette_score(data_scaled[mask_final], labels_db[mask_final])\n",
    "    print(f\"Silhouette finale (inlier): {final_score:.3f}\")\n",
    "else:\n",
    "    print(\"DBSCAN finale: meno di 2 cluster validi o troppi outlier.\")"
   ],
   "id": "207db70b42ca0be4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DBSCAN Visualization - Scatter Plot",
   "id": "51f59f74800dc546"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PCA 2D per visualizzare tutti i punti (anche rumore)\n",
    "pca2 = PCA(n_components=2, random_state=42)\n",
    "data_pca2 = pca2.fit_transform(data_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "palette = sns.color_palette(\"tab10\", len(set(labels_db)))\n",
    "sns.scatterplot(x=data_pca2[:, 0], y=data_pca2[:, 1],\n",
    "                hue=labels_db, palette=palette, legend=\"full\", s=80)\n",
    "n_points = min(len(data_filled.index), len(data_pca2))\n",
    "countries = data_filled.index[:n_points]\n",
    "for i, country in enumerate(countries):\n",
    "    plt.text(data_pca2[i, 0] + 0.1, data_pca2[i, 1] + 0.1, country, fontsize=7)\n",
    "\n",
    "plt.title(f\"DBSCAN su PCA 2D\")  #(Silhouette inlier: {score_db:.3f})\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Cluster DBSCAN\", bbox_to_anchor=(1, 1))\n",
    "plt.savefig(\"../report/images/DBSCAN.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "a5e7d2a49acfaa2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DBSCAN Visualization - Geographic Map",
   "id": "7fc631d5b3097823"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def iso2_to_iso3(code):\n",
    "    try:\n",
    "        country = pycountry.countries.get(alpha_2=code)\n",
    "        return country.alpha_3 if country else None\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "# Applichiamo la conversione\n",
    "iso3_list = [iso2_to_iso3(c) for c in data_filled.index]\n",
    "\n",
    "map_iso3 = [iso2_to_iso3(c) for c in data_filled.index]\n",
    "\n",
    "# Prepara DataFrame per la mappa\n",
    "map_df = pd.DataFrame({\n",
    "    'iso_alpha': iso3_list,\n",
    "    'cluster': labels_db\n",
    "}).dropna(subset=['iso_alpha'])\n",
    "\n",
    "fig = px.choropleth(\n",
    "    map_df,\n",
    "    locations='iso_alpha',\n",
    "    color='cluster',\n",
    "    hover_name='iso_alpha',\n",
    "    color_discrete_sequence=px.colors.qualitative.Light24,\n",
    "    projection=\"mercator\",\n",
    "    locationmode=\"ISO-3\"\n",
    ")\n",
    "\n",
    "# Limita la mappa all‚ÄôEuropa\n",
    "fig.update_geos(\n",
    "    scope=\"europe\",\n",
    "    showcountries=True,\n",
    "    showcoastlines=False,\n",
    "    showland=True,\n",
    "    fitbounds=\"locations\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Cluster DBSCAN su mappa europea\",\n",
    "    margin={\"r\": 0, \"t\": 40, \"l\": 0, \"b\": 0}\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "id": "e55b422d7f7da02e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Classification & Regression (Split Training-Testing)\n",
    "Ora che abbiamo tutte le feature, costruiamo i due sotto‚Äêinsiemi ‚ÄúRegressione‚Äù e ‚ÄúClassificazione‚Äù con lo stesso criterio temporale (anni ‚â§ 2016 ‚Üí train; > 2016 ‚Üí test).\n",
    "\n",
    "**Passaggi:**\n",
    "1. Definiamo un elenco di **feature candidate** (quelle con correlazione < 0.8 per evitare multicollinearit√†)\n",
    "2. **Regressione**:\n",
    "    - Usiamo `df.dropna(subset=['target_cost'])` per tenere solo le righe con `target_cost` definito.\n",
    "    - `X_reg = df[feature_cols]` e `y_reg = df['target_cost']`.\n",
    "    - Split:\n",
    "       - `train_mask = TIME_PERIOD ‚â§ 2016`\n",
    "       - `X_reg_train = X_reg[train_mask]`, `X_reg_test = X_reg[~train_mask]`\n",
    "       - Stessa logica per `y_reg`.\n",
    "3. **Classificazione**:\n",
    "    - Usiamo `df.dropna(subset=['label_variation'])` per le righe con etichetta.\n",
    "    - `X_clf = df[feature_cols]` e `y_clf = df['label_variation']`.\n",
    "    - Split identico (anni ‚â§ 2016 ‚Üí train; > 2016 ‚Üí test).\n",
    "\n",
    "Stampiamo in output le dimensioni di train/test per entrambe le modalit√†, giusto per avere un‚Äôidea di quante righe abbiamo su cui addestrare e testare."
   ],
   "id": "e0a4161ca6617104"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Consideriamo solo le colonne numeriche candidate per la regressione\n",
    "numerical_cols = ['OBS_VALUE', 'rolling_mean_3', 'rolling_std_3', 'grew_last_year', 'pct_change_3', 'rolling_mean_5', 'slope_3']\n",
    "\n",
    "# Calcolo della matrice di correlazione\n",
    "corr_matrix = df[numerical_cols].corr().abs()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='Blues', vmin=0, vmax=1)\n",
    "plt.title(\"Matrice di correlazione fra feature numeriche\")\n",
    "plt.savefig(\"../report/images/Correlation_Matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "7c4cef3eb271774a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature Scelte per la regressione (coefficiente di correlazione < 0.8)\n",
    "print(df[['geo', 'TIME_PERIOD', 'OBS_VALUE', 'pct_change_3', 'rolling_std_3', 'grew_last_year']].head(10))"
   ],
   "id": "770c7cb460a77c1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Analisi esplorativa\n",
    "df.info()\n",
    "df.describe()"
   ],
   "id": "f36f9be4cd6dedf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Boxplot\n",
    "paesi_da_plot = ['IT','DE','FR','ES','PL']\n",
    "df_box = df[df['geo'].isin(paesi_da_plot)].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    x='geo',\n",
    "    y='pct_change_3',\n",
    "    hue='geo',\n",
    "    data=df_box,\n",
    "    palette='pastel',\n",
    "    dodge=False\n",
    ")\n",
    "plt.title(\"Distribuzione di pct_change_3 per paese\")\n",
    "plt.xlabel(\"Paese (ISO-2)\")\n",
    "plt.ylabel(\"Variazione % media su 3 anni\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.savefig(\"../report/images/Box_Plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "f645d53f1f14bb90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Definiamo le feature candidate per regressione e classificazione\n",
    "feature_cols = ['OBS_VALUE', 'pct_change_3', 'rolling_std_3', 'grew_last_year']\n",
    "\n",
    "# Per regressione:\n",
    "regression_df = df.dropna(subset=['target_cost']).copy()\n",
    "x_reg = regression_df[feature_cols]\n",
    "y_reg = regression_df['target_cost']\n",
    "\n",
    "# Splitting: usiamo 'TIME_PERIOD' per train/test\n",
    "train_mask = regression_df['TIME_PERIOD'] <= 2016\n",
    "x_reg_train = x_reg[train_mask]\n",
    "x_reg_test = x_reg[~train_mask]\n",
    "y_reg_train = y_reg[train_mask]\n",
    "y_reg_test = y_reg[~train_mask]\n",
    "\n",
    "# Per classificazione:\n",
    "classification_df = df.dropna(subset=['label_variation']).copy()\n",
    "x_clf = classification_df[feature_cols]\n",
    "y_clf = classification_df['label_variation']\n",
    "\n",
    "# Stesso criterio temporale per la classificazione\n",
    "train_mask_clf = classification_df['TIME_PERIOD'] <= 2016\n",
    "x_clf_train = x_clf[train_mask_clf]\n",
    "x_clf_test = x_clf[~train_mask_clf]\n",
    "y_clf_train = y_clf[train_mask_clf]\n",
    "y_clf_test = y_clf[~train_mask_clf]\n",
    "\n",
    "print(\"Reg train size:\", x_reg_train.shape, \"Reg test size:\", x_reg_test.shape)\n",
    "print(\"Clf train size:\", x_clf_train.shape, \"Clf test size:\", x_clf_test.shape)"
   ],
   "id": "5a42c1be88bc3ea9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Decision Tree - Classification\n",
    "Costruiamo un **Decision Tree Classifier** per la classificazione di `label_variation`.\n",
    "- **Modello base**: `DecisionTreeClassifier(random_state=42)`.\n",
    "- Lo addestriamo su `(X_clf_train, y_clf_train)` e calcoliamo le predizioni su `X_clf_test`.\n",
    "- Stampiamo il **classification report** (precision, recall, F1, support) e la **confusion matrix** per valutare le prestazioni iniziali di un albero non potatore. "
   ],
   "id": "772c2db5c8c1b64d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepara i dati (X_clf_train/Test e y_clf_train/Test gi√† definiti)\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(x_clf_train, y_clf_train)\n",
    "\n",
    "# Predizione\n",
    "y_dt_clf_pred = dt_clf.predict(x_clf_test)\n",
    "\n",
    "print(\"Decision Tree Classificazione ‚Äì Report:\\n\", classification_report(y_clf_test, y_dt_clf_pred))"
   ],
   "id": "a85f63c6ac59d885",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pruned Decision Tree - Classification\n",
    "Partendo dall‚Äôalbero base, cerchiamo il **miglior valore di `ccp_alpha`** (parametro di potatura) che massimizza l‚ÄôF1‚Äêweighted sul test set:\n",
    "\n",
    "1. Otteniamo il **path di potatura** con `dt_clf.cost_complexity_pruning_path(X_clf_train, y_clf_train)`.\n",
    "2. Iteriamo su ogni `ccp_alpha` di quel path (escludendo l‚Äôultimo, che corrisponde a un albero di un solo nodo):\n",
    "   - Addestriamo un nuovo albero `DecisionTreeClassifier(ccp_alpha=ccp)`.\n",
    "   - Facciamo predizione su `X_clf_test` e calcoliamo l‚Äô`f1_score(average='weighted')`.\n",
    "3. Selezioniamo l‚Äô`alpha` con F1 pi√π alto.\n",
    "4. Confrontiamo l‚ÄôF1 dell‚Äôalbero potatore con l‚Äôalbero base.\n",
    "\n",
    "Alla fine salviamo il Decision Tree base e quello potatore su disco (joblib) per usi futuri (o meglio, andrebbe fatto ma non l'abbiamo fatto)."
   ],
   "id": "da51fb9980c1f82e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Trova percorso di ccp_alpha ottimale\n",
    "path_clf = dt_clf.cost_complexity_pruning_path(x_clf_train, y_clf_train)\n",
    "ccp_alphas_clf = path_clf.ccp_alphas[:-1]\n",
    "clf_trees = []\n",
    "f1_scores = []\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for ccp in ccp_alphas_clf:\n",
    "    dt = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp)\n",
    "    dt.fit(x_clf_train, y_clf_train)\n",
    "    y_pred = dt.predict(x_clf_test)\n",
    "    f1 = f1_score(y_clf_test, y_pred, average='weighted', zero_division=1)\n",
    "    clf_trees.append(dt)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "best_index_clf = np.argmax(f1_scores)\n",
    "best_alpha_clf = ccp_alphas_clf[best_index_clf]\n",
    "best_dt_clf = clf_trees[best_index_clf]\n",
    "print(f\"Pruned Tree Classificazione ‚Äì miglior ccp_alpha: {best_alpha_clf:.6f}, F1-weighted: {f1_scores[best_index_clf]:.3f}\")\n",
    "\n",
    "# Confronto con DT non potato\n",
    "base_f1 = f1_score(y_clf_test, y_dt_clf_pred, average='weighted', zero_division=1)\n",
    "print(f\"DT non potato F1-weighted: {base_f1:.3f}\")"
   ],
   "id": "bbf989f80b7614f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Confusion Matrix",
   "id": "206a770ee64f3a7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title='Matrice di Confusione'):\n",
    "    # Calcola la matrice di confusione\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Ottieni le classi uniche\n",
    "    classes = sorted(set(y_true))\n",
    "\n",
    "    # Crea la figura\n",
    "    plt.figure(figsize=(5, 4))\n",
    "\n",
    "    # Crea heatmap con seaborn\n",
    "    sns.heatmap(cm,\n",
    "                annot=True,  # Mostra i valori nelle celle\n",
    "                fmt='d',     # Formato numeri interi\n",
    "                cmap='Blues',  # Palette di colori\n",
    "                xticklabels=classes,\n",
    "                yticklabels=classes)\n",
    "\n",
    "    # Personalizza il grafico\n",
    "    plt.title(title, pad=20)\n",
    "    plt.xlabel('Predetto')\n",
    "    plt.ylabel('Reale')\n",
    "\n",
    "    # Aggiungi percentuali di accuratezza per ogni classe\n",
    "    accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "    for i, accuracy in enumerate(accuracies):\n",
    "        plt.text(-0.5, i, f'{accuracy:.1%}',\n",
    "                va='center',\n",
    "                ha='right',\n",
    "                fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizza la matrice per il Decision Tree originale\n",
    "plot_confusion_matrix(y_clf_test, y_dt_clf_pred,\n",
    "                     title='Matrice di Confusione - Decision Tree')\n",
    "\n",
    "# Visualizza la matrice per il Decision Tree potato\n",
    "plot_confusion_matrix(y_clf_test, best_dt_clf.predict(x_clf_test),\n",
    "                      title='Matrice di Confusione - Decision Tree Potato')"
   ],
   "id": "448a7a318d633844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Decision Tree - Regression\n",
    "Passiamo ora alla **regressione** per prevedere `target_cost`.\n",
    "1. Addestriamo un **Decision Tree Regressor** (`DecisionTreeRegressor(random_state=42)`) su `(X_reg_train, y_reg_train)`.\n",
    "2. Calcoliamo le predizioni `y_dt_pred` su `X_reg_test`.\n",
    "3. Valutiamo le metriche di regressione:\n",
    "   - **RMSE** (Root Mean Squared Error): `‚àö(MSE)`.\n",
    "   - **MAE** (Mean Absolute Error).\n",
    "   - **R¬≤** (coefficiente di determinazione).\n",
    "\n",
    "Stampiamo i risultati iniziali per l‚Äôalbero non potato."
   ],
   "id": "f98f8be2e7efca64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Prepara i dati (assumendo tu abbia X_reg_train/Test e y_reg_train/Test gi√† definiti)\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(x_reg_train, y_reg_train)\n",
    "\n",
    "# Predizioni\n",
    "y_dt_pred = dt_reg.predict(x_reg_test)\n",
    "\n",
    "# Metriche iniziali\n",
    "rmse_dt = mean_squared_error(y_reg_test, y_dt_pred)\n",
    "mae_dt  = mean_absolute_error(y_reg_test, y_dt_pred)\n",
    "r2_dt   = r2_score(y_reg_test, y_dt_pred)\n",
    "\n",
    "print(f\"Decision Tree Regressione ‚Äì RMSE: {rmse_dt:.3f}, MAE: {mae_dt:.3f}, R¬≤: {r2_dt:.3f}\")"
   ],
   "id": "ed150b7f3d834c9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pruned Decision Tree - Regression\n",
    "Proprio come per la classificazione, cerchiamo l‚Äô`alpha` di potatura che minimizza l‚ÄôMSE sul test set:\n",
    "\n",
    "1. Otteniamo il **path di cost complexity** con `dt_reg.cost_complexity_pruning_path(X_reg_train, y_reg_train)`.\n",
    "2. Iteriamo su ciascun `ccp_alpha` (escluso l‚Äôultimo), addestriamo un albero con `ccp_alpha=ccp`, facciamo predizione su `X_reg_test` e calcoliamo **MSE**.\n",
    "3. Selezioniamo `ccp_alpha_best` che minimizza l‚ÄôMSE.\n",
    "4. Addestriamo un nuovo `DecisionTreeRegressor(ccp_alpha=ccp_alpha_best)` su tutto il train e calcoliamo RMSE, MAE, R¬≤ sul test.\n",
    "5. Confrontiamo con i valori dell‚Äôalbero non potatore.\n",
    "6. Salviamo entrambi gli alberi (base e potato) per futuri utilizzi."
   ],
   "id": "6074695d9d90da03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Trova il percorso di ccp_alpha ottimale\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "path = dt_reg.cost_complexity_pruning_path(x_reg_train, y_reg_train)\n",
    "ccp_alphas = path.ccp_alphas[:-1]  # l‚Äôultimo crea un albero con un solo nodo\n",
    "reg_trees = []\n",
    "rmse_scores = []\n",
    "\n",
    "ccp_alphas = np.maximum(ccp_alphas, 0.0)\n",
    "\n",
    "for ccp in ccp_alphas:\n",
    "    dt = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp)\n",
    "    dt.fit(x_reg_train, y_reg_train)\n",
    "    y_pred = dt.predict(x_reg_test)\n",
    "    rmse_val = mean_squared_error(y_reg_test, y_pred)\n",
    "    reg_trees.append(dt)\n",
    "    rmse_scores.append(rmse_val)\n",
    "\n",
    "# Seleziona l‚Äôalpha che minimizza RMSE\n",
    "best_index = np.argmin(rmse_scores)\n",
    "best_alpha = ccp_alphas[best_index]\n",
    "best_dt = reg_trees[best_index]\n",
    "print(f\"Pruned Tree ‚Äì miglior ccp_alpha: {best_alpha:.6f}, RMSE: {rmse_scores[best_index]:.3f}\")\n",
    "\n",
    "# Confronto con il DT non potato\n",
    "print(f\"DT non potato RMSE: {rmse_dt:.3f}\")"
   ],
   "id": "51cdf79c45a8139c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Diagnostica di Regressione",
   "id": "92bea125c3a53579"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_regression_diagnostics(y_true, y_pred, title='Diagnostica Regressione'):\n",
    "    # Calcola i residui\n",
    "    residuals = y_true - y_pred\n",
    "\n",
    "    # Crea una figura con 2x2 subplot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "    fig.suptitle(title, fontsize=16, y=1.02)\n",
    "\n",
    "    # 1. Valori Predetti vs Valori Reali\n",
    "    axes[0,0].scatter(y_true, y_pred, alpha=0.5)\n",
    "    axes[0,0].plot([y_true.min(), y_true.max()],\n",
    "                   [y_true.min(), y_true.max()],\n",
    "                   'r--', lw=2)\n",
    "    axes[0,0].set_xlabel('Valori Reali')\n",
    "    axes[0,0].set_ylabel('Valori Predetti')\n",
    "    axes[0,0].set_title('Predetti vs Reali')\n",
    "\n",
    "    # 2. Distribuzione dei Residui\n",
    "    sns.histplot(residuals, kde=True, ax=axes[0,1])\n",
    "    axes[0,1].axvline(x=0, color='r', linestyle='--')\n",
    "    axes[0,1].set_xlabel('Residui')\n",
    "    axes[0,1].set_title('Distribuzione dei Residui')\n",
    "\n",
    "    # 3. Residui vs Valori Predetti\n",
    "    axes[1,0].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axes[1,0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[1,0].set_xlabel('Valori Predetti')\n",
    "    axes[1,0].set_ylabel('Residui')\n",
    "    axes[1,0].set_title('Residui vs Predetti')\n",
    "\n",
    "    # 4. Q-Q Plot dei Residui\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1,1])\n",
    "    axes[1,1].set_title('Q-Q Plot dei Residui')\n",
    "\n",
    "    # Aggiungi metriche di performance\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    metrics_text = f'RMSE: {rmse:.2f}\\nMAE: {mae:.2f}\\nR¬≤: {r2:.2f}'\n",
    "    fig.text(0.02, 0.98, metrics_text, fontsize=12,\n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizza diagnostica per il Decision Tree originale\n",
    "plot_regression_diagnostics(y_reg_test,\n",
    "                            dt_reg.predict(x_reg_test),\n",
    "                          'Diagnostica Regressione - Decision Tree')\n",
    "\n",
    "# Visualizza diagnostica per il Decision Tree potato\n",
    "plot_regression_diagnostics(y_reg_test,\n",
    "                            best_dt.predict(x_reg_test),\n",
    "                          'Diagnostica Regressione - Decision Tree Potato')"
   ],
   "id": "47d78a843538071a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decision Trees - Visualization",
   "id": "3a859f04ae1077b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Per la visualizzazione dei Decision Tree\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Decision Tree Classificazione\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt_clf,\n",
    "          feature_names=feature_cols,\n",
    "          class_names=sorted(y_clf_train.unique()),\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Decision Tree - Classificazione (Non Potato)\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Decision Tree Classificazione Potato\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(best_dt_clf,\n",
    "          feature_names=feature_cols,\n",
    "          class_names=sorted(y_clf_train.unique()),\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(f\"Decision Tree - Classificazione (Potato con ccp_alpha={best_alpha_clf:.6f})\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Decision Tree Regressione\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt_reg,\n",
    "          feature_names=feature_cols,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Decision Tree - Regressione (Non Potato)\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Decision Tree Regressione Potato\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(best_dt,\n",
    "          feature_names=feature_cols,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(f\"Decision Tree - Regressione (Potato con ccp_alpha={best_alpha:.6f})\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Grafico dell'andamento dell'errore in funzione di ccp_alpha\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ccp_alphas_clf, f1_scores, marker='o')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.ylabel('F1-score')\n",
    "plt.title('F1-score vs ccp_alpha (Classificazione)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 6. Grafico dell'andamento dell'errore in funzione di ccp_alpha per regressione\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ccp_alphas, rmse_scores, marker='o')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs ccp_alpha (Regressione)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "f8b6cda98889ac6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Random Forest - Regression\n",
    "Dopo aver valutato gli alberi, costruiamo un **Random Forest Regressor**:\n",
    "1. Creiamo il modello con parametri:\n",
    "   - `n_estimators`: 100\n",
    "   - `random state`: 42\n",
    "2. Calcoliamo le predizioni `y_reg_pred` su `X_reg_test` e quindi l‚Äô**RMSE**, **MAE**, **R¬≤** rispetto a `target_cost`.\n",
    "3. Stampiamo i risultati"
   ],
   "id": "d79c5bdadcfe4a78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Crea e addestra il modello\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(x_reg_train, y_reg_train)\n",
    "\n",
    "# 1. Fai le predizioni\n",
    "y_reg_pred = regressor.predict(x_reg_test)\n",
    "\n",
    "# 2. Calcola le metriche\n",
    "rmse = np.sqrt(mean_squared_error(y_reg_test, y_reg_pred))\n",
    "mae = mean_absolute_error(y_reg_test, y_reg_pred)\n",
    "r2 = r2_score(y_reg_test, y_reg_pred)\n",
    "\n",
    "print(f\"Regressione ‚Äì RMSE: {rmse:.3f}, MAE: {mae:.3f}, R¬≤: {r2:.3f}\")"
   ],
   "id": "2aa816bf279bff57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## SMOTE & Random Forest - Classification\n",
    "Per la **classificazione** su `label_variation` (aumento/stabile/diminuzione):\n",
    "1. Verifichiamo la **distribuzione delle classi** nel training set (`y_clf_train.value_counts(normalize=True)`).\n",
    "2. Costruiamo un **RandomForestClassifier** base (`n_estimators=100, random_state=42`).\n",
    "3. Addestriamo su `(X_clf_train, y_clf_train)` e calcoliamo predizioni su `X_clf_test`.\n",
    "4. Stampiamo il **classification report** e la **matrice di confusione**.\n",
    "5. Applichiamo **SMOTE** (oversampling) sul **solo** `X_clf_train, y_clf_train` per bilanciare le classi, creando `X_clf_train_balanced, y_clf_train_balanced`:\n",
    "   - Addestriamo un nuovo RF su `(X_clf_train_balanced, y_clf_train_balanced)`.\n",
    "   - Calcoliamo predizioni su `X_clf_test` e valutiamo nuovamente (report, confusion).\n",
    "6. Confrontiamo **accuracy** e **balanced accuracy** dei due modelli.\n",
    "\n",
    "In questo modo identifichiamo quale strategia di bilanciamento funziona meglio per le nostre classi sbilanciate."
   ],
   "id": "b2a36c960e498ee9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 1. Crea e addestra il modello\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(x_clf_train, y_clf_train)\n",
    "\n",
    "# 2. Predici sui dati di test\n",
    "y_clf_pred = clf.predict(x_clf_test)\n",
    "\n",
    "# 3. Calcola le metriche\n",
    "print(\"Classification Report:\\n\", classification_report(y_clf_test, y_clf_pred, zero_division=1))\n",
    "\n",
    "# Applica SMOTE solo al training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_clf_train_balanced, y_clf_train_balanced = smote.fit_resample(x_clf_train, y_clf_train)\n",
    "\n",
    "# Addestra il modello con i dati bilanciati\n",
    "clf_balanced = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_balanced.fit(X_clf_train_balanced, y_clf_train_balanced)\n",
    "\n",
    "# Fai predizioni e valuta\n",
    "y_clf_pred_balanced = clf_balanced.predict(x_clf_test)\n",
    "print(\"\\nReport di classificazione con SMOTE:\")\n",
    "print(classification_report(y_clf_test, y_clf_pred_balanced, zero_division=1))\n",
    "\n",
    "def plot_confusion_matrix_detailed(y_true, y_pred, title='Matrice di Confusione Dettagliata'):\n",
    "    # Calcola la matrice di confusione\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calcola le percentuali per ogni riga\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "    # Ottieni le classi uniche\n",
    "    classes = sorted(set(y_true))\n",
    "\n",
    "    # Crea la figura\n",
    "    plt.figure(figsize=(5, 4))\n",
    "\n",
    "    # Crea la heatmap principale\n",
    "    sns.heatmap(cm, annot=np.asarray([\n",
    "        [f'{count}\\n({percentage:.1f}%)'\n",
    "         for count, percentage in zip(row_counts, row_percentages)]\n",
    "        for row_counts, row_percentages in zip(cm, cm_percentage)\n",
    "    ]),\n",
    "                fmt='',\n",
    "                cmap='Blues',\n",
    "                xticklabels=classes,\n",
    "                yticklabels=classes,\n",
    "                cbar_kws={'label': 'Numero di campioni'})\n",
    "\n",
    "    # Aggiungi le etichette e il titolo\n",
    "    plt.title(title, pad=20, fontsize=14)\n",
    "    plt.xlabel('Classe Predetta', fontsize=12)\n",
    "    plt.ylabel('Classe Reale', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Valutiamo i modelli\n",
    "plot_confusion_matrix_detailed(y_clf_test, y_clf_pred, \"Modello Base\")\n",
    "plot_confusion_matrix_detailed(y_clf_test, y_clf_pred_balanced, \"Modello con SMOTE\")"
   ],
   "id": "e95533716168e564",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Importance\n",
    "Per capire quali feature hanno pi√π peso nella decisione del Random Forest Classifier, calcoliamo la **feature importance** per ogni modello:\n",
    "1. **`clf` (RF base)**\n",
    "2. **`clf_balanced` (RF + SMOTE)**\n",
    "\n",
    "Procedura:\n",
    "- Per ciascun modello, otteniamo `model.feature_importances_`, un array con importanza (da 0 a 1) per ciascuna feature nell‚Äôordine di `feature_cols`.\n",
    "- Ordiniamo le feature in base all‚Äôimportanza (decrescente).\n",
    "- Tracciamo un **bar plot** con `plt.bar()` per visualizzare graficamente quale feature √® pi√π rilevante.\n",
    "\n",
    "Questo ci permette di capire, ad esempio, se `pct_change_3` √® pi√π informativa di `OBS_VALUE` o `rolling_std_3`, e se l‚Äôaggiunta di SMOTE cambia la dinamica delle importanze."
   ],
   "id": "804d2314f1e280f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualizziamo le feature importance\n",
    "def plot_feature_importance(model, feature_names, title):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(f\"Feature Importance ({title})\")\n",
    "    plt.bar(range(x_clf_train.shape[1]), importances[indices])\n",
    "    plt.xticks(range(x_clf_train.shape[1]), [feature_names[i] for i in indices], rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizza feature importance per ogni modello\n",
    "plot_feature_importance(clf, feature_cols, \"Modello Base\")\n",
    "plot_feature_importance(clf_balanced, feature_cols, \"Modello con SMOTE\")"
   ],
   "id": "a5982481c8976d85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Predicted Models Export (.csv)\n",
    "A questo punto dovremmo aver creato e salvato i modelli (Decision Tree, Random Forest) per:\n",
    "\n",
    "- **Regressione**: previsione del costo t+1 (`pred_cost`).\n",
    "- **Classificazione**: previsione dell‚Äôetichetta `label_variation`.\n",
    "\n",
    "Questi sono i passaggi che abbiamo eseguito noi:\n",
    "- `df.to_csv(\"dataset_for_models.csv\")` salverebbe il dataset con tutte le feature utili.\n",
    "- `regression_df['pred_cost'] = regressor.predict(...)` calcola il costo predetto per ciascuna riga del test set; lo salviamo in `regression_results.csv`.\n",
    "- Per la classificazione, estraiamo le probabilit√† di ciascuna classe (`clf.predict_proba`), le aggiungiamo a `classification_output` e salviamo in `classification_results.csv`.\n",
    "\n",
    "In questo notebook di esempio lo facciamo per illustrare il formato, ma nella pipeline ‚Äúreale‚Äù questi passaggi sono eseguiti in uno script esterno tipo `train_and_infer.py`, costituito dai seguenti passaggi (ma noi siamo pigri e impediti e quindi non li abbiamo fatti):\n",
    "1. **Carichiamo i modelli serializzati** (`joblib.load()`).\n",
    "2. **Facciamo le predizioni** sul dataset di test (anno > 2016).\n",
    "3. **Salviamo** in `predictions/` i file CSV (uno per regressione, uno per classificazione) contenenti:\n",
    "   - `(geo, TIME_PERIOD, OBS_VALUE, target_cost, pred_cost)`\n",
    "   - `(geo, TIME_PERIOD, OBS_VALUE, label_variation, pred_label, prob_aumento, prob_diminuzione, prob_stabile)`\n",
    "4. **Stiliamo un breve log** per sapere dove e con quale timestamp √® stato salvato ciascun file."
   ],
   "id": "1c73ed0865e8c9cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Esporta dataset pulito con feature e target per regressione e classificazione\n",
    "df.to_csv(\"../data/csv/dataset_for_models.csv\", index=False)\n",
    "\n",
    "# 2) Esporta previsioni del modello di regressione (aggiungiamo una colonna con y_reg_pred)\n",
    "regression_df = regression_df.copy()\n",
    "regression_df['pred_cost'] = regressor.predict(regression_df[feature_cols]).round(1)\n",
    "regression_df[['geo', 'TIME_PERIOD', 'OBS_VALUE', 'pred_cost']].to_csv(\"../data/csv/regression_results.csv\", index=False)\n",
    "\n",
    "# 3) Esporta etichette di variazione (classification) con probabilit√†\n",
    "probs = clf.predict_proba(x_clf_test).round(1)\n",
    "classification_output = classification_df.loc[~train_mask_clf, ['geo', 'TIME_PERIOD', 'OBS_VALUE', 'label_variation']].copy()\n",
    "# Assumiamo che le classi siano ['aumento','diminuzione','stabile'] nell‚Äôordine di clf.classes_\n",
    "for idx, cls in enumerate(clf.classes_):\n",
    "    classification_output[f\"prob_{cls}\"] = probs[:, idx]\n",
    "classification_output.to_csv(\"../data/csv/classification_results.csv\", index=False)\n",
    "\n",
    "print(\"Dataset pulito con feature e target per regressione e classificazione:\")\n",
    "print(regression_df)"
   ],
   "id": "dd6f9fdbab1ee7ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Dataset con etichette di variazione (classification):\")\n",
    "print(classification_df)"
   ],
   "id": "e9a393f82f902321",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Proof of Concepts ‚Äì Time Series & Confronto Predizione/Reale\n",
    "Questa sezione dimostra la bont√† del modello di regressione **su un singolo paese di esempio (Italia, codice ‚ÄúIT‚Äù)** (cos√¨ a caso eh...):\n",
    "\n",
    "1. **Filtriamo** il DataFrame di test per ‚ÄúIT‚Äù (`regression_df[regression_df['geo']=='IT']`).\n",
    "2. **Ricreiamo** la colonna `pred_cost` tramite `regressor.predict(...)` per ciascun anno di test.\n",
    "3. **Disegniamo** uno `sns.lineplot` con due serie:\n",
    "   - **Costo reale** (asse ‚Äúanno vs OBS_VALUE‚Äù),\n",
    "   - **Costo predetto** (asse ‚Äúanno+1 vs pred_cost‚Äù), in modo da allineare i punti.\n",
    "\n",
    "Lo scopo √® mostrare se i valori reali e predetti ‚Äúcamminano‚Äù in modo simile, con un leggero ritardo di 1 anno (cosa naturale, dato che stiamo predicendo il costo t+1). SPOILER: √® **quasi** preciso."
   ],
   "id": "57f669fb596881ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Esempio: confronto tra COST reale e COST predetto per un dato paese (es. 'IT')\n",
    "it_df = regression_df[regression_df['geo'] == 'IT'].copy()\n",
    "it_df['year_pred'] = it_df['TIME_PERIOD'] + 1\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "# Costo reale in t\n",
    "sns.lineplot(x='TIME_PERIOD', y='OBS_VALUE', data=it_df, label='Costo reale')\n",
    "# Costo predetto allineato su year_pred (cio√® pred_cost in anno t)\n",
    "sns.lineplot(x='year_pred', y='pred_cost', data=it_df, label='Costo predetto')\n",
    "plt.title(\"Italia: Costo reale vs Costo predetto\")\n",
    "plt.xlabel(\"Anno\")\n",
    "plt.ylabel(\"Indice costo\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../report/images/Proof_of_Concepts(IT).png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "54182ad998f1c952",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Bootstrap Analysis\n",
    "\n",
    "Il **bootstrap** √® una tecnica di ri-campionamento non-parametrica che ci permette di stimare la variabilit√† e ottenere intervalli di confidenza per le nostre metriche senza assumere nulla sulla distribuzione dei dati originali.\n",
    "In pratica, per ciascuna replica:\n",
    "1. Si campiona con ripetizione il training set.\n",
    "2. Si riallena il modello sul campione bootstrap.\n",
    "3. Si calcola la metrica (es. RMSE o F1) sul test set fisso.\n",
    "\n",
    "Ripetendo questo processo molte volte (qui 200 iterazioni), otteniamo una distribuzione empirica della metrica e possiamo calcolare un intervallo di confidenza al 95%.  "
   ],
   "id": "bded6fd35a788b54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "\n",
    "def bootstrap_rmse(model_cls, x_train, y_train, x_test, y_test, n_boot=200):\n",
    "    rmses = []\n",
    "    for _ in range(n_boot):\n",
    "        xb, yb = resample(x_train, y_train, replace=True)\n",
    "        model = model_cls()\n",
    "        model.fit(xb, yb)\n",
    "        yh = model.predict(x_test)\n",
    "        mse = mean_squared_error(y_test, yh)  # <‚Äì MSE classico\n",
    "        rmses.append(np.sqrt(mse))  # <‚Äì radice per ottenere RMSE\n",
    "    return np.array(rmses)\n",
    "\n",
    "def bootstrap_f1(model_cls, x_train, y_train, x_test, y_test, n_boot=200):\n",
    "    f1s = []\n",
    "    for _ in range(n_boot):\n",
    "        xb, yb = resample(x_train, y_train, replace=True)\n",
    "        model = model_cls()\n",
    "        model.fit(xb, yb)\n",
    "        yh = model.predict(x_test)\n",
    "        f1s.append(f1_score(y_test, yh, average='weighted', zero_division=1))\n",
    "    return np.array(f1s)\n",
    "\n",
    "# 1) RMSE Bootstrap per RandomForestRegressor\n",
    "rmse_boot = bootstrap_rmse(\n",
    "    model_cls=lambda: RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    x_train=x_clf_train,\n",
    "    y_train=y_reg_train,\n",
    "    x_test=x_reg_test,\n",
    "    y_test=y_reg_test,\n",
    "    n_boot=200\n",
    ")\n",
    "ci_lower_rmse, ci_upper_rmse = np.percentile(rmse_boot, [2.5, 97.5])\n",
    "print(f\"RMSE bootstrap 95% CI: [{ci_lower_rmse:.2f}, {ci_upper_rmse:.2f}]\")\n",
    "\n",
    "# 2) F1-weighted Bootstrap per RandomForestClassifier\n",
    "f1_boot = bootstrap_f1(\n",
    "    model_cls=lambda: RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    x_train=x_clf_train,\n",
    "    y_train=y_clf_train,\n",
    "    x_test=x_clf_test,\n",
    "    y_test=y_clf_test,\n",
    "    n_boot=200\n",
    ")\n",
    "ci_lower_f1, ci_upper_f1 = np.percentile(f1_boot, [2.5, 97.5])\n",
    "print(f\"F1-weighted bootstrap 95% CI: [{ci_lower_f1:.3f}, {ci_upper_f1:.3f}]\")"
   ],
   "id": "de86f1af6aa3f7dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Istogramma per RMSE\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(rmse_boot, bins=20, edgecolor='black')\n",
    "plt.title(\"Distribuzione RMSE tramite Bootstrap\")\n",
    "plt.xlabel(\"RMSE\")\n",
    "plt.ylabel(\"Frequenza\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../report/images/RMSE_bootstrap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Istogramma per F1-weighted\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(f1_boot, bins=20, edgecolor='black')\n",
    "plt.title(\"Distribuzione F1-weighted tramite Bootstrap\")\n",
    "plt.xlabel(\"F1-weighted\")\n",
    "plt.ylabel(\"Frequenza\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../report/images/F1_bootstrap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "168bbd6617750b7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Decomposizione dell‚Äôerrore e validit√† clustering\n",
    "Per la **regressione** abbiamo utilizzato la Bias‚ÄìVarianza Decomposition (mlxtend) su Decision Tree e Random Forest.\n",
    "- Il MSE totale √® stato scomposto in Bias¬≤, Varianza e Rumore irreducibile, mostrando che il Random Forest riduce significativamente il bias rispetto al Decision Tree, mantenendo varianza moderata.\n",
    "\n",
    "Per la **classificazione** abbiamo fatto la stessa decomposizione (loss=0‚Äì1). Anche qui il Random Forest presenta un bias inferiore e una varianza pi√π contenuta rispetto al Decision Tree.\n",
    "\n",
    "Per il **clustering** (KMeans e DBSCAN) abbiamo calcolato tre indici di validit√† interna:\n",
    "1. **Silhouette Score** (compattezza/separazione),\n",
    "2. **Calinski‚ÄìHarabasz** (indice di varianza interna vs inter‚Äêcluster),\n",
    "3. **Davies‚ÄìBouldin** (indice di similarit√† tra cluster).\n",
    "\n",
    "In particolare, KMeans (k=2) ha ottenuto silhouette ‚âÉ 0.5, CH ‚âÉ 25 e DB ‚âÉ 0.9, confermando una buona separazione dei cluster; DBSCAN ha mostrato bassa validit√† a causa di un‚Äôalta percentuale di outlier."
   ],
   "id": "93f0d394616b5a92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Definiamo i modelli\n",
    "models_reg = {\n",
    "    'DT': DecisionTreeRegressor(random_state=42),\n",
    "    'RF': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_reg.items():\n",
    "    mse, bias_sq, var = bias_variance_decomp(\n",
    "        model,\n",
    "        x_reg_train.values, y_reg_train.values,\n",
    "        x_reg_test.values,  y_reg_test.values,\n",
    "        loss='mse',\n",
    "        num_rounds=30,\n",
    "        random_seed=42\n",
    "    )\n",
    "    print(f\"{name} Regression:\")\n",
    "    print(f\"  MSE total = {mse:.3f}\")\n",
    "    print(f\"  Bias¬≤     = {bias_sq:.3f}\")\n",
    "    print(f\"  Varianza  = {var:.3f}\")\n",
    "    print(f\"  Rumore    = {mse - bias_sq - var:.3f}\\n\")"
   ],
   "id": "f7304cc04fd76951",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Creiamo il codificatore per le labels di classificazione\n",
    "le = LabelEncoder()\n",
    "y_clf_train_enc = le.fit_transform(y_clf_train)\n",
    "y_clf_test_enc = le.transform(y_clf_test)\n",
    "\n",
    "# Verifichiamo la mappatura\n",
    "print(\"Labels Encoding:\")\n",
    "for orig, enc in zip(le.classes_, le.transform(le.classes_)):\n",
    "    print(f\"{orig} -> {enc}\")\n",
    "\n",
    "# Usiamo loss='0-1' per classificazione\n",
    "models_clf = {\n",
    "    'DT': DecisionTreeClassifier(random_state=42),\n",
    "    'RF': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_clf.items():\n",
    "    avg_err, bias, var = bias_variance_decomp(\n",
    "        model,\n",
    "        x_clf_train.values, y_clf_train_enc,\n",
    "        x_clf_test.values,  y_clf_test_enc,\n",
    "        loss='0-1_loss',    # classificazione\n",
    "        num_rounds=30,\n",
    "        random_seed=42\n",
    "    )\n",
    "    print(f\"\\n{name} Classification:\")\n",
    "    print(f\"  Errore 0-1 medio = {avg_err:.3f}\")\n",
    "    print(f\"  Bias            = {bias:.3f}\")\n",
    "    print(f\"  Varianza        = {var:.3f}\")"
   ],
   "id": "bea056da6524c296",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import numpy as np\n",
    "\n",
    "scores = {'KMeans_silhouette': silhouette_score(data_pca, labels),\n",
    "          'KMeans_calinski_harabasz': calinski_harabasz_score(data_pca, labels),\n",
    "          'KMeans_davies_bouldin': davies_bouldin_score(data_pca, labels)}\n",
    "\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ],
   "id": "8a0ae4cb0939d869",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusioni - Post Outliers Elimination\n",
    "1. **Regressione**\n",
    "L'eliminazione degli outlier ha avuto un impatto molto positivo sui modelli di regressione. Il Decision Tree ha visto un calo dell‚ÄôRMSE da 75 a 9, e la Random Forest da 6.1 a 2.4. Inoltre, l‚ÄôR¬≤ √® passato da negativo a positivo (0.071), segno che il modello ha iniziato a spiegare parte della varianza nei dati. Anche l'analisi bias-varianza conferma la riduzione dell‚Äôerrore complessivo e della varianza, soprattutto nel modello a foresta.\n",
    "2. **Classificazione**\n",
    "Dopo la rimozione degli outlier, la Random Forest ha migliorato la capacit√† di generalizzazione mantenendo basso il bias (0.263) e contenendo la varianza. Anche la media F1 dei modelli potati √® migliorata. Questo dimostra come l‚Äôoutlier elimination abbia aiutato a rendere pi√π rappresentativi i dati di training.\n",
    "3. **Clustering**\n",
    "La qualit√† del clustering KMeans si √® mantenuta pressoch√© costante (Silhouette intorno a 0.52‚Äì0.58), ma gli altri indici (Calinski-Harabasz e Davies-Bouldin) indicano lievi miglioramenti. DBSCAN continua a fallire nel trovare cluster significativi, suggerendo che il dataset non presenta strutture di densit√† nette, o che serve una migliore ottimizzazione dei parametri eps/min_samples o una diversa rappresentazione dei dati (feature o metriche).\n",
    "\n",
    "‚úÖ **Sintesi**\n",
    "- L‚Äôeliminazione degli outlier ha migliorato sensibilmente le prestazioni dei modelli supervisionati, soprattutto nella regressione.\n",
    "- La Random Forest si conferma un modello stabile e con varianza contenuta, mentre l‚Äôerrore totale resta principalmente causato da bias, suggerendo la necessit√† di migliorare le feature.\n",
    "- La qualit√† del clustering √® discreta ma non ottimale: algoritmi alternativi o una nuova trasformazione delle feature potrebbero migliorare la separabilit√†.\n",
    "- DBSCAN non risulta adatto alla struttura del dataset in uso."
   ],
   "id": "a24e19485fea16c8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
