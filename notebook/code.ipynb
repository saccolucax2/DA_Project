{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **ðŸ EU's Residential Buildings Costs - Data Analytics Campaign**\n",
    "Benvenuti in questo notebook: lâ€™obiettivo Ã¨ mostrare passo passo come abbiamo costruito, testato e automatizzato unâ€™intera pipeline di **Data Analytics** sui **costi di produzione dei nuovi edifici residenziali in Europa**, basandoci sui dati Eurostat.\n",
    "\n",
    "**Punti principali che affronteremo:**\n",
    "1. **Data Preparation & Feature Engineering**\n",
    "   - Lettura dei CSV Eurostat, espansione degli aggregati (EA19, EA20, EU27_2020), rimozione di outlier geografici.\n",
    "   - Costruzione di tutte le feature (rolling, pct_change, slope, ecc.) necessarie per i modelli.\n",
    "2. **Clustering non supervisionato (KMeans & DBSCAN)**\n",
    "   - Raggruppamento dei paesi in base allâ€™andamento delle variazioni percentuali sui costi.\n",
    "   - Valutazione con silhouette score e visualizzazione su PCA 2D e mappa europea.\n",
    "3. **Modelli Supervisionati (Regressione & Classificazione)**\n",
    "   - Previsione del â€œcostoâ€ dellâ€™anno successivo (regressione) e della â€œvariazioneâ€ (classificazione).\n",
    "   - Costruzione di Decision Tree (base e potati) e Random Forest (tuned), con metriche di valutazione (RMSE, RÂ², accuracy, F1, ecc.).\n",
    "4. **Output & Automazione**\n",
    "   - Salvataggio di modelli serializzati (joblib), metriche in JSON e previsioni in CSV.\n",
    "   - Generazione di grafici statici (PNG) e interattivi (HTML) in una cartella â€œreportsâ€.\n",
    "   - Spunti per schedulare il notebook in modalitÃ  automatizzata (cron, GitHub Actions, ecc.).\n",
    "\n",
    "In questo notebook troverete, prima di ogni blocco di codice, una spiegazione in italiano (e qualche dettaglio metodologico) per capire **cosa** stiamo facendo e **perchÃ©**. Buona lettura!"
   ],
   "id": "d3e7a8fece29910f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Flow Diagram\n",
    "![DFD Progetto](../report/images/Data_Flow_Diagram.png)\n",
    "\n",
    "#### 1. Data Ingestion & Expansion (P1)\n",
    "- **Input**: file CSV scaricato da Eurostat\n",
    "- **Logica**:\n",
    "  1. Selezione di tutte le righe con `indic_bt == 'COST'` e `unit == 'PCH_SM'`\n",
    "  2. Separazione delle righe di aggregati (`EA19`, `EA20`, `EU27_2020`) ed espansione in singoli paesi (AT, BE, â€¦)\n",
    "  3. Rimozione degli outlier geografici (`UA`, `TR`, `ME`, `RS`, `RO`)\n",
    "- **Output**: `DS1` (Expanded & Filtered Data)\n",
    "\n",
    "#### 2. Cleaning & Imputation (P2)\n",
    "- **Input**: `DS1`\n",
    "- **Logica**:\n",
    "  1. Rimozione delle colonne ridondanti (`DATAFLOW`, `LAST UPDATE`, `unit`, â€¦)\n",
    "  2. Imputazione iniziale dei valori mancanti con la media **globale** del dataset (per non perdere troppe righe)\n",
    "- **Output**: `DS2` (Cleaned Data)\n",
    "\n",
    "#### 3. Pivoting & Standardization (P3)\n",
    "- **Input**: `DS2`\n",
    "- **Logica**:\n",
    "  1. Creazione di `pivot_df = country Ã— year` con i valori di `OBS_VALUE`\n",
    "  2. Trasposizione (`data = pivot_df.T`) e rimozione delle righe completamente vuote\n",
    "  3. Imputazione rigaâ€‘perâ€‘riga con `fillna(row.mean())` e drop delle eventuali righe residue con NaN\n",
    "  4. Applicazione di `StandardScaler` per ottenere `data_scaled`\n",
    "- **Output**: `DS3` (Standardized Pivot Table)\n",
    "\n",
    "#### 4. Feature Engineering (P4)\n",
    "- **Input**: `DS2`\n",
    "- **Logica**:\n",
    "  1. `target_cost = OBS_VALUE.shift(-1)` per la regressione\n",
    "  2. Calcolo di `prev_cost`, `var_perc` (sostituendo Â±âˆž con 0) e creazione di `label_variation` con soglia Â±20%\n",
    "  3. Computazione di rolling features: `rolling_mean_3`, `rolling_std_3`, `pct_change_3`, `slope_3`, `grew_last_year`\n",
    "- **Output**: `DS4` (Feature Store)\n",
    "\n",
    "#### 5. Clustering (P5)\n",
    "- **Input**: `DS3` (in particolare `data_scaled`)\n",
    "- **Logica**:\n",
    "  1. Riduzione dimensionale con `PCA(n_components=3)`\n",
    "  2. **KMeans**: ricerca di _k_ ottimale tramite silhouette score â†’ cluster â€œsfericiâ€\n",
    "  3. **DBSCAN**: analisi del _k-distance graph_ + gridâ€‘search su `eps` e `min_samples` â†’ cluster basati sulla densitÃ  e rilevazione outlier\n",
    "- **Output**: `DS5` (Cluster Assignments)\n",
    "\n",
    "#### 6. Modeling (P6)\n",
    "- **Input**: `DS4` â†’ i dataset `(X_reg, y_reg, X_clf, y_clf)` e split train/test basato su `TIME_PERIOD` (anni â‰¤â€¯2016 vs >â€¯2016)\n",
    "- **Logica**:\n",
    "  1. **Decision Tree** (base + pruned) per regressione e classificazione\n",
    "  2. **Random Forest** (base, tuning di iperparametri, SMOTE applicato solo al _training_ set di classificazione, `class_weight`)\n",
    "  3. Decomposizione **Biasâ€“Varianza** (`bias_variance_decomp`) per RF e DT\n",
    "- **Output**: `DS6` (Trained Models)\n",
    "\n",
    "#### 7. Evaluation & Reporting (P7)\n",
    "- **Input**: `DS5` + `DS6`\n",
    "- **Logica**:\n",
    "  1. Calcolo delle **metriche**:\n",
    "     - Clustering: silhouette score, Calinskiâ€“Harabasz, Daviesâ€“Bouldin\n",
    "     - Regressione: RMSE, MAE, RÂ²\n",
    "     - Classificazione: precision, recall, F1, accuracy, balanced accuracy\n",
    "  2. Generazione di **visualizzazioni**: heatmap, scatterâ€¯PCA, mappeâ€¯choropleth, lineâ€‘plot reale vs predetto, boxplot, barplot delle feature importance, kâ€‘distance graph\n",
    "  3. Compilazione del **report testuale** (CRISPâ€‘DM, ML Canvas, DFD, risultati, discussione, conclusioni)\n",
    "- **Output**: `DS7` (Reports & Visualizations)\n",
    "\n",
    "### Distribuzione agli Stakeholder\n",
    "- **Output finale**:\n",
    "  - Report in PDF o PowerPoint\n",
    "  - Jupyter Notebook completamente documentato\n",
    "  - Modelli serializzati (pickle/joblib)\n",
    "  - CSV con risultati e predizioni\n",
    "  - Script di pipeline pronto per schedulazione"
   ],
   "id": "aad564534a501dc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Preparation and Dataset Split\n",
    "In questa prima sezione prepariamo i dati di base:\n",
    "\n",
    "1. **Import delle librerie necessarie**,\n",
    "2. **Lettura del CSV â€œgrezzoâ€** (dataset Eurostat),\n",
    "3. **Split preliminare** dei dati per eventuali usi successivi.\n",
    "\n",
    "Lâ€™idea Ã¨ di creare un punto di partenza â€œpulitoâ€: un DataFrame che contenga tutte le righe â€œpaeseâ€annoâ€ di nostro interesse, pronto per le fasi di **feature engineering** e **clustering**.\n",
    "\n",
    "Di seguito:\n",
    "- Caricheremo il file `dataset.csv` dalla cartella `data/raw` (o da un URL, se disponibile).\n",
    "- Verificheremo che non ci siano record duplicati o righe outlier (in questa fase limitiamoci a leggere e dare una prima occhiata alle colonne).  "
   ],
   "id": "16fbbe096992dcc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:13:46.581475Z",
     "start_time": "2025-06-26T09:13:46.551262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# Carica il dataset\n",
    "dataset = pd.read_csv(r\"../data/raw/dataset.csv\")"
   ],
   "id": "f34c98759f3112ef",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 Import delle librerie e caricamento del CSV\n",
    "\n",
    "- `os` serve per eventuali operazioni di sistema (es. variabili dâ€™ambiente).\n",
    "- `pandas` Ã¨ il pacchetto principale per il caricamento e la manipolazione dei DataFrame.\n",
    "- Impostiamo `os.environ['OMP_NUM_THREADS']='1'` per limitare al singolo thread alcune operazioni BLAS/NumPy, in modo da evitare avvisi di parallellismo eccessivo.\n",
    "\n",
    "Infine, leggiamo il CSV principale (`dataset.csv`) che contiene i dati **Eurostat** sui costi di produzione di nuovi edifici residenziali (annuale) per ciascun paese/anno."
   ],
   "id": "2a3e25347ccf6a9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:13:46.794580Z",
     "start_time": "2025-06-26T09:13:46.643274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Splitta il dataset per Business Tendency Indicator (Cost)\n",
    "df = dataset[dataset['indic_bt'] == 'COST']\n",
    "\n",
    "# Prima rimuoviamo le righe dove unit Ã¨ I15 o I21\n",
    "df = df[~df['unit'].isin(['I15', 'I21'])]\n",
    "\n",
    "# Mappiamo le sigle aggregate dividendole nei singoli paesi:\n",
    "mapping_aggregati = {\n",
    "    'EA19': ['AT','BE','CY','EE','FI','FR','DE','GR','IE','IT','LV','LT','LU','MT','NL','PT','SK','SI','ES'],\n",
    "    'EA20': ['AT','BE','CY','HR','EE','FI','FR','DE','GR','IE','IT','LV','LT','LU','MT','NL','PT','SK','SI','ES'],\n",
    "    'EU27_2020': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE']\n",
    "}\n",
    "\n",
    "# 1) Separiamo le righe aggregate\n",
    "aggregati_da_espandere = set(mapping_aggregati.keys())\n",
    "df_agg = df[df['geo'].isin(aggregati_da_espandere)].copy()\n",
    "df_rest = df[~df['geo'].isin(aggregati_da_espandere)].copy()\n",
    "\n",
    "# 2) costruiamo il set di chiavi esistenti per paesi singoli\n",
    "existing_keys = set(zip(df_rest['geo'], df_rest['TIME_PERIOD']))\n",
    "\n",
    "# 2) Creiamo un DataFrame vuoto per le righe espanse\n",
    "df_expanded = []\n",
    "\n",
    "# 3) Per ogni riga in df_agg, generiamo copie per ciascun membro\n",
    "for idx, row in df_agg.iterrows():\n",
    "    codice_agg = row['geo']\n",
    "    anno = row['TIME_PERIOD']\n",
    "    paesi_membri = mapping_aggregati[codice_agg]\n",
    "\n",
    "    for iso2 in paesi_membri:\n",
    "         if (iso2, anno) in existing_keys:\n",
    "            continue\n",
    "         nuova_riga = row.copy()\n",
    "         nuova_riga['geo'] = iso2\n",
    "         nuova_riga['OBS_VALUE'] = round(nuova_riga['OBS_VALUE'], 1)\n",
    "         df_expanded.append(nuova_riga)\n",
    "\n",
    "# 4) Concateniamo df_rest e tutte le righe espanse in df_expanded\n",
    "df_expanded = pd.DataFrame(df_expanded)\n",
    "df_final = pd.concat([df_rest, df_expanded], ignore_index=True)\n",
    "\n",
    "# df_final ora contiene tutte le righe non aggregate + copie per paesi singoli\n",
    "df = df_final.copy()\n",
    "\n",
    "# Rimuovere gli Outliers\n",
    "df = df[~df['geo'].isin(['UA', 'TR', 'ME', 'RS', 'RO'])]\n",
    "\n",
    "# Elimina le colonne vuote o rindondanti\n",
    "df = df.drop(columns=['DATAFLOW', 'LAST UPDATE', 's_adj', 'indic_bt', 'freq', 'cpa2_1', 'CONF_STATUS', 'OBS_FLAG', 'unit'])\n",
    "df = df.fillna(dataset.mean(numeric_only=True))\n",
    "\n",
    "# Approssima OBS_VALUE a una cifra dopo la virgola\n",
    "df['OBS_VALUE'] = df['OBS_VALUE'].round(1)\n",
    "\n",
    "print(df.head(10))"
   ],
   "id": "47c2c4832770344a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  geo  TIME_PERIOD  OBS_VALUE\n",
      "0  AL         2003        2.6\n",
      "1  AL         2004        3.0\n",
      "2  AL         2005        1.4\n",
      "3  AL         2006        1.1\n",
      "4  AL         2007        2.5\n",
      "5  AL         2008        1.2\n",
      "6  AL         2009        0.0\n",
      "7  AL         2010        0.3\n",
      "8  AL         2011        0.5\n",
      "9  AL         2012        0.6\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Creazione di una tabella pivot iniziale (paesi Ã— anni)\n",
    "\n",
    "âœ… **Scopo**: avere immediatamente sottâ€™occhio la struttura â€œpaese Ã— annoâ€ di `OBS_VALUE` (il valore dellâ€™indice dei costi).\n",
    "- `pivot_df.index` = serie temporali (`TIME_PERIOD`, ovvero gli anni 2000â€“2024).\n",
    "- `pivot_df.columns` = codici â€œgeoâ€ (paesi e aggregati Eurostat).\n",
    "- `pivot_df.values` = i valori numerici di `OBS_VALUE` (lâ€™indice vero e proprio).\n",
    "\n",
    "Nei prossimi passi manterremo questo formato per operare imputazioni, transizioni verso clustering e cosÃ¬ via."
   ],
   "id": "acac4b3b9adec81f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:13:47.674946Z",
     "start_time": "2025-06-26T09:13:47.610351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Crea la tabella pivot\n",
    "pivot_df = df.pivot_table(index='TIME_PERIOD', columns='geo', values='OBS_VALUE')\n",
    "\n",
    "# Riapprossima perchÃ¨ Ã¨ coglione\n",
    "pivot_df = pivot_df.round(1)\n",
    "\n",
    "print(pivot_df)"
   ],
   "id": "72ee2931a3b07766",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geo           AL    AT    BE    BG    CY    CZ    DE    DK    EE   EL  ...  \\\n",
      "TIME_PERIOD                                                            ...   \n",
      "1981         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1982         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1983         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1984         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1985         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1986         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1987         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1988         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1989         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1990         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1991         NaN   4.4   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1992         NaN   4.9   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1993         NaN   4.7   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1994         NaN   3.4   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1995         NaN   3.5   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  ...   \n",
      "1996         NaN   1.6   NaN   NaN   NaN   NaN   NaN   2.6   NaN  NaN  ...   \n",
      "1997         NaN   2.8   NaN   NaN   NaN   NaN   NaN   2.5   NaN  NaN  ...   \n",
      "1998         NaN   2.1   NaN   NaN   NaN   NaN   NaN   3.0   NaN  NaN  ...   \n",
      "1999         NaN   2.1   NaN   NaN  -0.2   NaN   NaN   3.2   2.2  NaN  ...   \n",
      "2000         NaN   2.2   NaN   NaN   4.4   NaN   NaN   2.6   2.4  NaN  ...   \n",
      "2001         NaN   2.2   1.7   2.0   3.6   3.5   0.6   3.2   5.6  2.7  ...   \n",
      "2002         NaN   1.4   1.9   1.9   3.9   2.5   0.9   2.2   4.0  2.5  ...   \n",
      "2003         2.6   2.8   1.8   2.0   5.4   2.0   1.1   2.3   3.5  2.8  ...   \n",
      "2004         3.0   5.1   3.1   2.9   7.2   8.3   2.6   2.1   5.2  3.1  ...   \n",
      "2005         1.4   2.1   2.5   8.5   4.5   3.7   1.5   2.3   6.3  3.4  ...   \n",
      "2006         1.1   4.6   3.1   6.1   5.1   2.2   2.5   4.7  10.4  4.3  ...   \n",
      "2007         2.5   4.5   4.1   6.3   5.0   4.8   3.3   6.4  12.6  4.6  ...   \n",
      "2008         1.2   5.2   3.8  12.1   7.9   3.5   3.4   2.9   3.5  5.2  ...   \n",
      "2009         0.0   0.7   0.4   7.2   0.9  -0.3   0.3  -0.5  -8.3 -0.4  ...   \n",
      "2010         0.3   3.1   1.9  -1.5   3.1   1.0   2.3   1.2  -2.7  1.9  ...   \n",
      "2011         0.5   2.4   3.2   0.5   3.5   1.8   3.7   3.6   3.4  1.0  ...   \n",
      "2012         0.6   2.1   1.9  -0.7   0.9   0.5   2.0   2.6   4.5 -0.1  ...   \n",
      "2013         0.9   1.9   0.7   1.4  -4.0  -0.4   0.7   1.5   4.9 -1.3  ...   \n",
      "2014         0.3   1.1   0.6   0.5  -1.8   1.1   0.9   1.6   0.3 -3.0  ...   \n",
      "2015         0.2   1.5   0.9   1.2  -0.8   0.1   1.3   1.9   0.8 -2.3  ...   \n",
      "2016         0.1   0.7   0.9   0.6  -0.8   0.2   1.3   1.5  -0.5 -1.7  ...   \n",
      "2017         0.6   3.5   2.5   2.7   0.1   2.7   3.5   0.8   1.3  0.2  ...   \n",
      "2018         0.6   2.9   2.9   2.7   1.2   4.0   3.6   1.8   1.8  0.5  ...   \n",
      "2019         0.2   1.1   1.9   5.3   2.2   5.5   2.2   0.7   1.9  0.2  ...   \n",
      "2020         0.2   0.8   1.4   2.2   0.2   2.1   2.0   0.7   0.4  0.1  ...   \n",
      "2021         1.9  10.5   6.4  11.4   7.6   8.3   6.8   4.0   9.4  3.2  ...   \n",
      "2022         6.4  10.1  12.2  54.8  11.7  20.4  13.0  10.0  18.2  8.8  ...   \n",
      "2023         4.5   1.1   3.4  16.3   2.5   4.4   3.5   4.5   5.2  6.3  ...   \n",
      "2024         2.2   3.7   NaN   4.0   0.9   0.5   1.6   1.6   1.5  3.6  ...   \n",
      "\n",
      "geo            LV    MK    MT   NL   NO    PL    PT    SE    SI    SK  \n",
      "TIME_PERIOD                                                            \n",
      "1981          NaN   NaN   NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1982          NaN   NaN   NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1983          NaN   NaN   NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1984          NaN   NaN   NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1985          NaN   NaN   NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1986          NaN   NaN   NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1987          NaN   NaN   NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1988          NaN   NaN   NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1989          NaN   NaN   NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1990          NaN   NaN   NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1991          NaN   NaN   NaN  NaN  NaN   NaN   NaN   6.0   NaN   NaN  \n",
      "1992          NaN   NaN   NaN  NaN  NaN   NaN   NaN   0.7   NaN   NaN  \n",
      "1993          NaN   NaN   NaN  NaN  NaN   NaN   NaN   0.5   NaN   NaN  \n",
      "1994          NaN   NaN   NaN  NaN  NaN   NaN   NaN   2.4   NaN   NaN  \n",
      "1995          NaN   NaN   NaN  NaN  NaN   NaN   NaN   5.5   NaN   NaN  \n",
      "1996          NaN   NaN   NaN  NaN  NaN   NaN   NaN   3.6   NaN   NaN  \n",
      "1997          NaN   NaN   NaN  NaN  NaN   NaN   NaN   2.4   NaN   NaN  \n",
      "1998          NaN   NaN   NaN  NaN  NaN   NaN   NaN   2.8   NaN   NaN  \n",
      "1999          NaN   NaN   NaN  NaN  NaN   NaN   NaN   2.5   6.4   NaN  \n",
      "2000          NaN   NaN   NaN  NaN  NaN   NaN   NaN   4.0   8.3   NaN  \n",
      "2001         -2.2   NaN   6.4  4.2  4.7   2.0   0.2   4.5  11.1   1.7  \n",
      "2002          1.3   NaN   4.9  3.0  3.3   1.9   2.8   3.3   3.3   1.9  \n",
      "2003          6.3   NaN   2.3  1.3  3.2   2.0   1.2   3.4   5.9   1.8  \n",
      "2004          8.8   NaN   0.5  0.7  2.9   3.2   4.1   4.0   7.7   3.1  \n",
      "2005         11.9   NaN   1.4  1.4  3.5   2.5   2.0   3.8   4.6   2.5  \n",
      "2006         36.5   9.1   1.3  3.1  3.8   3.3   3.3   5.0   6.1   3.1  \n",
      "2007         22.5   2.4  10.4  4.1  7.3   4.2   3.4   6.1   6.9   4.1  \n",
      "2008          8.7   4.7   2.4  4.6  5.7   4.0   6.3   4.9   6.6   3.8  \n",
      "2009         -7.8   5.9   1.6  0.3  2.4   0.4  -2.2   1.9  -3.1   0.4  \n",
      "2010         -7.4  -2.3  -2.4  0.4  3.1   1.8   1.6   2.6   5.6   1.9  \n",
      "2011          2.5   5.2   1.4  1.9  3.6   3.3   1.4   3.0   4.7   3.2  \n",
      "2012          3.2   1.7   2.3  2.3  3.1   2.0   0.6   2.6  -1.2   1.9  \n",
      "2013          4.0   1.9   1.5  0.2  3.0   0.7   1.3   1.6  -1.1   0.7  \n",
      "2014          0.7  -0.4   2.5  0.8  3.3   0.7   0.7   0.9  -0.5   0.6  \n",
      "2015          3.4  -1.6   1.1  1.4  2.5   0.9   1.2   2.3   0.6   0.9  \n",
      "2016          5.7  -2.4   2.3  1.6  2.9   1.1   1.2   2.1  -1.1   0.9  \n",
      "2017          2.4   5.0   1.6  2.1  2.4   2.6   1.9   2.7   4.7   2.5  \n",
      "2018          4.5   6.0   1.2  2.6  3.2   2.9   2.3   4.0   2.7   2.9  \n",
      "2019          4.8   2.6   1.0  2.9  2.6   2.0   2.3   2.7   3.1   1.9  \n",
      "2020          7.3   1.1   0.3  2.1  2.4   1.4   2.1  -0.1   1.2   1.4  \n",
      "2021          6.8  13.1  12.5  4.5  8.7   6.5   6.5   6.3  10.7   6.4  \n",
      "2022         13.3  23.7  19.1  9.1  8.8  12.4  12.2  12.3  14.4  12.2  \n",
      "2023          4.7   9.4  -0.3  5.8  4.4   3.6   3.8   4.2   7.3   3.4  \n",
      "2024          5.6  -7.2   0.8  2.9  3.7   NaN   3.3   1.5   3.7   NaN  \n",
      "\n",
      "[44 rows x 30 columns]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Imputation and Standardization\n",
    "Prima di procedere con lâ€™analisi non supervisionata (clustering), Ã¨ fondamentale:\n",
    "\n",
    "1. **Rimuovere le righe (paesi) completamente vuote** (se un paese non ha mai pubblicato dati).\n",
    "2. **Imputare i valori mancanti** (NaN) con la **media** di ciascuna riga (ciÃ² significa: â€œper ogni paese, sostituisci i NaN con la media dei valori disponibili in quellâ€™annoâ€serieâ€).\n",
    "3. **Standardizzare** (Zâ€score) le serie, in modo da rendere le variabili confrontabili e garantire che il clustering non sia dominato da scale diverse.\n",
    "\n",
    "Di seguito:\n",
    "- Trasponiamo `pivot_df` per passare da â€œanno Ã— paeseâ€ a â€œpaese Ã— annoâ€.\n",
    "- Riempiamo i NaN con la media riga per riga (`row.mean()`).\n",
    "- Eliminiamo ancora eventuali paesi che restano tutti NaN (criterio di sicurezza).\n",
    "- Applichiamo lo `StandardScaler()` di scikitâ€learn (media 0, varianza 1) a tutta la matrice finale."
   ],
   "id": "ebb2c799c395bc48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:13:48.283279Z",
     "start_time": "2025-06-26T09:13:47.974951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 1. Trasposizione\n",
    "data = pivot_df.T\n",
    "\n",
    "# 2. Rimuove righe completamente vuote\n",
    "data = data.dropna(how='all')\n",
    "\n",
    "# 3. Imputazione: riempi i NaN con la media di ogni riga\n",
    "data_filled = data.apply(lambda column: column.fillna(column.mean().round(1)), axis=1)\n",
    "\n",
    "# 4. Rimuovi righe che ancora hanno tutti NaN (es. Media era NaN)\n",
    "data_filled = data_filled.dropna()\n",
    "\n",
    "# 7. Standardizzazione\n",
    "data_scaled = StandardScaler().fit_transform(data_filled)\n",
    "\n",
    "print(data_filled.head(10))"
   ],
   "id": "2d79b1cc9603c166",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME_PERIOD  1981  1982  1983  1984  1985  1986  1987  1988  1989  1990  ...  \\\n",
      "geo                                                                      ...   \n",
      "AL            1.4   1.4   1.4   1.4   1.4   1.4   1.4   1.4   1.4   1.4  ...   \n",
      "AT            3.1   3.1   3.1   3.1   3.1   3.1   3.1   3.1   3.1   3.1  ...   \n",
      "BE            2.7   2.7   2.7   2.7   2.7   2.7   2.7   2.7   2.7   2.7  ...   \n",
      "BG            6.3   6.3   6.3   6.3   6.3   6.3   6.3   6.3   6.3   6.3  ...   \n",
      "CY            2.9   2.9   2.9   2.9   2.9   2.9   2.9   2.9   2.9   2.9  ...   \n",
      "CZ            3.4   3.4   3.4   3.4   3.4   3.4   3.4   3.4   3.4   3.4  ...   \n",
      "DE            2.7   2.7   2.7   2.7   2.7   2.7   2.7   2.7   2.7   2.7  ...   \n",
      "DK            2.7   2.7   2.7   2.7   2.7   2.7   2.7   2.7   2.7   2.7  ...   \n",
      "EE            3.8   3.8   3.8   3.8   3.8   3.8   3.8   3.8   3.8   3.8  ...   \n",
      "EL            1.9   1.9   1.9   1.9   1.9   1.9   1.9   1.9   1.9   1.9  ...   \n",
      "\n",
      "TIME_PERIOD  2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  \n",
      "geo                                                                      \n",
      "AL            0.2   0.1   0.6   0.6   0.2   0.2   1.9   6.4   4.5   2.2  \n",
      "AT            1.5   0.7   3.5   2.9   1.1   0.8  10.5  10.1   1.1   3.7  \n",
      "BE            0.9   0.9   2.5   2.9   1.9   1.4   6.4  12.2   3.4   2.7  \n",
      "BG            1.2   0.6   2.7   2.7   5.3   2.2  11.4  54.8  16.3   4.0  \n",
      "CY           -0.8  -0.8   0.1   1.2   2.2   0.2   7.6  11.7   2.5   0.9  \n",
      "CZ            0.1   0.2   2.7   4.0   5.5   2.1   8.3  20.4   4.4   0.5  \n",
      "DE            1.3   1.3   3.5   3.6   2.2   2.0   6.8  13.0   3.5   1.6  \n",
      "DK            1.9   1.5   0.8   1.8   0.7   0.7   4.0  10.0   4.5   1.6  \n",
      "EE            0.8  -0.5   1.3   1.8   1.9   0.4   9.4  18.2   5.2   1.5  \n",
      "EL           -2.3  -1.7   0.2   0.5   0.2   0.1   3.2   8.8   6.3   3.6  \n",
      "\n",
      "[10 rows x 44 columns]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\n",
    "Prima di fare qualsiasi clustering o training supervisionato, calcoliamo tutte le feature derivate (rolling, variazioni percentuali, slope, ecc.) per ogni (paese, anno).\n",
    "\n",
    "**Passaggi principali:**\n",
    "\n",
    "1. **Ordinare i dati** per `(geo, TIME_PERIOD)` in modo da avere serie ordinate di anno in anno.\n",
    "2. **Target per regressione (`target_cost`)**:\n",
    "   - `target_cost = OBS_VALUE` dellâ€™anno successivo, calcolato con `groupby('geo')['OBS_VALUE'].shift(-1)`.\n",
    "   - Rimuoviamo le righe in cui `target_cost` Ã¨ NaN (ultimo anno di ciascun paese), perchÃ© non possiamo fare previsione oltre lâ€™ultimo dato disponibile.\n",
    "3. **Variazione percentuale annoâ€suâ€anno (`var_perc`)**:\n",
    "   - Calcoliamo `prev_cost = lag(OBS_VALUE)`.\n",
    "   - `var_perc = 100 * (OBS_VALUE - prev_cost) / prev_cost`.\n",
    "   - Sostituiamo i valori infiniti con 0 (`.replace([np.inf, -np.inf], 0.0)`), poi riempiamo i NaN con 0.\n",
    "   - Creiamo `label_variation` in base a una soglia Â±20%:\n",
    "     - `\"aumento\"` se `var_perc > 20%`,\n",
    "     - `\"diminuzione\"` se `var_perc < -20%`,\n",
    "     - `\"stabile\"` altrimenti.\n",
    "4. **Rolling e deviazione standard su 3 e 5 anni**:\n",
    "   - `rolling_mean_3`: media mobile 3â€anni incluso lâ€™anno corrente (`.rolling(window=3, min_periods=1).mean()`).\n",
    "   - `rolling_mean_5`: media mobile 5â€anni.\n",
    "   - `rolling_std_3`: deviazione standard mobile 3â€anni.\n",
    "   - `pct_change_3`: variazione percentuale rispetto a 3 anni prima (`.pct_change(periods=3)`).\n",
    "   - Sostituiamo infiniti con NaN e quindi `fillna(0.0)`.\n",
    "5. **Feature binaria â€œcrescitaâ€**:\n",
    "   - `grew_last_year = 1` se `var_perc > 0`, altrimenti `0`.\n",
    "6. **Slope locale su 3 anni**:\n",
    "   - Definiamo `slope_3` come la pendenza (coefficiente angolare) di una regressione lineare sui 3 anni precedenti.\n",
    "   - Usiamo `np.polyfit([0,1,2], last_3_values, 1)` dentro a una `transform(lambda x: x.expanding().apply(...))` per calcolare la pendenza dellâ€™ultimo triennio, cumulativa.\n",
    "\n",
    "Al termine, ogni riga del DataFrame contiene:\n",
    "- `OBS_VALUE` (valore indice corrente),\n",
    "- `target_cost` (valore indice t+1),\n",
    "- `prev_cost`, `var_perc`, `label_variation`,\n",
    "- `rolling_mean_3`, `rolling_mean_5`, `rolling_std_3`, `pct_change_3`,\n",
    "- `grew_last_year`, `slope_3`.\n",
    "\n",
    "Queste feature verranno poi usate sia per il clustering sia per i modelli di regressione e classificazione supervisionata.\n"
   ],
   "id": "e9a89ae702730655"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:13:48.419816Z",
     "start_time": "2025-06-26T09:13:48.398901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prima, ordina per paese e anno\n",
    "df = df.sort_values(by=['geo', 'TIME_PERIOD']).reset_index(drop=True)\n",
    "\n",
    "# Creiamo un DataFrame con costi per paese e anno\n",
    "# Raggruppiamo per 'geo' e poi applichiamo shift(-1) su OBS_VALUE per avere il valore dellâ€™anno successivo\n",
    "df['target_cost'] = df.groupby('geo')['OBS_VALUE'].shift(-1)\n",
    "\n",
    "# Rimuovi righe dove target_cost Ã¨ NaN (ultimo anno di ciascun paese)\n",
    "df = df.dropna(subset=['target_cost'])\n",
    "\n",
    "print(df[['geo', 'TIME_PERIOD', 'OBS_VALUE', 'target_cost']].head(10))"
   ],
   "id": "600db4ee614d30dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  geo  TIME_PERIOD  OBS_VALUE  target_cost\n",
      "0  AL         2003        2.6          3.0\n",
      "1  AL         2004        3.0          1.4\n",
      "2  AL         2005        1.4          1.1\n",
      "3  AL         2006        1.1          2.5\n",
      "4  AL         2007        2.5          1.2\n",
      "5  AL         2008        1.2          0.0\n",
      "6  AL         2009        0.0          0.3\n",
      "7  AL         2010        0.3          0.5\n",
      "8  AL         2011        0.5          0.6\n",
      "9  AL         2012        0.6          0.9\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:13:48.557555Z",
     "start_time": "2025-06-26T09:13:48.533056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calcola la variazione percentuale anno su anno\n",
    "df['prev_cost'] = df.groupby('geo')['OBS_VALUE'].shift(1)\n",
    "df['var_perc'] = (df['OBS_VALUE'] - df['prev_cost']) / df['prev_cost'] * 100.0\n",
    "\n",
    "# Etichetta le classi\n",
    "def label_func(x):\n",
    "    if x > 15:\n",
    "        return 'aumento'\n",
    "    elif x < -15:\n",
    "        return 'diminuzione'\n",
    "    else:\n",
    "        return 'stabile'\n",
    "\n",
    "df['label_variation'] = df['var_perc'].apply(lambda x: label_func(x) if pd.notna(x) else None)\n",
    "\n",
    "# Gestiamo i valori infiniti\n",
    "df['var_perc'] = df['var_perc'].replace([np.inf, -np.inf], 100.0).round(1)\n",
    "\n",
    "# Rimuovi righe dove prev_cost Ã¨ NaN (primo anno di ciascun paese)\n",
    "df = df.dropna(subset=['prev_cost', 'label_variation'])\n",
    "\n",
    "print(df[['geo', 'TIME_PERIOD', 'OBS_VALUE', 'var_perc', 'label_variation']].head(10))"
   ],
   "id": "f1b5ff21d790b1cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   geo  TIME_PERIOD  OBS_VALUE  var_perc label_variation\n",
      "1   AL         2004        3.0      15.4         aumento\n",
      "2   AL         2005        1.4     -53.3     diminuzione\n",
      "3   AL         2006        1.1     -21.4     diminuzione\n",
      "4   AL         2007        2.5     127.3         aumento\n",
      "5   AL         2008        1.2     -52.0     diminuzione\n",
      "6   AL         2009        0.0    -100.0     diminuzione\n",
      "7   AL         2010        0.3     100.0         aumento\n",
      "8   AL         2011        0.5      66.7         aumento\n",
      "9   AL         2012        0.6      20.0         aumento\n",
      "10  AL         2013        0.9      50.0         aumento\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:13:49.009675Z",
     "start_time": "2025-06-26T09:13:48.678107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Media mobile su 3 anni (incluso l'anno corrente e i due precedenti)\n",
    "df['rolling_mean_3'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window=3, min_periods=1).mean()).round(1)\n",
    "\n",
    "# Media mobile su 5 anni\n",
    "df['rolling_mean_5'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window=5, min_periods=1).mean()).round(1)\n",
    "\n",
    "# Deviazione standard su 3 anni\n",
    "df['rolling_std_3'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window=3, min_periods=1).std()).round(1)\n",
    "\n",
    "# Variazione percentuale media su 3 anni\n",
    "df['pct_change_3'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.pct_change(periods=3)).round(1)\n",
    "\n",
    "# 3) Sostituisci inf e -inf con NaN\n",
    "df['pct_change_3'] = df['pct_change_3'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Dato che il primo anno di ogni paese non ha tre valori precedenti, std e pct saranno NaN\n",
    "# Sostituire NaN con 0 o con un valore minimo\n",
    "df['rolling_std_3'] = df['rolling_std_3'].fillna(0.0)\n",
    "df['pct_change_3'] = df['pct_change_3'].fillna(0.0)\n",
    "\n",
    "# Feature binaria se var_perc > 0 (crescita dellâ€™anno precedente)\n",
    "df['grew_last_year'] = (df['var_perc'] > 0).astype(int)\n",
    "\n",
    "# Slope di regressione lineare sui 3 anni precedenti (trend locale)\n",
    "def local_slope(series):\n",
    "    # Calcola slope ultimo valore basandosi sui 3 punti precedenti\n",
    "    if series.shape[0] < 3:\n",
    "        return 0\n",
    "    y = series.values[-3:]\n",
    "    x = np.arange(len(y))\n",
    "    # fit lineare y = ax + b\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    return a\n",
    "\n",
    "df['slope_3'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.expanding().apply(local_slope, raw=False)).round(1)\n",
    "print(df)"
   ],
   "id": "aea77d8214a4b1d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     geo  TIME_PERIOD  OBS_VALUE  target_cost  prev_cost  var_perc  \\\n",
      "1     AL         2004        3.0          1.4        2.6      15.4   \n",
      "2     AL         2005        1.4          1.1        3.0     -53.3   \n",
      "3     AL         2006        1.1          2.5        1.4     -21.4   \n",
      "4     AL         2007        2.5          1.2        1.1     127.3   \n",
      "5     AL         2008        1.2          0.0        2.5     -52.0   \n",
      "...   ..          ...        ...          ...        ...       ...   \n",
      "1065  SK         2022       12.1         12.1        6.5      86.2   \n",
      "1066  SK         2022       12.1         12.4       12.1       0.0   \n",
      "1067  SK         2022       12.4          3.3       12.1       2.5   \n",
      "1068  SK         2023        3.3          3.3       12.4     -73.4   \n",
      "1069  SK         2023        3.3          3.6        3.3       0.0   \n",
      "\n",
      "     label_variation  rolling_mean_3  rolling_mean_5  rolling_std_3  \\\n",
      "1            aumento             3.0             3.0            0.0   \n",
      "2        diminuzione             2.2             2.2            1.1   \n",
      "3        diminuzione             1.8             1.8            1.0   \n",
      "4            aumento             1.7             2.0            0.7   \n",
      "5        diminuzione             1.6             1.8            0.8   \n",
      "...              ...             ...             ...            ...   \n",
      "1065         aumento             8.3             6.5            3.3   \n",
      "1066         stabile            10.2             8.7            3.2   \n",
      "1067         stabile            12.2             9.9            0.2   \n",
      "1068     diminuzione             9.3             9.3            5.2   \n",
      "1069         stabile             6.3             8.6            5.3   \n",
      "\n",
      "      pct_change_3  grew_last_year  slope_3  \n",
      "1              0.0               1      0.0  \n",
      "2              0.0               0      0.0  \n",
      "3              0.0               0     -0.9  \n",
      "4             -0.2               1      0.6  \n",
      "5             -0.1               0      0.1  \n",
      "...            ...             ...      ...  \n",
      "1065           0.9               1      2.9  \n",
      "1066           0.9               0      2.8  \n",
      "1067           0.9               1      0.1  \n",
      "1068          -0.7               0     -4.4  \n",
      "1069          -0.7               0     -4.6  \n",
      "\n",
      "[1011 rows x 13 columns]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clustering: KMeans\n",
    "In questa sezione utilizziamo lâ€™algoritmo **KMeans** per individuare gruppi di paesi â€œsimiliâ€ sulla base delle caratteristiche **temporaliâ€statistiche** calcolate (in questo caso, su `pct_change_3`, ma esporremo comunque il flusso logico completo).\n",
    "\n",
    "### Procedura:\n",
    "1. **PCA a 3 componenti**:\n",
    "   - Riduciamo la dimensionalitÃ  di `data_scaled` (che Ã¨ dimensione â€œpaese Ã— numero_anniâ€) a uno spazio a 3 dimensioni (`n_components=3`).\n",
    "   - Lo scopo Ã¨ catturare almeno il 70â€“80% della varianza originale, mantenendo comunque la rappresentazione piÃ¹ compatta.\n",
    "2. **Ricerca del numero ottimale di cluster (k)**:\n",
    "   - Proviamo `k` da `2` a `min(11, numero_paesi)`.\n",
    "   - Per ciascun `k`, eseguiamo `KMeans(n_clusters=k)` con `random_state=42, n_init=10`, quindi calcoliamo il **silhouette score**.\n",
    "   - Il silhouette score varia da `-1 a +1`: valori > 0.50 indicano cluster ben separati.\n",
    "   - Selezioniamo `best_k` che massimizza la silhouette.\n",
    "3. **Applicazione definitiva di KMeans**:\n",
    "   - Eseguiamo KMeans con `k=best_k` sullâ€™output di PCA e otteniamo `labels`, un array di lunghezza pari al numero di paesi, in cui `labels[i]` Ã¨ lâ€™indice del cluster di **data_filled.index[i]** (il codice ISO-2 del paese).\n",
    "4. **Salvataggio dei risultati**:\n",
    "   - Creeremo un DataFrame `clustered_df` in cui ogni riga corrisponde a un paese (index) e conterrÃ  la colonna `cluster=labels[i]`.\n",
    "   - Lo salveremo in CSV per poterlo usare come â€œfeature aggiuntivaâ€ o per consultazioni successive.\n",
    "\n",
    "**Nota**: la metrica **silhouette** ci aiuta a capire se i cluster formati sono â€œravvicinatiâ€ allâ€™interno e â€œlontaniâ€ tra di loro. Un valore di circa 0.58â€“0.60 Ã¨ giÃ  buono per serie temporali di questo tipo."
   ],
   "id": "ab8702eaa08e8583"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:13:49.807034Z",
     "start_time": "2025-06-26T09:13:49.091903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8. PCA a 3 componenti prima del clustering\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# 9. Clustering con KMeans e silhouette score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "best_k = 2\n",
    "best_score = -1\n",
    "for k in range(2, min(11, data_pca.shape[0])):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(data_pca)\n",
    "    score = silhouette_score(data_pca, labels)\n",
    "    print(f\"[PCA] Silhouette score per k={k}: {score:.3f}\")\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "print(f\"\\n[PCA] Miglior numero di cluster: {best_k} con Silhouette Score = {best_score:.3f}\")\n",
    "\n",
    "# 10. Applica KMeans finale\n",
    "kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "labels = kmeans_final.fit_predict(data_pca)"
   ],
   "id": "59d340024f8c8bc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PCA] Silhouette score per k=2: 0.536\n",
      "[PCA] Silhouette score per k=3: 0.475\n",
      "[PCA] Silhouette score per k=4: 0.580\n",
      "[PCA] Silhouette score per k=5: 0.377\n",
      "[PCA] Silhouette score per k=6: 0.465\n",
      "[PCA] Silhouette score per k=7: 0.448\n",
      "[PCA] Silhouette score per k=8: 0.468\n",
      "[PCA] Silhouette score per k=9: 0.436\n",
      "[PCA] Silhouette score per k=10: 0.441\n",
      "\n",
      "[PCA] Miglior numero di cluster: 4 con Silhouette Score = 0.580\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clustered Countries\n",
    "In questo blocco creiamo il DataFrame `clustered_df` che associa a ogni **paese** (indice di `data_filled`) il `cluster` corrispondente:\n",
    "\n",
    "1. **Costruiamo un DataFrame `clustered_df`** a partire da `data_filled` (paese Ã— serie completa),\n",
    "2. Aggiungiamo una colonna `cluster` con il valore di `labels[i]` per ciascun paese.\n",
    "3. Salviamo `clustered_df` in un file CSV (`clustering_results.csv`) per riferimento esterno.\n",
    "\n",
    "In questo modo, potremo facilmente ricavare la â€œetichetta di clusterâ€ per ogni paese e usarla come feature aggiuntiva o semplicemente per reporting."
   ],
   "id": "9bf42fd6ecdff82a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T09:13:52.725655Z",
     "start_time": "2025-06-26T09:13:49.883290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clustered_df = pd.DataFrame(data_filled)\n",
    "clustered_df = clustered_df.rename(columns={'index': 'country'})\n",
    "labels_df = pd.Series(labels)\n",
    "clustered_df = clustered_df.iloc[:len(labels)]\n",
    "clustered_df['cluster'] = labels\n",
    "clustered_df.to_csv(\"data/csv/clustering_results.csv\")\n",
    "print(clustered_df.head())"
   ],
   "id": "c079e633ba72accc",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data\\csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m clustered_df = clustered_df.iloc[:\u001B[38;5;28mlen\u001B[39m(labels)]\n\u001B[32m      5\u001B[39m clustered_df[\u001B[33m'\u001B[39m\u001B[33mcluster\u001B[39m\u001B[33m'\u001B[39m] = labels\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m clustered_df.to_csv(\u001B[33m\"\u001B[39m\u001B[33mdata/csv/clustering_results.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(clustered_df.head())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DA_Project\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001B[39m, in \u001B[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    327\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) > num_allow_args:\n\u001B[32m    328\u001B[39m     warnings.warn(\n\u001B[32m    329\u001B[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001B[32m    330\u001B[39m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[32m    331\u001B[39m         stacklevel=find_stack_level(),\n\u001B[32m    332\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m333\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m func(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DA_Project\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001B[39m, in \u001B[36mNDFrame.to_csv\u001B[39m\u001B[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[39m\n\u001B[32m   3956\u001B[39m df = \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.to_frame()\n\u001B[32m   3958\u001B[39m formatter = DataFrameFormatter(\n\u001B[32m   3959\u001B[39m     frame=df,\n\u001B[32m   3960\u001B[39m     header=header,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3964\u001B[39m     decimal=decimal,\n\u001B[32m   3965\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m3967\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrameRenderer(formatter).to_csv(\n\u001B[32m   3968\u001B[39m     path_or_buf,\n\u001B[32m   3969\u001B[39m     lineterminator=lineterminator,\n\u001B[32m   3970\u001B[39m     sep=sep,\n\u001B[32m   3971\u001B[39m     encoding=encoding,\n\u001B[32m   3972\u001B[39m     errors=errors,\n\u001B[32m   3973\u001B[39m     compression=compression,\n\u001B[32m   3974\u001B[39m     quoting=quoting,\n\u001B[32m   3975\u001B[39m     columns=columns,\n\u001B[32m   3976\u001B[39m     index_label=index_label,\n\u001B[32m   3977\u001B[39m     mode=mode,\n\u001B[32m   3978\u001B[39m     chunksize=chunksize,\n\u001B[32m   3979\u001B[39m     quotechar=quotechar,\n\u001B[32m   3980\u001B[39m     date_format=date_format,\n\u001B[32m   3981\u001B[39m     doublequote=doublequote,\n\u001B[32m   3982\u001B[39m     escapechar=escapechar,\n\u001B[32m   3983\u001B[39m     storage_options=storage_options,\n\u001B[32m   3984\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DA_Project\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001B[39m, in \u001B[36mDataFrameRenderer.to_csv\u001B[39m\u001B[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[39m\n\u001B[32m    993\u001B[39m     created_buffer = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    995\u001B[39m csv_formatter = CSVFormatter(\n\u001B[32m    996\u001B[39m     path_or_buf=path_or_buf,\n\u001B[32m    997\u001B[39m     lineterminator=lineterminator,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1012\u001B[39m     formatter=\u001B[38;5;28mself\u001B[39m.fmt,\n\u001B[32m   1013\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1014\u001B[39m csv_formatter.save()\n\u001B[32m   1016\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[32m   1017\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DA_Project\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001B[39m, in \u001B[36mCSVFormatter.save\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    247\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[33;03mCreate the writer & save.\u001B[39;00m\n\u001B[32m    249\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    250\u001B[39m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m251\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m get_handle(\n\u001B[32m    252\u001B[39m     \u001B[38;5;28mself\u001B[39m.filepath_or_buffer,\n\u001B[32m    253\u001B[39m     \u001B[38;5;28mself\u001B[39m.mode,\n\u001B[32m    254\u001B[39m     encoding=\u001B[38;5;28mself\u001B[39m.encoding,\n\u001B[32m    255\u001B[39m     errors=\u001B[38;5;28mself\u001B[39m.errors,\n\u001B[32m    256\u001B[39m     compression=\u001B[38;5;28mself\u001B[39m.compression,\n\u001B[32m    257\u001B[39m     storage_options=\u001B[38;5;28mself\u001B[39m.storage_options,\n\u001B[32m    258\u001B[39m ) \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[32m    259\u001B[39m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[32m    260\u001B[39m     \u001B[38;5;28mself\u001B[39m.writer = csvlib.writer(\n\u001B[32m    261\u001B[39m         handles.handle,\n\u001B[32m    262\u001B[39m         lineterminator=\u001B[38;5;28mself\u001B[39m.lineterminator,\n\u001B[32m   (...)\u001B[39m\u001B[32m    267\u001B[39m         quotechar=\u001B[38;5;28mself\u001B[39m.quotechar,\n\u001B[32m    268\u001B[39m     )\n\u001B[32m    270\u001B[39m     \u001B[38;5;28mself\u001B[39m._save()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DA_Project\\Lib\\site-packages\\pandas\\io\\common.py:749\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    747\u001B[39m \u001B[38;5;66;03m# Only for write methods\u001B[39;00m\n\u001B[32m    748\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n\u001B[32m--> \u001B[39m\u001B[32m749\u001B[39m     check_parent_directory(\u001B[38;5;28mstr\u001B[39m(handle))\n\u001B[32m    751\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m compression:\n\u001B[32m    752\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m compression != \u001B[33m\"\u001B[39m\u001B[33mzstd\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    753\u001B[39m         \u001B[38;5;66;03m# compression libraries do not like an explicit text-mode\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DA_Project\\Lib\\site-packages\\pandas\\io\\common.py:616\u001B[39m, in \u001B[36mcheck_parent_directory\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m    614\u001B[39m parent = Path(path).parent\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent.is_dir():\n\u001B[32m--> \u001B[39m\u001B[32m616\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[33mrf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCannot save file into a non-existent directory: \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mOSError\u001B[39m: Cannot save file into a non-existent directory: 'data\\csv'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## K-Means Visualization - Scatter Plot",
   "id": "3f8d525b259c7e5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Riduci a 2 dimensioni per lo scatter plot\n",
    "pca_2d = PCA(n_components=3, random_state=42)\n",
    "data_2d = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Prima del plotting, verifichiamo le dimensioni\n",
    "data_2d_len = len(data_2d)\n",
    "labels_len = len(labels)\n",
    "\n",
    "# Assicuriamoci che i dati siano della stessa lunghezza\n",
    "data_2d = data_2d[:min(data_2d_len, labels_len)]\n",
    "labels = labels[:min(data_2d_len, labels_len)]\n",
    "\n",
    "# Ora creiamo il plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=data_2d[:, 0], y=data_2d[:, 1], hue=labels, palette='tab10', s=80)\n",
    "\n",
    "# Aggiungiamo le etichette solo per i punti che abbiamo effettivamente plottato\n",
    "for i, name in enumerate(data_filled.index[:len(data_2d)]):\n",
    "    plt.text(data_2d[i, 0]+0.2, data_2d[i, 1]+0.2, name, fontsize=8)\n",
    "plt.title(f\"Cluster con K={best_k}, Silhouette Score={best_score:.3f}\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "6487a877418dfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## K-Means Visualization - Geographic Map",
   "id": "6b354f8a16f279f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "import pycountry\n",
    "\n",
    "def iso2_to_iso3(code):\n",
    "    # Verifica se il codice Ã¨ una stringa\n",
    "    if not isinstance(code, str):\n",
    "        return None\n",
    "    # Normalizza il codice in maiuscolo\n",
    "    code = code.upper()\n",
    "    try:\n",
    "        country = pycountry.countries.get(alpha_2=code)\n",
    "        return country.alpha_3 if country else None\n",
    "    except (AttributeError, LookupError):\n",
    "        return None\n",
    "\n",
    "iso3_codes = [iso2_to_iso3(c) for c in data_filled.index]\n",
    "\n",
    "map_iso3 = [iso2_to_iso3(c) for c in data_filled.index]\n",
    "\n",
    "map_k_df = pd.DataFrame({\n",
    "    'iso_alpha': iso3_codes,\n",
    "    'cluster': labels_df\n",
    "}).dropna(subset=['iso_alpha'])\n",
    "\n",
    "# 2) Disegniamo la choropleth per KMeans\n",
    "fig_k = px.choropleth(\n",
    "    map_k_df,\n",
    "    locations='iso_alpha',\n",
    "    color='cluster',\n",
    "    hover_name='iso_alpha',\n",
    "    color_discrete_sequence=px.colors.qualitative.Light24,\n",
    "    projection=\"mercator\",\n",
    "    locationmode=\"ISO-3\"\n",
    ")\n",
    "fig_k.update_geos(\n",
    "    scope=\"europe\",\n",
    "    showcountries=True,\n",
    "    showcoastlines=False,\n",
    "    showland=True,\n",
    "    fitbounds=\"locations\"\n",
    ")\n",
    "fig_k.update_layout(\n",
    "    title=f\"KMeans (k={best_k}) â€“ Cluster su mappa europea\",\n",
    "    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}\n",
    ")\n",
    "fig_k.show()"
   ],
   "id": "1d6601ef57387070",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Re-Clustering: DBSCAN\n",
    "In alternativa a KMeans, applichiamo un algoritmo **DBSCAN** non supervisionato, che raggruppa i punti per densitÃ :\n",
    "\n",
    "1. **kâ€distance graph**:\n",
    "   - Calcoliamo la distanza del 4Â° nearest neighbor (dato `min_samples=4`) per ogni punto in `data_scaled`.\n",
    "   - Tiriamo fuori le distanze ordinate `k_distances = sorted(distances[:,3])` e tracciamo un grafico per individuare il â€œgomitoâ€ (valore di eps ottimale).\n",
    "2. **Gridâ€search di eps e min_samples** (per trovare combinazione ottimale con silhouette)"
   ],
   "id": "6c8e4d3505ec683c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=4).fit(data_scaled)  # k = min_samples\n",
    "distances, indices = nbrs.kneighbors(data_scaled)\n",
    "\n",
    "# Prendi la distanza k-esima per ogni punto e ordina\n",
    "k_distances = np.sort(distances[:, 3])  # 3 perchÃ© 0=min_auto, 1,2,3=4th nearest neighbor\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(k_distances)\n",
    "plt.ylabel('4th Nearest Neighbor Distance')\n",
    "plt.xlabel('Sorted Points')\n",
    "plt.title('k-distance Graph per DBSCAN')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "3efe2a4425f0b441",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Grid-search per DBSCAN\n",
    "# -------------------------------\n",
    "\n",
    "# 1) Definire range di eps e min_samples da testare\n",
    "eps_values = np.linspace(1.0, 8.0, num=15)\n",
    "min_samples_values = list(range(3, 7))\n",
    "\n",
    "best_score = -1\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "\n",
    "for eps_candidate in eps_values:\n",
    "    for min_s in min_samples_values:\n",
    "        db = DBSCAN(eps=eps_candidate, min_samples=min_s, metric='euclidean')\n",
    "        labels_tmp = db.fit_predict(data_scaled)\n",
    "\n",
    "        # Consideriamo solo i punti non outlier (label != -1)\n",
    "        mask = labels_tmp != -1\n",
    "        unique_clusters = set(labels_tmp[mask])\n",
    "\n",
    "        # Serve almeno 2 cluster reali per silhouette; altrimenti skip\n",
    "        if len(unique_clusters) <= 1:\n",
    "            continue\n",
    "\n",
    "        score_tmp = silhouette_score(data_scaled[mask], labels_tmp[mask])\n",
    "        # Se il silhouette Ã¨ migliorativo, aggiorniamo\n",
    "        if score_tmp > best_score:\n",
    "            best_score = score_tmp\n",
    "            best_eps = eps_candidate\n",
    "            best_min_samples = min_s\n",
    "\n",
    "# 2) Stampa dei parametri ottimali (o avviso se non ne trovi)\n",
    "if best_eps is None:\n",
    "    print(\"DBSCAN grid-search: nessuna combinazione ha prodotto â‰¥2 cluster validi.\")\n",
    "    best_eps = 7.5\n",
    "    best_min_samples = 5\n",
    "    print(f\"Usiamo valori di default eps={best_eps}, min_samples={best_min_samples}\")\n",
    "else:\n",
    "    print(f\"DBSCAN grid-search â€“ miglior eps={best_eps:.2f}, min_samples={best_min_samples}, silhouette={best_score:.3f}\")\n",
    "\n",
    "# 3) Applichiamo DBSCAN finale con i parametri selezionati\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples, metric='euclidean')\n",
    "labels_db = dbscan.fit_predict(data_scaled)\n",
    "\n",
    "# Eventuale verifica finale della silhouette\n",
    "mask_final = labels_db != -1\n",
    "if len(set(labels_db[mask_final])) > 1:\n",
    "    final_score = silhouette_score(data_scaled[mask_final], labels_db[mask_final])\n",
    "    print(f\"Silhouette finale (inlier): {final_score:.3f}\")\n",
    "else:\n",
    "    print(\"DBSCAN finale: meno di 2 cluster validi o troppi outlier.\")"
   ],
   "id": "207db70b42ca0be4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DBSCAN Visualization - Scatter Plot",
   "id": "51f59f74800dc546"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PCA 2D per visualizzare tutti i punti (anche rumore)\n",
    "pca2 = PCA(n_components=2, random_state=42)\n",
    "data_pca2 = pca2.fit_transform(data_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "palette = sns.color_palette(\"tab10\", len(set(labels_db)))\n",
    "sns.scatterplot(x=data_pca2[:, 0], y=data_pca2[:, 1],\n",
    "                hue=labels_db, palette=palette, legend=\"full\", s=80)\n",
    "n_points = min(len(data_filled.index), len(data_pca2))\n",
    "countries = data_filled.index[:n_points]\n",
    "for i, country in enumerate(countries):\n",
    "    plt.text(data_pca2[i, 0] + 0.1, data_pca2[i, 1] + 0.1, country, fontsize=7)\n",
    "\n",
    "plt.title(f\"DBSCAN su PCA 2D\")  #(Silhouette inlier: {score_db:.3f})\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Cluster DBSCAN\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ],
   "id": "a5e7d2a49acfaa2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DBSCAN Visualization - Geographic Map",
   "id": "7fc631d5b3097823"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def iso2_to_iso3(code):\n",
    "    try:\n",
    "        country = pycountry.countries.get(alpha_2=code)\n",
    "        return country.alpha_3 if country else None\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "# Applichiamo la conversione\n",
    "iso3_list = [iso2_to_iso3(c) for c in data_filled.index]\n",
    "\n",
    "map_iso3 = [iso2_to_iso3(c) for c in data_filled.index]\n",
    "\n",
    "# Prepara DataFrame per la mappa\n",
    "map_df = pd.DataFrame({\n",
    "    'iso_alpha': iso3_list,\n",
    "    'cluster': labels_db\n",
    "}).dropna(subset=['iso_alpha'])\n",
    "\n",
    "fig = px.choropleth(\n",
    "    map_df,\n",
    "    locations='iso_alpha',\n",
    "    color='cluster',\n",
    "    hover_name='iso_alpha',\n",
    "    color_discrete_sequence=px.colors.qualitative.Light24,\n",
    "    projection=\"mercator\",\n",
    "    locationmode=\"ISO-3\"\n",
    ")\n",
    "\n",
    "# Limita la mappa allâ€™Europa\n",
    "fig.update_geos(\n",
    "    scope=\"europe\",\n",
    "    showcountries=True,\n",
    "    showcoastlines=False,\n",
    "    showland=True,\n",
    "    fitbounds=\"locations\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Cluster DBSCAN su mappa europea\",\n",
    "    margin={\"r\": 0, \"t\": 40, \"l\": 0, \"b\": 0}\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "id": "e55b422d7f7da02e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Classification & Regression (Split Training-Testing)\n",
    "Ora che abbiamo tutte le feature, costruiamo i due sottoâ€insiemi â€œRegressioneâ€ e â€œClassificazioneâ€ con lo stesso criterio temporale (anni â‰¤ 2016 â†’ train; > 2016 â†’ test).\n",
    "\n",
    "**Passaggi:**\n",
    "1. Definiamo un elenco di **feature candidate** (quelle con correlazione < 0.8 per evitare multicollinearitÃ )\n",
    "2. **Regressione**:\n",
    "    - Usiamo `df.dropna(subset=['target_cost'])` per tenere solo le righe con `target_cost` definito.\n",
    "    - `X_reg = df[feature_cols]` e `y_reg = df['target_cost']`.\n",
    "    - Split:\n",
    "       - `train_mask = TIME_PERIOD â‰¤ 2016`\n",
    "       - `X_reg_train = X_reg[train_mask]`, `X_reg_test = X_reg[~train_mask]`\n",
    "       - Stessa logica per `y_reg`.\n",
    "3. **Classificazione**:\n",
    "    - Usiamo `df.dropna(subset=['label_variation'])` per le righe con etichetta.\n",
    "    - `X_clf = df[feature_cols]` e `y_clf = df['label_variation']`.\n",
    "    - Split identico (anni â‰¤ 2016 â†’ train; > 2016 â†’ test).\n",
    "\n",
    "Stampiamo in output le dimensioni di train/test per entrambe le modalitÃ , giusto per avere unâ€™idea di quante righe abbiamo su cui addestrare e testare."
   ],
   "id": "e0a4161ca6617104"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Consideriamo solo le colonne numeriche candidate per la regressione\n",
    "numerical_cols = ['OBS_VALUE', 'rolling_mean_3', 'rolling_std_3', 'grew_last_year', 'pct_change_3', 'rolling_mean_5', 'slope_3']\n",
    "\n",
    "# Calcolo della matrice di correlazione\n",
    "corr_matrix = df[numerical_cols].corr().abs()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='Blues', vmin=0, vmax=1)\n",
    "plt.title(\"Matrice di correlazione fra feature numeriche\")\n",
    "plt.show()"
   ],
   "id": "7c4cef3eb271774a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature Scelte per la regressione (coefficiente di correlazione < 0.8)\n",
    "print(df[['geo', 'TIME_PERIOD', 'OBS_VALUE', 'pct_change_3', 'rolling_std_3', 'grew_last_year']].head(10))"
   ],
   "id": "770c7cb460a77c1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Analisi esplorativa\n",
    "df.info()\n",
    "df.describe()"
   ],
   "id": "f36f9be4cd6dedf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Boxplot\n",
    "paesi_da_plot = ['IT','DE','FR','ES','PL']\n",
    "df_box = df[df['geo'].isin(paesi_da_plot)].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    x='geo',\n",
    "    y='pct_change_3',\n",
    "    hue='geo',\n",
    "    data=df_box,\n",
    "    palette='pastel',\n",
    "    dodge=False\n",
    ")\n",
    "plt.title(\"Distribuzione di pct_change_3 per paese\")\n",
    "plt.xlabel(\"Paese (ISO-2)\")\n",
    "plt.ylabel(\"Variazione % media su 3 anni\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ],
   "id": "f645d53f1f14bb90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Definiamo le feature candidate per regressione e classificazione\n",
    "feature_cols = ['OBS_VALUE', 'pct_change_3', 'rolling_std_3', 'grew_last_year']\n",
    "\n",
    "# Per regressione:\n",
    "regression_df = df.dropna(subset=['target_cost']).copy()\n",
    "X_reg = regression_df[feature_cols]\n",
    "y_reg = regression_df['target_cost']\n",
    "\n",
    "# Splitting: usiamo 'TIME_PERIOD' per train/test\n",
    "train_mask = regression_df['TIME_PERIOD'] <= 2016\n",
    "X_reg_train = X_reg[train_mask]\n",
    "X_reg_test = X_reg[~train_mask]\n",
    "y_reg_train = y_reg[train_mask]\n",
    "y_reg_test = y_reg[~train_mask]\n",
    "\n",
    "# Per classificazione:\n",
    "classification_df = df.dropna(subset=['label_variation']).copy()\n",
    "X_clf = classification_df[feature_cols]\n",
    "y_clf = classification_df['label_variation']\n",
    "\n",
    "# Stesso criterio temporale per la classificazione\n",
    "train_mask_clf = classification_df['TIME_PERIOD'] <= 2016\n",
    "X_clf_train = X_clf[train_mask_clf]\n",
    "X_clf_test = X_clf[~train_mask_clf]\n",
    "y_clf_train = y_clf[train_mask_clf]\n",
    "y_clf_test = y_clf[~train_mask_clf]\n",
    "\n",
    "print(\"Reg train size:\", X_reg_train.shape, \"Reg test size:\", X_reg_test.shape)\n",
    "print(\"Clf train size:\", X_clf_train.shape, \"Clf test size:\", X_clf_test.shape)"
   ],
   "id": "5a42c1be88bc3ea9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Decision Tree - Classification\n",
    "Costruiamo un **Decision Tree Classifier** per la classificazione di `label_variation`.\n",
    "- **Modello base**: `DecisionTreeClassifier(random_state=42)`.\n",
    "- Lo addestriamo su `(X_clf_train, y_clf_train)` e calcoliamo le predizioni su `X_clf_test`.\n",
    "- Stampiamo il **classification report** (precision, recall, F1, support) e la **confusion matrix** per valutare le prestazioni iniziali di un albero non potatore. "
   ],
   "id": "772c2db5c8c1b64d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepara i dati (X_clf_train/Test e y_clf_train/Test giÃ  definiti)\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "# Predizione\n",
    "y_dt_clf_pred = dt_clf.predict(X_clf_test)\n",
    "\n",
    "print(\"Decision Tree Classificazione â€“ Report:\\n\", classification_report(y_clf_test, y_dt_clf_pred))"
   ],
   "id": "a85f63c6ac59d885",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pruned Decision Tree - Classification\n",
    "Partendo dallâ€™albero base, cerchiamo il **miglior valore di `ccp_alpha`** (parametro di potatura) che massimizza lâ€™F1â€weighted sul test set:\n",
    "\n",
    "1. Otteniamo il **path di potatura** con `dt_clf.cost_complexity_pruning_path(X_clf_train, y_clf_train)`.\n",
    "2. Iteriamo su ogni `ccp_alpha` di quel path (escludendo lâ€™ultimo, che corrisponde a un albero di un solo nodo):\n",
    "   - Addestriamo un nuovo albero `DecisionTreeClassifier(ccp_alpha=ccp)`.\n",
    "   - Facciamo predizione su `X_clf_test` e calcoliamo lâ€™`f1_score(average='weighted')`.\n",
    "3. Selezioniamo lâ€™`alpha` con F1 piÃ¹ alto.\n",
    "4. Confrontiamo lâ€™F1 dellâ€™albero potatore con lâ€™albero base.\n",
    "\n",
    "Alla fine salviamo il Decision Tree base e quello potatore su disco (joblib) per usi futuri (o meglio, andrebbe fatto ma non l'abbiamo fatto)."
   ],
   "id": "da51fb9980c1f82e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Trova percorso di ccp_alpha ottimale\n",
    "path_clf = dt_clf.cost_complexity_pruning_path(X_clf_train, y_clf_train)\n",
    "ccp_alphas_clf = path_clf.ccp_alphas[:-1]\n",
    "clf_trees = []\n",
    "f1_scores = []\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for ccp in ccp_alphas_clf:\n",
    "    dt = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp)\n",
    "    dt.fit(X_clf_train, y_clf_train)\n",
    "    y_pred = dt.predict(X_clf_test)\n",
    "    f1 = f1_score(y_clf_test, y_pred, average='weighted', zero_division=1)\n",
    "    clf_trees.append(dt)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "best_index_clf = np.argmax(f1_scores)\n",
    "best_alpha_clf = ccp_alphas_clf[best_index_clf]\n",
    "best_dt_clf = clf_trees[best_index_clf]\n",
    "print(f\"Pruned Tree Classificazione â€“ miglior ccp_alpha: {best_alpha_clf:.6f}, F1-weighted: {f1_scores[best_index_clf]:.3f}\")\n",
    "\n",
    "# Confronto con DT non potato\n",
    "base_f1 = f1_score(y_clf_test, y_dt_clf_pred, average='weighted', zero_division=1)\n",
    "print(f\"DT non potato F1-weighted: {base_f1:.3f}\")"
   ],
   "id": "bbf989f80b7614f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Confusion Matrix",
   "id": "206a770ee64f3a7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title='Matrice di Confusione'):\n",
    "    # Calcola la matrice di confusione\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Ottieni le classi uniche\n",
    "    classes = sorted(set(y_true))\n",
    "\n",
    "    # Crea la figura\n",
    "    plt.figure(figsize=(5, 4))\n",
    "\n",
    "    # Crea heatmap con seaborn\n",
    "    sns.heatmap(cm,\n",
    "                annot=True,  # Mostra i valori nelle celle\n",
    "                fmt='d',     # Formato numeri interi\n",
    "                cmap='Blues',  # Palette di colori\n",
    "                xticklabels=classes,\n",
    "                yticklabels=classes)\n",
    "\n",
    "    # Personalizza il grafico\n",
    "    plt.title(title, pad=20)\n",
    "    plt.xlabel('Predetto')\n",
    "    plt.ylabel('Reale')\n",
    "\n",
    "    # Aggiungi percentuali di accuratezza per ogni classe\n",
    "    accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "    for i, accuracy in enumerate(accuracies):\n",
    "        plt.text(-0.5, i, f'{accuracy:.1%}',\n",
    "                va='center',\n",
    "                ha='right',\n",
    "                fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizza la matrice per il Decision Tree originale\n",
    "plot_confusion_matrix(y_clf_test, y_dt_clf_pred,\n",
    "                     title='Matrice di Confusione - Decision Tree')\n",
    "\n",
    "# Visualizza la matrice per il Decision Tree potato\n",
    "plot_confusion_matrix(y_clf_test, best_dt_clf.predict(X_clf_test),\n",
    "                     title='Matrice di Confusione - Decision Tree Potato')"
   ],
   "id": "448a7a318d633844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Decision Tree - Regression\n",
    "Passiamo ora alla **regressione** per prevedere `target_cost`.\n",
    "1. Addestriamo un **Decision Tree Regressor** (`DecisionTreeRegressor(random_state=42)`) su `(X_reg_train, y_reg_train)`.\n",
    "2. Calcoliamo le predizioni `y_dt_pred` su `X_reg_test`.\n",
    "3. Valutiamo le metriche di regressione:\n",
    "   - **RMSE** (Root Mean Squared Error): `âˆš(MSE)`.\n",
    "   - **MAE** (Mean Absolute Error).\n",
    "   - **RÂ²** (coefficiente di determinazione).\n",
    "\n",
    "Stampiamo i risultati iniziali per lâ€™albero non potato."
   ],
   "id": "f98f8be2e7efca64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Prepara i dati (assumendo tu abbia X_reg_train/Test e y_reg_train/Test giÃ  definiti)\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "# Predizioni\n",
    "y_dt_pred = dt_reg.predict(X_reg_test)\n",
    "\n",
    "# Metriche iniziali\n",
    "rmse_dt = mean_squared_error(y_reg_test, y_dt_pred)\n",
    "mae_dt  = mean_absolute_error(y_reg_test, y_dt_pred)\n",
    "r2_dt   = r2_score(y_reg_test, y_dt_pred)\n",
    "\n",
    "print(f\"Decision Tree Regressione â€“ RMSE: {rmse_dt:.3f}, MAE: {mae_dt:.3f}, RÂ²: {r2_dt:.3f}\")"
   ],
   "id": "ed150b7f3d834c9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pruned Decision Tree - Regression\n",
    "Proprio come per la classificazione, cerchiamo lâ€™`alpha` di potatura che minimizza lâ€™MSE sul test set:\n",
    "\n",
    "1. Otteniamo il **path di cost complexity** con `dt_reg.cost_complexity_pruning_path(X_reg_train, y_reg_train)`.\n",
    "2. Iteriamo su ciascun `ccp_alpha` (escluso lâ€™ultimo), addestriamo un albero con `ccp_alpha=ccp`, facciamo predizione su `X_reg_test` e calcoliamo **MSE**.\n",
    "3. Selezioniamo `ccp_alpha_best` che minimizza lâ€™MSE.\n",
    "4. Addestriamo un nuovo `DecisionTreeRegressor(ccp_alpha=ccp_alpha_best)` su tutto il train e calcoliamo RMSE, MAE, RÂ² sul test.\n",
    "5. Confrontiamo con i valori dellâ€™albero non potatore.\n",
    "6. Salviamo entrambi gli alberi (base e potato) per futuri utilizzi."
   ],
   "id": "6074695d9d90da03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Trova il percorso di ccp_alpha ottimale\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "path = dt_reg.cost_complexity_pruning_path(X_reg_train, y_reg_train)\n",
    "ccp_alphas = path.ccp_alphas[:-1]  # lâ€™ultimo crea un albero con un solo nodo\n",
    "reg_trees = []\n",
    "rmse_scores = []\n",
    "\n",
    "ccp_alphas = np.maximum(ccp_alphas, 0.0)\n",
    "\n",
    "for ccp in ccp_alphas:\n",
    "    dt = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp)\n",
    "    dt.fit(X_reg_train, y_reg_train)\n",
    "    y_pred = dt.predict(X_reg_test)\n",
    "    rmse_val = mean_squared_error(y_reg_test, y_pred)\n",
    "    reg_trees.append(dt)\n",
    "    rmse_scores.append(rmse_val)\n",
    "\n",
    "# Seleziona lâ€™alpha che minimizza RMSE\n",
    "best_index = np.argmin(rmse_scores)\n",
    "best_alpha = ccp_alphas[best_index]\n",
    "best_dt = reg_trees[best_index]\n",
    "print(f\"Pruned Tree â€“ miglior ccp_alpha: {best_alpha:.6f}, RMSE: {rmse_scores[best_index]:.3f}\")\n",
    "\n",
    "# Confronto con il DT non potato\n",
    "print(f\"DT non potato RMSE: {rmse_dt:.3f}\")"
   ],
   "id": "51cdf79c45a8139c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Diagnostica di Regressione",
   "id": "92bea125c3a53579"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_regression_diagnostics(y_true, y_pred, title='Diagnostica Regressione'):\n",
    "    # Calcola i residui\n",
    "    residuals = y_true - y_pred\n",
    "\n",
    "    # Crea una figura con 2x2 subplot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "    fig.suptitle(title, fontsize=16, y=1.02)\n",
    "\n",
    "    # 1. Valori Predetti vs Valori Reali\n",
    "    axes[0,0].scatter(y_true, y_pred, alpha=0.5)\n",
    "    axes[0,0].plot([y_true.min(), y_true.max()],\n",
    "                   [y_true.min(), y_true.max()],\n",
    "                   'r--', lw=2)\n",
    "    axes[0,0].set_xlabel('Valori Reali')\n",
    "    axes[0,0].set_ylabel('Valori Predetti')\n",
    "    axes[0,0].set_title('Predetti vs Reali')\n",
    "\n",
    "    # 2. Distribuzione dei Residui\n",
    "    sns.histplot(residuals, kde=True, ax=axes[0,1])\n",
    "    axes[0,1].axvline(x=0, color='r', linestyle='--')\n",
    "    axes[0,1].set_xlabel('Residui')\n",
    "    axes[0,1].set_title('Distribuzione dei Residui')\n",
    "\n",
    "    # 3. Residui vs Valori Predetti\n",
    "    axes[1,0].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axes[1,0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[1,0].set_xlabel('Valori Predetti')\n",
    "    axes[1,0].set_ylabel('Residui')\n",
    "    axes[1,0].set_title('Residui vs Predetti')\n",
    "\n",
    "    # 4. Q-Q Plot dei Residui\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1,1])\n",
    "    axes[1,1].set_title('Q-Q Plot dei Residui')\n",
    "\n",
    "    # Aggiungi metriche di performance\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    metrics_text = f'RMSE: {rmse:.2f}\\nMAE: {mae:.2f}\\nRÂ²: {r2:.2f}'\n",
    "    fig.text(0.02, 0.98, metrics_text, fontsize=12,\n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizza diagnostica per il Decision Tree originale\n",
    "plot_regression_diagnostics(y_reg_test,\n",
    "                          dt_reg.predict(X_reg_test),\n",
    "                          'Diagnostica Regressione - Decision Tree')\n",
    "\n",
    "# Visualizza diagnostica per il Decision Tree potato\n",
    "plot_regression_diagnostics(y_reg_test,\n",
    "                          best_dt.predict(X_reg_test),\n",
    "                          'Diagnostica Regressione - Decision Tree Potato')"
   ],
   "id": "47d78a843538071a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decision Trees - Visualization",
   "id": "3a859f04ae1077b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Per la visualizzazione dei Decision Tree\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Decision Tree Classificazione\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt_clf,\n",
    "          feature_names=feature_cols,\n",
    "          class_names=sorted(y_clf_train.unique()),\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Decision Tree - Classificazione (Non Potato)\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Decision Tree Classificazione Potato\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(best_dt_clf,\n",
    "          feature_names=feature_cols,\n",
    "          class_names=sorted(y_clf_train.unique()),\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(f\"Decision Tree - Classificazione (Potato con ccp_alpha={best_alpha_clf:.6f})\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Decision Tree Regressione\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt_reg,\n",
    "          feature_names=feature_cols,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Decision Tree - Regressione (Non Potato)\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Decision Tree Regressione Potato\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(best_dt,\n",
    "          feature_names=feature_cols,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(f\"Decision Tree - Regressione (Potato con ccp_alpha={best_alpha:.6f})\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Grafico dell'andamento dell'errore in funzione di ccp_alpha\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ccp_alphas_clf, f1_scores, marker='o')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.ylabel('F1-score')\n",
    "plt.title('F1-score vs ccp_alpha (Classificazione)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 6. Grafico dell'andamento dell'errore in funzione di ccp_alpha per regressione\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ccp_alphas, rmse_scores, marker='o')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs ccp_alpha (Regressione)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "f8b6cda98889ac6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Random Forest - Regression\n",
    "Dopo aver valutato gli alberi, costruiamo un **Random Forest Regressor**:\n",
    "1. Creiamo il modello con parametri:\n",
    "   - `n_estimators`: 100\n",
    "   - `random state`: 42\n",
    "2. Calcoliamo le predizioni `y_reg_pred` su `X_reg_test` e quindi lâ€™**RMSE**, **MAE**, **RÂ²** rispetto a `target_cost`.\n",
    "3. Stampiamo i risultati"
   ],
   "id": "d79c5bdadcfe4a78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Crea e addestra il modello\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "# 1. Fai le predizioni\n",
    "y_reg_pred = regressor.predict(X_reg_test)\n",
    "\n",
    "# 2. Calcola le metriche\n",
    "rmse = np.sqrt(mean_squared_error(y_reg_test, y_reg_pred))\n",
    "mae = mean_absolute_error(y_reg_test, y_reg_pred)\n",
    "r2 = r2_score(y_reg_test, y_reg_pred)\n",
    "\n",
    "print(f\"Regressione â€“ RMSE: {rmse:.3f}, MAE: {mae:.3f}, RÂ²: {r2:.3f}\")"
   ],
   "id": "2aa816bf279bff57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## SMOTE & Random Forest - Classification\n",
    "Per la **classificazione** su `label_variation` (aumento/stabile/diminuzione):\n",
    "1. Verifichiamo la **distribuzione delle classi** nel training set (`y_clf_train.value_counts(normalize=True)`).\n",
    "2. Costruiamo un **RandomForestClassifier** base (`n_estimators=100, random_state=42`).\n",
    "3. Addestriamo su `(X_clf_train, y_clf_train)` e calcoliamo predizioni su `X_clf_test`.\n",
    "4. Stampiamo il **classification report** e la **matrice di confusione**.\n",
    "5. Applichiamo **SMOTE** (oversampling) sul **solo** `X_clf_train, y_clf_train` per bilanciare le classi, creando `X_clf_train_balanced, y_clf_train_balanced`:\n",
    "   - Addestriamo un nuovo RF su `(X_clf_train_balanced, y_clf_train_balanced)`.\n",
    "   - Calcoliamo predizioni su `X_clf_test` e valutiamo nuovamente (report, confusion).\n",
    "6. Confrontiamo **accuracy** e **balanced accuracy** dei due modelli.\n",
    "\n",
    "In questo modo identifichiamo quale strategia di bilanciamento funziona meglio per le nostre classi sbilanciate."
   ],
   "id": "b2a36c960e498ee9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# 0. Verifica della distribuzione delle classi\n",
    "print(\"Distribuzione delle classi nel training set:\")\n",
    "print(y_clf_train.value_counts(normalize=True) * 100)\n",
    "print(\"\\nDistribuzione delle classi nel test set:\")\n",
    "print(y_clf_test.value_counts(normalize=True) * 100)\n",
    "\n",
    "# 1. Crea e addestra il modello\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "# 2. Predici sui dati di test\n",
    "y_clf_pred = clf.predict(X_clf_test)\n",
    "\n",
    "# 3. Calcola le metriche\n",
    "print(\"Classification Report:\\n\", classification_report(y_clf_test, y_clf_pred, zero_division=1))\n",
    "\n",
    "# Applica SMOTE solo al training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_clf_train_balanced, y_clf_train_balanced = smote.fit_resample(X_clf_train, y_clf_train)\n",
    "\n",
    "print(\"\\nDistribuzione originale delle classi:\", Counter(y_clf_train))\n",
    "print(\"Distribuzione dopo SMOTE:\", Counter(y_clf_train_balanced))\n",
    "\n",
    "# Addestra il modello con i dati bilanciati\n",
    "clf_balanced = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_balanced.fit(X_clf_train_balanced, y_clf_train_balanced)\n",
    "\n",
    "# Fai predizioni e valuta\n",
    "y_clf_pred_balanced = clf_balanced.predict(X_clf_test)\n",
    "print(\"\\nReport di classificazione con SMOTE:\")\n",
    "print(classification_report(y_clf_test, y_clf_pred_balanced, zero_division=1))\n",
    "\n",
    "def plot_confusion_matrix_detailed(y_true, y_pred, title='Matrice di Confusione Dettagliata'):\n",
    "    # Calcola la matrice di confusione\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calcola le percentuali per ogni riga\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "    # Ottieni le classi uniche\n",
    "    classes = sorted(set(y_true))\n",
    "\n",
    "    # Crea la figura\n",
    "    plt.figure(figsize=(5, 4))\n",
    "\n",
    "    # Crea la heatmap principale\n",
    "    sns.heatmap(cm, annot=np.asarray([\n",
    "        [f'{count}\\n({percentage:.1f}%)'\n",
    "         for count, percentage in zip(row_counts, row_percentages)]\n",
    "        for row_counts, row_percentages in zip(cm, cm_percentage)\n",
    "    ]),\n",
    "                fmt='',\n",
    "                cmap='Blues',\n",
    "                xticklabels=classes,\n",
    "                yticklabels=classes,\n",
    "                cbar_kws={'label': 'Numero di campioni'})\n",
    "\n",
    "    # Aggiungi le etichette e il titolo\n",
    "    plt.title(title, pad=20, fontsize=14)\n",
    "    plt.xlabel('Classe Predetta', fontsize=12)\n",
    "    plt.ylabel('Classe Reale', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Valutiamo i modelli\n",
    "plot_confusion_matrix_detailed(y_clf_test, y_clf_pred, \"Modello Base\")\n",
    "plot_confusion_matrix_detailed(y_clf_test, y_clf_pred_balanced, \"Modello con SMOTE\")"
   ],
   "id": "e95533716168e564",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Importance\n",
    "Per capire quali feature hanno piÃ¹ peso nella decisione del Random Forest Classifier, calcoliamo la **feature importance** per ogni modello:\n",
    "1. **`clf` (RF base)**\n",
    "2. **`clf_balanced` (RF + SMOTE)**\n",
    "\n",
    "Procedura:\n",
    "- Per ciascun modello, otteniamo `model.feature_importances_`, un array con importanza (da 0 a 1) per ciascuna feature nellâ€™ordine di `feature_cols`.\n",
    "- Ordiniamo le feature in base allâ€™importanza (decrescente).\n",
    "- Tracciamo un **bar plot** con `plt.bar()` per visualizzare graficamente quale feature Ã¨ piÃ¹ rilevante.\n",
    "\n",
    "Questo ci permette di capire, ad esempio, se `pct_change_3` Ã¨ piÃ¹ informativa di `OBS_VALUE` o `rolling_std_3`, e se lâ€™aggiunta di SMOTE cambia la dinamica delle importanze."
   ],
   "id": "804d2314f1e280f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualizziamo le feature importance\n",
    "def plot_feature_importance(model, feature_names, title):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(f\"Feature Importance ({title})\")\n",
    "    plt.bar(range(X_clf_train.shape[1]), importances[indices])\n",
    "    plt.xticks(range(X_clf_train.shape[1]), [feature_names[i] for i in indices], rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizza feature importance per ogni modello\n",
    "plot_feature_importance(clf, feature_cols, \"Modello Base\")\n",
    "plot_feature_importance(clf_balanced, feature_cols, \"Modello con SMOTE\")"
   ],
   "id": "a5982481c8976d85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Predicted Models Export (.csv)\n",
    "A questo punto dovremmo aver creato e salvato i modelli (Decision Tree, Random Forest) per:\n",
    "\n",
    "- **Regressione**: previsione del costo t+1 (`pred_cost`).\n",
    "- **Classificazione**: previsione dellâ€™etichetta `label_variation`.\n",
    "\n",
    "Questi sono i passaggi che abbiamo eseguito noi:\n",
    "- `df.to_csv(\"dataset_for_models.csv\")` salverebbe il dataset con tutte le feature utili.\n",
    "- `regression_df['pred_cost'] = regressor.predict(...)` calcola il costo predetto per ciascuna riga del test set; lo salviamo in `regression_results.csv`.\n",
    "- Per la classificazione, estraiamo le probabilitÃ  di ciascuna classe (`clf.predict_proba`), le aggiungiamo a `classification_output` e salviamo in `classification_results.csv`.\n",
    "\n",
    "In questo notebook di esempio lo facciamo per illustrare il formato, ma nella pipeline â€œrealeâ€ questi passaggi sono eseguiti in uno script esterno tipo `train_and_infer.py`, costituito dai seguenti passaggi (ma noi siamo pigri e impediti e quindi non li abbiamo fatti):\n",
    "1. **Carichiamo i modelli serializzati** (`joblib.load()`).\n",
    "2. **Facciamo le predizioni** sul dataset di test (anno > 2016).\n",
    "3. **Salviamo** in `predictions/` i file CSV (uno per regressione, uno per classificazione) contenenti:\n",
    "   - `(geo, TIME_PERIOD, OBS_VALUE, target_cost, pred_cost)`\n",
    "   - `(geo, TIME_PERIOD, OBS_VALUE, label_variation, pred_label, prob_aumento, prob_diminuzione, prob_stabile)`\n",
    "4. **Stiliamo un breve log** per sapere dove e con quale timestamp Ã¨ stato salvato ciascun file."
   ],
   "id": "1c73ed0865e8c9cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Esporta dataset pulito con feature e target per regressione e classificazione\n",
    "df.to_csv(\"data/csv/dataset_for_models.csv\", index=False)\n",
    "\n",
    "# 2) Esporta previsioni del modello di regressione (aggiungiamo una colonna con y_reg_pred)\n",
    "regression_df = regression_df.copy()\n",
    "regression_df['pred_cost'] = regressor.predict(regression_df[feature_cols]).round(1)\n",
    "regression_df[['geo', 'TIME_PERIOD', 'OBS_VALUE', 'pred_cost']].to_csv(\"data/csv/regression_results.csv\", index=False)\n",
    "\n",
    "# 3) Esporta etichette di variazione (classification) con probabilitÃ \n",
    "probs = clf.predict_proba(X_clf_test).round(1)\n",
    "classification_output = classification_df.loc[~train_mask_clf, ['geo', 'TIME_PERIOD', 'OBS_VALUE', 'label_variation']].copy()\n",
    "# Assumiamo che le classi siano ['aumento','diminuzione','stabile'] nellâ€™ordine di clf.classes_\n",
    "for idx, cls in enumerate(clf.classes_):\n",
    "    classification_output[f\"prob_{cls}\"] = probs[:, idx]\n",
    "classification_output.to_csv(\"data/csv/classification_results.csv\", index=False)\n",
    "\n",
    "print(\"Dataset pulito con feature e target per regressione e classificazione:\")\n",
    "print(regression_df)"
   ],
   "id": "dd6f9fdbab1ee7ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Dataset con etichette di variazione (classification):\")\n",
    "print(classification_df)"
   ],
   "id": "e9a393f82f902321",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Proof of Concepts â€“ Time Series & Confronto Predizione/Reale\n",
    "Questa sezione dimostra la bontÃ  del modello di regressione **su un singolo paese di esempio (Italia, codice â€œITâ€)** (cosÃ¬ a caso eh...):\n",
    "\n",
    "1. **Filtriamo** il DataFrame di test per â€œITâ€ (`regression_df[regression_df['geo']=='IT']`).\n",
    "2. **Ricreiamo** la colonna `pred_cost` tramite `regressor.predict(...)` per ciascun anno di test.\n",
    "3. **Disegniamo** uno `sns.lineplot` con due serie:\n",
    "   - **Costo reale** (asse â€œanno vs OBS_VALUEâ€),\n",
    "   - **Costo predetto** (asse â€œanno+1 vs pred_costâ€), in modo da allineare i punti.\n",
    "\n",
    "Lo scopo Ã¨ mostrare se i valori reali e predetti â€œcamminanoâ€ in modo simile, con un leggero ritardo di 1 anno (cosa naturale, dato che stiamo predicendo il costo t+1). SPOILER: Ã¨ **quasi** preciso."
   ],
   "id": "57f669fb596881ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Esempio: confronto tra COST reale e COST predetto per un dato paese (es. 'IT')\n",
    "it_df = regression_df[regression_df['geo'] == 'IT'].copy()\n",
    "it_df['year_pred'] = it_df['TIME_PERIOD'] + 1\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "# Costo reale in t\n",
    "sns.lineplot(x='TIME_PERIOD', y='OBS_VALUE', data=it_df, label='Costo reale')\n",
    "# Costo predetto allineato su year_pred (cioÃ¨ pred_cost in anno t)\n",
    "sns.lineplot(x='year_pred', y='pred_cost', data=it_df, label='Costo predetto')\n",
    "plt.title(\"Italia: Costo reale vs Costo predetto\")\n",
    "plt.xlabel(\"Anno\")\n",
    "plt.ylabel(\"Indice costo\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "54182ad998f1c952",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Decomposizione dellâ€™errore e validitÃ  clustering\n",
    "Per la **regressione** abbiamo utilizzato la Biasâ€“Varianza Decomposition (mlxtend) su Decision Tree e Random Forest.\n",
    "- Il MSE totale Ã¨ stato scomposto in BiasÂ², Varianza e Rumore irreducibile, mostrando che il Random Forest riduce significativamente il bias rispetto al Decision Tree, mantenendo varianza moderata.\n",
    "\n",
    "Per la **classificazione** abbiamo fatto la stessa decomposizione (loss=0â€“1). Anche qui il Random Forest presenta un bias inferiore e una varianza piÃ¹ contenuta rispetto al Decision Tree.\n",
    "\n",
    "Per il **clustering** (KMeans e DBSCAN) abbiamo calcolato tre indici di validitÃ  interna:\n",
    "1. **Silhouette Score** (compattezza/separazione),\n",
    "2. **Calinskiâ€“Harabasz** (indice di varianza interna vs interâ€cluster),\n",
    "3. **Daviesâ€“Bouldin** (indice di similaritÃ  tra cluster).\n",
    "\n",
    "In particolare, KMeans (k=4) ha ottenuto silhouette â‰ƒ 0.6, CH â‰ƒ 9.6 e DB â‰ƒ 1.1, confermando una buona separazione dei cluster; DBSCAN ha mostrato bassa validitÃ  a causa di unâ€™alta percentuale di outlier."
   ],
   "id": "93f0d394616b5a92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Definiamo i modelli\n",
    "models_reg = {\n",
    "    'DT': DecisionTreeRegressor(random_state=42),\n",
    "    'RF': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_reg.items():\n",
    "    mse, bias_sq, var = bias_variance_decomp(\n",
    "        model,\n",
    "        X_reg_train.values, y_reg_train.values,\n",
    "        X_reg_test.values,  y_reg_test.values,\n",
    "        loss='mse',\n",
    "        num_rounds=30,\n",
    "        random_seed=42\n",
    "    )\n",
    "    print(f\"{name} Regression:\")\n",
    "    print(f\"  MSE total = {mse:.3f}\")\n",
    "    print(f\"  BiasÂ²     = {bias_sq:.3f}\")\n",
    "    print(f\"  Varianza  = {var:.3f}\")\n",
    "    print(f\"  Rumore    = {mse - bias_sq - var:.3f}\\n\")"
   ],
   "id": "f7304cc04fd76951",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Creiamo il codificatore per le labels di classificazione\n",
    "le = LabelEncoder()\n",
    "y_clf_train_enc = le.fit_transform(y_clf_train)\n",
    "y_clf_test_enc = le.transform(y_clf_test)\n",
    "\n",
    "# Verifichiamo la mappatura\n",
    "print(\"Labels Encoding:\")\n",
    "for orig, enc in zip(le.classes_, le.transform(le.classes_)):\n",
    "    print(f\"{orig} -> {enc}\")\n",
    "\n",
    "# Usiamo loss='0-1' per classificazione\n",
    "models_clf = {\n",
    "    'DT': DecisionTreeClassifier(random_state=42),\n",
    "    'RF': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_clf.items():\n",
    "    avg_err, bias, var = bias_variance_decomp(\n",
    "        model,\n",
    "        X_clf_train.values, y_clf_train_enc,\n",
    "        X_clf_test.values,  y_clf_test_enc,\n",
    "        loss='0-1_loss',    # classificazione\n",
    "        num_rounds=30,\n",
    "        random_seed=42\n",
    "    )\n",
    "    print(f\"\\n{name} Classification:\")\n",
    "    print(f\"  Errore 0-1 medio = {avg_err:.3f}\")\n",
    "    print(f\"  Bias            = {bias:.3f}\")\n",
    "    print(f\"  Varianza        = {var:.3f}\")"
   ],
   "id": "bea056da6524c296",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import numpy as np\n",
    "\n",
    "scores = {'KMeans_silhouette': silhouette_score(data_scaled, labels),\n",
    "          'KMeans_calinski_harabasz': calinski_harabasz_score(data_scaled, labels),\n",
    "          'KMeans_davies_bouldin': davies_bouldin_score(data_scaled, labels)}\n",
    "\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ],
   "id": "8a0ae4cb0939d869",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusioni\n",
    "1. **Regression**:\n",
    "La Random Forest ha ridotto significativamente la varianza rispetto al Decision Tree, migliorando lâ€™errore complessivo.\n",
    "Entrambi i modelli mostrano bias elevato, quindi potrebbe servire maggiore complessitÃ  (feature, tuning) o piÃ¹ dati.\n",
    "Il rumore Ã¨ nullo (o trascurabile), quindi lâ€™errore Ã¨ interamente spiegabile con bias + varianza.\n",
    "2. **CLassification**:\n",
    "Anche nella classificazione la Random Forest ha ridotto la varianza rispetto al Decision Tree, mantenendo perÃ² un bias simile.\n",
    "Lâ€™errore piÃ¹ basso la rende piÃ¹ robusta a fluttuazioni nei dati.\n",
    "3. **Clustering**:\n",
    "I valori indicano una qualitÃ  accettabile ma migliorabile del clustering KMeans.\n",
    "Il Silhouette Score > 0.4 suggerisce cluster parzialmente separati ma sovrapposti.\n",
    "Potrebbero servire: riduzione feature, miglior scelta k, outlier removal o algoritmi alternativi (es. Agglomerative).\n",
    "\n",
    "âœ… Sintesi\n",
    "- I modelli di Random Forest risultano piÃ¹ stabili e con minore varianza sia in regressione che classificazione.\n",
    "- Lâ€™errore Ã¨ dominato dal bias, segno che serve migliorare le feature o complessificare il modello.\n",
    "- La qualitÃ  del clustering Ã¨ discreta: migliorabile via engineering o metriche derivate."
   ],
   "id": "a24e19485fea16c8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
